{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \nimport transformers\nprint('Using Tensorflow version:', tf.__version__)\nprint('Using transformers version:', transformers.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A dependency of the preprocessing for BERT inputs\n!pip install -q -U tensorflow-text\n\n#You will use the AdamW optimizer from tensorflow/models.\n!pip install -q tf-models-official\n\n!pip install simpletransformers\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:36:06.663353Z","iopub.execute_input":"2021-06-13T10:36:06.664258Z","iopub.status.idle":"2021-06-13T10:36:25.609807Z","shell.execute_reply.started":"2021-06-13T10:36:06.663673Z","shell.execute_reply":"2021-06-13T10:36:25.608807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fsspec==0.9.0 #After instaiing this restart using three dots in right corner\nimport fsspec\n#import simpletransformers\nfrom simpletransformers.classification import ClassificationModel\nfrom simpletransformers.classification import MultiLabelClassificationModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A dependency of the preprocessing for BERT inputs\n#!pip install -q -U tensorflow-text\n\n#You will use the AdamW optimizer from tensorflow/models.\n#!pip install -q tf-models-official\n\n#!pip install simpletransformers\n\nimport pandas as pd \nimport numpy as np\n\nimport os\nimport shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer\n\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')\n\nimport collections\nimport pickle\nimport re\nimport random\nimport sys\nimport os \nimport time\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom numpy import cumsum\n\nfrom transformers import pipeline\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn import preprocessing\nfrom transformers import AutoTokenizer #this automatically identifies tokenizer related to the model\nfrom transformers import TFAutoModelForSequenceClassification, TFTrainingArguments, TFTrainer\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom simpletransformers.classification import MultiLabelClassificationModel\n\n\nfrom simpletransformers.classification import MultiLabelClassificationModel\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:36:37.492809Z","iopub.execute_input":"2021-06-13T10:36:37.493143Z","iopub.status.idle":"2021-06-13T10:36:40.772865Z","shell.execute_reply.started":"2021-06-13T10:36:37.49311Z","shell.execute_reply":"2021-06-13T10:36:40.771894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encodeY(y,classes):\n    encoder = preprocessing.LabelEncoder()\n    y_c = encoder.fit_transform(y)\n    y_c = label_binarize(y_c, classes=classes)\n    return y_c\n    \ndef accuracy_neural(y_val,y_pred,avg_method,target_names):\n    print(\"Accuracy Score: \",metrics.accuracy_score(y_val,y_pred))\n    print(\"Precision: \", metrics.precision_score(y_val,y_pred,average=avg_method))\n    print(\"Recall: \", metrics.recall_score(y_val,y_pred,average=avg_method))\n    print(\"F1 score: \", metrics.f1_score(y_val,y_pred,average=avg_method))\n    \n    print(\"\\nClassification report\")\n    print(\"---------------------\")\n    print(metrics.classification_report(y_val, y_pred,target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:16.396411Z","iopub.execute_input":"2021-06-13T10:37:16.396805Z","iopub.status.idle":"2021-06-13T10:37:16.405002Z","shell.execute_reply.started":"2021-06-13T10:37:16.39677Z","shell.execute_reply":"2021-06-13T10:37:16.404014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:20.673185Z","iopub.execute_input":"2021-06-13T10:37:20.673534Z","iopub.status.idle":"2021-06-13T10:37:20.736125Z","shell.execute_reply.started":"2021-06-13T10:37:20.673487Z","shell.execute_reply":"2021-06-13T10:37:20.734993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_sentiment(X_train,y_train,X_test,y_test):\n  # Create a TransformerModel\n  model = ClassificationModel('bert', 'bert-base-cased', num_labels=7, args={'reprocess_input_data': True, \n                                        'num_train_epochs':10,'overwrite_output_dir': True},use_cuda=True)\n\n  y_train_=[] \n  y_test_=[]\n\n  for arr in y_train:\n    y_train_.append(np.argmax(arr))\n\n  for arr in y_test:\n    y_test_.append(np.argmax(arr))\n\n  train_df=pd.DataFrame({\"text\":X_train,\"labels\":y_train_})\n  test_df=pd.DataFrame({\"text\":X_test,\"labels\":y_test_})\n\n  model.train_model(train_df)\n  result, model_outputs, wrong_predictions = model.eval_model(test_df)\n\n  y_pred = []\n  for arr in model_outputs:\n    y_pred.append(np.argmax(arr))\n\n  y_test = test_df['labels'].tolist()\n\n  acc=metrics.accuracy_score(y_test,y_pred)\n  pre=metrics.precision_score(y_test,y_pred,average=avg_method)\n  rec=metrics.recall_score(y_test,y_pred,average=avg_method)\n  f1=metrics.f1_score(y_test,y_pred,average=avg_method)\n\n  return acc,pre,rec,f1\n\ndef bert_aspect(X_train,y_train,X_test,y_test):\n    \n  # Create a TransformerModel\n\n  aspects=['network','billing_price','package','customer_service','data','service_product']\n  #model = MultiLabelClassificationModel('bert', 'bert-base-cased', num_labels=6, args={'train_batch_size':2, 'gradient_accumulation_steps':16, 'learning_rate': 3e-5, 'num_train_epochs': 3, 'max_seq_length': 512,\n  #                                                                                        'reprocess_input_data': True, 'overwrite_output_dir': True},use_cuda=False)\n  \n  model = MultiLabelClassificationModel('bert', 'bert-base-cased', num_labels=6, args={'reprocess_input_data': True,\n                                        'overwrite_output_dir': True,'num_train_epochs':10},use_cuda=True)\n  \n  x_train_=pd.DataFrame(X_train)\n  x_train_.columns=['text']\n  x_test_=pd.DataFrame(X_test)\n  x_test_.columns=['text']\n\n  y_train_=pd.DataFrame(y_train)\n  y_train_.columns=aspects\n  y_test_=pd.DataFrame(y_test)\n  y_test_.columns=aspects\n  \n\n  x_train_=x_train_.reset_index()\n  x_train_=x_train_[['text']]\n  y_train_=y_train_.reset_index()\n  y_train_=y_train_[aspects]\n\n  x_test_=x_test_.reset_index()\n  x_test_=x_test_[['text']]\n  y_test_=y_test_.reset_index()\n  y_test_=y_test_[aspects]\n\n  train_df=pd.concat([x_train_,y_train_],axis=1)\n  test_df=pd.concat([x_test_,y_test_],axis=1)\n  train_df['labels'] = list(train_df[aspects].values)\n  test_df['labels'] = list(test_df[aspects].values)\n\n  print(train_df.head(2))\n\n  train_df=train_df[['text','labels']]\n  test_df=test_df[['text','labels']]\n\n  model.train_model(train_df)\n  result, model_outputs, wrong_predictions = model.eval_model(test_df)\n\n  predictions = model_outputs >=0.5\n  y_pred = predictions.astype(int)\n  \n  acc=metrics.accuracy_score(y_test,y_pred)\n  pre=metrics.precision_score(y_test,y_pred,average=avg_method)\n  rec=metrics.recall_score(y_test,y_pred,average=avg_method)\n  f1=metrics.f1_score(y_test,y_pred,average=avg_method)\n\n  return acc,pre,rec,f1\n\ndef crossval(model_name,x,y):\n    acc_per_fold = []\n    loss_per_fold = []\n    f1_per_fold = []\n    precision_per_fold =[]\n    recall_per_fold=[]\n    \n    if(model_name =='bert_sentiment'):\n        kf = StratifiedKFold(n_splits=N_FOLDS,random_state=None, shuffle=False)\n        lst=[]\n        for arr in y:\n            lst.append(np.argmax(arr))\n        kf_split=kf.split(x,lst)\n        \n    elif(model_name =='bert_aspect'):\n        kf = KFold(n_splits=N_FOLDS)\n        kf_split=kf.split(x,y)\n\n    fold_no = 1\n    for train_index, test_index in kf_split:\n        X_train=x[train_index]\n        y_train=y[train_index]\n        X_test=x[test_index]\n        y_test=y[test_index]\n    \n        if model_name=='bert_sentiment':\n            acc,pre,rec,f1=bert_sentiment(X_train,y_train,X_test,y_test)  \n        elif model_name=='bert_aspect':\n            acc,pre,rec,f1=bert_aspect(X_train,y_train,X_test,y_test)    \n        else:\n            return print(\"No Model found\")\n            \n        acc_per_fold.append(acc * 100)\n        precision_per_fold.append(pre)\n        recall_per_fold.append(rec)\n        f1_per_fold.append(f1)\n        \n        print(\"Accuracy:\",acc,rec,rec,f1)\n\n    # Increase fold number\n    fold_no = fold_no + 1\n\n    acc=np.mean(acc_per_fold)\n    precision=np.mean(precision_per_fold)\n    recall=np.mean(recall_per_fold)\n    f1=np.mean(f1_per_fold)\n    \n    return model_name,acc,precision,recall,f1","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:32.711051Z","iopub.execute_input":"2021-06-13T10:37:32.711387Z","iopub.status.idle":"2021-06-13T10:37:32.734428Z","shell.execute_reply.started":"2021-06-13T10:37:32.711356Z","shell.execute_reply":"2021-06-13T10:37:32.733432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:38.457289Z","iopub.execute_input":"2021-06-13T10:37:38.457656Z","iopub.status.idle":"2021-06-13T10:37:38.466134Z","shell.execute_reply.started":"2021-06-13T10:37:38.457623Z","shell.execute_reply":"2021-06-13T10:37:38.4649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Executing  BERT with telco dataset \n\nEPOCHS=10\nN_FOLDS=3\navg_method='weighted'\n\ndf=pd.read_csv('../input/dftrain/df_train.csv')\ndf=df[~df['comment'].isna()]\ndf['sentiment'] = df['sentiment']+1\n#df=df.head(100)\n\n#MODELS_LIST=['bert_sentiment','bert_aspect']\nMODELS_LIST=['bert_sentiment']\nacclist=[] ; prelist=[]  ; reclist=[]  ; f1list=[]  ; losslist=[] ; modelname=[]\n\nfor x in MODELS_LIST:\n    if(x=='bert_aspect'):\n        aspects=['network','billing_price','package','customer_service','data','service_product']\n        x=df['comment']\n        y=df[aspects]\n        y=y.values\n        mod,acc,precision,recall,f1=crossval('bert_aspect',x,y)  \n        \n    elif(x=='bert_sentiment'):\n        x=df['comment']\n        y=df['sentiment']\n        y=encodeY(y,[0,1,2])\n        mod,acc,precision,recall,f1=crossval('bert_sentiment',x,y)  \n    \n    else: \n        print(\"No Model Found in MODEL_LIST\")\n        \n    modelname.append(mod)\n    acclist.append(acc)\n    prelist.append(precision)\n    reclist.append(recall)\n    f1list.append(f1)\n\n    accdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n    print(accdf)\n\naccdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n\nprint(accdf)\naccdf.to_csv(\"/kaggle/working/accuracy_bert_sent.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Executing  BERT with MALAYALAM\n\nEPOCHS=10\nN_FOLDS=3\navg_method='weighted'\n\ndf=pd.read_csv(\"../input/malayalam-new/malayalam_data.csv\")\ndf=df[df['text'].map(lambda x: x.isascii())]\ndf.columns=['comment','sentiment']\ndf=df.reset_index()\ndf=df[['comment','sentiment']]\n\n#MODELS_LIST=['bert_sentiment','bert_aspect']\nMODELS_LIST=['bert_sentiment']\nacclist=[] ; prelist=[]  ; reclist=[]  ; f1list=[]  ; losslist=[] ; modelname=[]\n\nfor x in MODELS_LIST:\n    if(x=='bert_aspect'):\n        aspects=['network','billing_price','package','customer_service','data','service_product']\n        x=df['comment']\n        y=df[aspects]\n        y=y.values\n        mod,acc,precision,recall,f1=crossval('bert_aspect',x,y)  \n        \n    elif(x=='bert_sentiment'):\n        x=df['comment']\n        y=df['sentiment']\n        y=encodeY(y,[0,1,2,3,4,5,6])\n        mod,acc,precision,recall,f1=crossval('bert_sentiment',x,y)  \n    \n    else: \n        print(\"No Model Found in MODEL_LIST\")\n        \n    modelname.append(mod)\n    acclist.append(acc)\n    prelist.append(precision)\n    reclist.append(recall)\n    f1list.append(f1)\n\n    accdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n    print(accdf)\n\naccdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n\nprint(accdf)\naccdf.to_csv(\"/kaggle/working/accuracy_bert_MALAYALAM.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:46.729149Z","iopub.execute_input":"2021-06-13T10:37:46.729477Z","iopub.status.idle":"2021-06-13T11:06:04.026581Z","shell.execute_reply.started":"2021-06-13T10:37:46.729445Z","shell.execute_reply":"2021-06-13T11:06:04.025222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nModel   Accuracy  Precision    Recall        F1\n0  bert_sentiment  73.359804   0.791900  0.733598  0.717336\n1     bert_aspect  54.076245   0.807218  0.810686  0.805948","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"            Model   Accuracy  Precision    Recall        F1\n0  bert_sentiment  74.940522   0.785375  0.749405  0.725243\n1     bert_aspect  45.210100   0.765713  0.674802  0.708485","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}