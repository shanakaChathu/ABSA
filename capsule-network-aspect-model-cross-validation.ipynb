{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\n\nimport pandas as pd \nimport numpy as np \nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.corpus import wordnet\nimport pandas as pd \nimport numpy as np \nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom nltk.corpus import wordnet\nimport pandas as pd \nimport numpy as np \nimport string\n!pip install nltk\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('treebank')\nfrom nltk.tag import UnigramTagger\nfrom nltk.corpus import treebank\nimport pandas as pd \nimport numpy as np \nimport string\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\n#import fasttext\nimport gensim\nfrom gensim.test.utils import datapath\nfrom gensim.models import KeyedVectors\nimport seaborn as sns \nimport re  # For preprocessing\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\nimport spacy  # For preprocessing\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n#from xlwt import Workbook \nimport wordcloud\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport lightgbm as lgb\nfrom sklearn.naive_bayes import MultinomialNB\n#from xgboost import XGBClassifier\n#import xgboost\nimport scipy.sparse\n\n\nimport time\nimport warnings\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \nimport pandas as pd \n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport os\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nimport multiprocessing\nfrom gensim.models import Word2Vec\n#from gensim.models import FastText\nfrom sklearn.metrics import roc_auc_score\nfrom numpy import asarray\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport os\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nimport multiprocessing\nfrom gensim.models import Word2Vec\n#from gensim.models import FastText\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers import BatchNormalization\nfrom keras.models import model_from_json\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import BatchNormalization\nfrom keras import backend as K\nfrom sklearn.metrics import recall_score\nfrom keras.models import load_model\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nimport matplotlib.pyplot as pyplot\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers import GlobalMaxPooling1D\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers import BatchNormalization\nfrom keras.models import model_from_json\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import BatchNormalization\nfrom keras import backend as K\nfrom sklearn.metrics import recall_score\nfrom keras.models import load_model\nimport tensorflow as tf \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nfrom keras import backend as K\nfrom keras.models import Sequential,Model,load_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dropout, Activation, Flatten, \\\n    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D\nfrom keras.layers.recurrent import LSTM, GRU, SimpleRNN\nfrom keras.regularizers import l2, l1_l2\nfrom keras.constraints import maxnorm\nfrom keras import callbacks\nfrom keras.utils import generic_utils,plot_model\nfrom keras.optimizers import Adadelta\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import Bidirectional\n\n\n#Capsule network related libraries \n\nimport os\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:23:33.083293Z","iopub.execute_input":"2021-06-13T13:23:33.083961Z","iopub.status.idle":"2021-06-13T13:23:43.386361Z","shell.execute_reply.started":"2021-06-13T13:23:33.083844Z","shell.execute_reply":"2021-06-13T13:23:43.385327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Functions \n\ndef encodeY(y,classes):\n    encoder = preprocessing.LabelEncoder()\n    y_c = encoder.fit_transform(y)\n    y_c = label_binarize(y_c, classes=classes)\n    return y_c\n\ndef emb_dictionary(filename):\n    embedding_dictionary = dict()\n    f = open(filename)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = asarray(values[1:], dtype='float32')\n        embedding_dictionary[word] = coefs\n    f.close()\n    return embedding_dictionary\n\ndef emb_matrix(vocab_length,word_tokenizer,embeddings_dictionary,we_size):\n    embedding_matrix = zeros((vocab_length, we_size))\n    for word, index in word_tokenizer.word_index.items():\n        embedding_vector = embeddings_dictionary.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector\n    return  embedding_matrix\n\ndef get_emb_sentences(X):\n    word_tokenizer = Tokenizer()\n    word_tokenizer.fit_on_texts(x)\n    vocab_length = len(word_tokenizer.word_index) + 1\n    embedded_sentences = word_tokenizer.texts_to_sequences(x)\n    return word_tokenizer,embedded_sentences,vocab_length\n\ndef get_padded_sequence(x,embedded_sentences):\n    word_count = lambda sentence: len(word_tokenize(sentence))\n    longest_sentence = max(x, key=word_count)\n    length_long_sentence = len(word_tokenize(longest_sentence))\n    padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n    return length_long_sentence,padded_sentences \n\ndef loadembedding_matrix(x,loc,emb_size):\n    word_tokenizer,embedded_sentences,vocab_length=get_emb_sentences(x)\n    length_long_sentence,padded_sentences = get_padded_sequence(x,embedded_sentences)\n    embeddings_dictionary=emb_dictionary(loc)\n    embedding_matrix=emb_matrix(vocab_length,word_tokenizer,embeddings_dictionary,emb_size)\n    return vocab_length,padded_sentences,length_long_sentence,embedding_matrix\n    \ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef splitdata(x,y,splitratio,valflag):\n    x_train_main, x_test, y_train_main, y_test = train_test_split(x, y, test_size=splitratio, random_state=1)\n    x_train, x_val, y_train, y_val = train_test_split(x_train_main, y_train_main, test_size=splitratio, random_state=1)\n\n    y_train_main_c=y_train_main.values\n    y_test_c=y_test.values\n    y_train_c=y_train.values\n    y_val_c=y_val.values\n    \n    if(valflag):\n        return  x_train,x_val,x_test,y_train,y_val, y_test,y_train_c,y_val_c, y_test_c\n    else:\n        return  x_train_main,x_test,y_train_main, y_test,y_train_main_c, y_test_c\n\ndef train_model(model,x_train, y_train):\n    print('Model Training')\n    #es = EarlyStopping(monitor=MONITOR, mode=MONITOR_MODE, verbose=1, patience=PATIENCE)\n    #mc = ModelCheckpoint(MODEL_SAVE_PATH, monitor=MONITOR, verbose=1, save_best_only=True, mode=MONITOR_MODE)\n    #callbacks_list = [es,mc]\n    #history = model.fit(x_train, y_train, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE,callbacks=callbacks_list, verbose=1)\n    history = model.fit(x_train, y_train, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n    return model,history \n\ndef pred_neural(clf, feature_vector_valid,multiflag):\n    if (multiflag):\n        predictions_prob = clf.predict(feature_vector_valid)\n        predictions_prob_ = predictions_prob >= 0.5\n        predictions = predictions_prob_.astype(int)\n    else: \n        predictions = clf.predict(feature_vector_valid).argmax(axis=1)\n        predictions_prob = clf.predict(feature_vector_valid)\n    return predictions, predictions_prob    \n    \ndef accuracy_neural(y_val,y_pred,avg_method,target_names):\n    print(\"Accuracy Score: \",metrics.accuracy_score(y_val,y_pred))\n    print(\"Precision: \", metrics.precision_score(y_val,y_pred,average=avg_method))\n    print(\"Recall: \", metrics.recall_score(y_val,y_pred,average=avg_method))\n    print(\"F1 score: \", metrics.f1_score(y_val,y_pred,average=avg_method))\n    \n    print(\"\\nClassification report\")\n    print(\"---------------------\")\n    print(metrics.classification_report(y_val, y_pred,target_names=target_names))\n\ndef learning_curve(history,measure,title):\n    epochs=range(len(history.history[measure]))\n    plt.plot(epochs,history.history[measure], 'r')\n    plt.plot(epochs,history.history['val_'+measure], 'b')\n    plt.title('Training and validation '+title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(title)\n    plt.legend([\"Training \"+measure, \"Validation \"+measure])\n    plt.figure()\n\ndef plot_confusion_matrix(cm,target_names, title='Confusion matrix', cmap=None, normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:24:41.21809Z","iopub.execute_input":"2021-06-13T13:24:41.218461Z","iopub.status.idle":"2021-06-13T13:24:41.254618Z","shell.execute_reply.started":"2021-06-13T13:24:41.218431Z","shell.execute_reply":"2021-06-13T13:24:41.253926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.engine import Layer\nfrom keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\ngru_len = 128\nRoutings = 5\nNum_capsule = 10\nDim_capsule = 16\ndropout_p = 0.25\nrate_drop_dense = 0.28\n\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n            #if i < self.routings - 1:\n            #    b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:24:48.460191Z","iopub.execute_input":"2021-06-13T13:24:48.460679Z","iopub.status.idle":"2021-06-13T13:24:48.477558Z","shell.execute_reply.started":"2021-06-13T13:24:48.460646Z","shell.execute_reply":"2021-06-13T13:24:48.476528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\nmainloc = '../input/dftrain/'\ndata_path= mainloc+\"df_train.csv\"\n\ndf=pd.read_csv(data_path)\ndf=df[~df['comment'].isna()]\nprint(\"Train size: \", df.shape)\ndf.head(2)\n\n#Extract X and Y \n\ncategories=['network','billing_price','package','customer_service','data','service_product']   \nx=df['comment']\ny=df[categories]\ny_c=y.values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_SIZE=400\nWE_TYPE='cbow'\nNUM_CLASSES=6\nNB_FILTERS=200 \nKERNEL_SIZE=5\nSHUFFLE=True\nHIDDEN_DIMS=NB_FILTERS*2\nDROPOUT_VALUE_1=0.5 \nDROPOUT_VALUE_2=0.5 \nL2_REG=0.1\nMONITOR='val_f1_m' \nMONITOR_MODE='max' \nPATIENCE=10\nEPOCHS=8\nVALIDATION_SPLIT=0.2 \nSHUFFLE=True \nBATCH_SIZE=32\nMODEL_SAVE_PATH='../output/best_model.h5'\nTARGET_NAMES=['network','billing_price','package','customer_service','data','service_product']\nactivation='sigmoid'\nloss='binary_crossentropy'\n\n\n#Define dependencied for model loading\ndependencies = {\n    'recall_m': recall_m,\n    'f1_m':f1_m,\n    'precision_m':precision_m    \n}\n\ndef capsule_network():\n    model = Sequential()\n    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\n    model.add(embedding_layer)\n    model.add(SpatialDropout1D(rate_drop_dense))\n    model.add(Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True)))\n    model.add(Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True))\n    model.add(Flatten())\n    model.add(Dropout(dropout_p))\n    model.add(Dense(NUM_CLASSES,activation=activation))\n    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])    \n    model.summary()\n    return model \n\n\n#we_path=mainloc+'word_embeddings/w2v_'+WE_TYPE+'_'+str(EMBEDDING_SIZE)+'.txt'\nwe_path = '../input/wordembeddings/w2v_cbow_400.txt'\nvocab_length,padded_sentences,length_long_sentence,embedding_matrix=loadembedding_matrix(x,we_path,EMBEDDING_SIZE)\nx_train,x_test,y_train,y_test,y_train_c,y_test_c = splitdata(padded_sentences,y,0.2,False)\nmodel=capsule_network()\nmodel,history=train_model(model,x_train,y_train_c) \n\nprint(\"\\nTraining accuracy\")\nprint(\"--------------------------------------------\")\npredictions, predictions_prob  = pred_neural(model,x_train,True)\naccuracy_neural(y_train,predictions,'weighted',TARGET_NAMES)\n\nprint(\"\\n Testing accuracy\")\nprint(\"--------------------------------------------\")\npredictions, predictions_prob  = pred_neural(model,x_test,True)\naccuracy_neural(y_test,predictions,'weighted',TARGET_NAMES)\n\nmultilabel_confusion_matrix(y_test_c, predictions)\n\nlearning_curve(history,'loss','loss')\nlearning_curve(history,'accuracy','accuracy')\nlearning_curve(history,'f1_m','F1')\nlearning_curve(history,'precision_m','Precision')\nlearning_curve(history,'recall_m','Recall')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross validation ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsulegru_network():\n    model = Sequential()\n    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\n    model.add(embedding_layer)\n    model.add(SpatialDropout1D(rate_drop_dense))\n    model.add(Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True)))\n    model.add(Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True))\n    model.add(Flatten())\n    model.add(Dropout(dropout_p))\n    model.add(Dense(NUM_CLASSES,activation=activation))\n    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])    \n    model.summary()\n    return model \n\ndef capsulelstm_network():\n    model = Sequential()\n    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\n    model.add(embedding_layer)\n    model.add(SpatialDropout1D(rate_drop_dense))\n    model.add(Bidirectional(LSTM(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True)))\n    model.add(Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True))\n    model.add(Flatten())\n    model.add(Dropout(dropout_p))\n    model.add(Dense(NUM_CLASSES,activation=activation))\n    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])    \n    model.summary()\n    return model \n\ndef capsule_network():\n    model = Sequential()\n    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\n    model.add(embedding_layer)\n    model.add(SpatialDropout1D(rate_drop_dense))\n    model.add(Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True))\n    model.add(Flatten())\n    model.add(Dropout(dropout_p))\n    model.add(Dense(NUM_CLASSES,activation=activation))\n    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])    \n    model.summary()\n    return model\n\ndef capsulemalay_network():\n    model = Sequential()\n    #embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence,trainable=False)\n    #model.add(embedding_layer)\n    model.add(Embedding(vocab_length,EMBEDDING_SIZE,input_length=length_long_sentence,trainable=True))\n    model.add(SpatialDropout1D(rate_drop_dense))\n    model.add(Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True)))\n    model.add(Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True))\n    model.add(Flatten())\n    model.add(Dropout(dropout_p))\n    model.add(Dense(NUM_CLASSES,activation=activation))\n    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])    \n    model.summary()\n    return model\n\ndef crossval(model_name):\n\n  # Define per-fold score containers <-- these are new\n  acc_per_fold = []\n  loss_per_fold = []\n  f1_per_fold = []\n  precision_per_fold =[]\n  recall_per_fold=[]\n\n  kf = KFold(n_splits=N_FOLDS)\n  fold_no = 1\n  for train_index, test_index in kf.split(padded_sentences,y):\n    if model_name=='capsule':\n      model=capsule_network()\n    elif model_name=='capsulegru':\n      model=capsulegru_network()\n    elif model_name=='capsulelstm':\n      model=capsulelstm_network()\n    elif model_name=='capsulemalay':\n      model=capsulemalay_network()\n    else:\n      print(\"No Model found\")\n    \n    es = EarlyStopping(monitor=MONITOR, mode=MONITOR_MODE, verbose=1,patience= PATIENCE )\n    history = model.fit(padded_sentences[train_index],y_c[train_index],epochs=EPOCHS,callbacks=[es],batch_size=BATCH_SIZE,shuffle=SHUFFLE,validation_split=VALIDATION_SPLIT)\n    \n    # Generate generalization metrics\n    scores = model.evaluate(padded_sentences[test_index], y_c[test_index], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% ;{model.metrics_names[2]} of {scores[2]} ;{model.metrics_names[3]} of {scores[3]} ; \\\n           {model.metrics_names[4]} of {scores[4]}')\n    loss_per_fold.append(scores[0])\n    acc_per_fold.append(scores[1] * 100)\n    precision_per_fold.append(scores[2])\n    recall_per_fold.append(scores[3])\n    f1_per_fold.append(scores[4])\n\n    # Increase fold number\n    fold_no = fold_no + 1\n\n  acc=np.mean(acc_per_fold)\n  precision=np.mean(precision_per_fold)\n  recall=np.mean(recall_per_fold)\n  f1=np.mean(f1_per_fold)\n  loss=np.mean(loss_per_fold)\n  return acc,precision,recall,f1,loss\n\n\nimport torch\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:27:10.825142Z","iopub.execute_input":"2021-06-13T13:27:10.825507Z","iopub.status.idle":"2021-06-13T13:27:11.814891Z","shell.execute_reply.started":"2021-06-13T13:27:10.825474Z","shell.execute_reply":"2021-06-13T13:27:11.813675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For telco dataset - aspect model \n\n#Extract X and Y \ndf=pd.read_csv('../input/dftrain/df_train.csv')\ndf=df[~df['comment'].isna()]\naspects=['network','billing_price','package','customer_service','data','service_product']   \n\n#df=df.head(100)\n\n#Extract X and Y \nx=df['comment']\ny=df[aspects]\ny_c=y.values\n\nEMBEDDING_SIZE=400\nWE_TYPE='cbow'\nNUM_CLASSES=6\nNB_FILTERS=200 \nKERNEL_SIZE=5\nSHUFFLE=True\nHIDDEN_DIMS=NB_FILTERS*2\nDROPOUT_VALUE_1=0.5 \nDROPOUT_VALUE_2=0.5 \nL2_REG=0.1\nN_FOLDS=2\nMONITOR='val_f1_m' \nMONITOR_MODE='max' \nPATIENCE=10\nEPOCHS=10\nVALIDATION_SPLIT=0.2 \nSHUFFLE=True \nBATCH_SIZE=32 \nactivation='sigmoid'\nloss='binary_crossentropy'\n\nMODELS_LIST=['capsulegru']\nacclist=[] ; prelist=[]  ; reclist=[]  ; f1list=[]  ; losslist=[] ; modelname=[] ; wename=[] ; wesize = []\n\nfor mod in MODELS_LIST:\n  #we_path=mainloc+'word_embeddings/w2v_'+WE_TYPE+'_'+str(EMBEDDING_SIZE)+'.txt'\n  we_path='../input/wordembeddings/w2v_cbow_400.txt'\n  vocab_length,padded_sentences,length_long_sentence,embedding_matrix=loadembedding_matrix(x,we_path,EMBEDDING_SIZE)\n  if (mod=='capsule'):\n    acc,precision,recall,f1,loss=crossval('capsule')\n  elif (mod=='capsulegru'):\n    acc,precision,recall,f1,loss=crossval('capsulegru')\n  elif (mod=='capsulelstm'):\n    acc,precision,recall,f1,loss=crossval('capsulelstm')\n  else: \n     print(\"Inputed model is not available\")\n \n  modelname.append(mod)\n  wename.append(WE_TYPE)\n  wesize.append(EMBEDDING_SIZE)\n  acclist.append(acc)\n  prelist.append(precision)\n  reclist.append(recall)\n  f1list.append(f1)\n  losslist.append(loss)        \n  \n  accdf=pd.DataFrame({'Model':modelname,'Word_Embedding':wename, 'Embedding_Size':wesize,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n  #accdf.to_csv(mainloc+\"Sentiment_Accuracy_.csv\",index=False)\n\naccdf=pd.DataFrame({'Model':modelname,'Word_Embedding':wename, 'Embedding_Size':wesize,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\naccdf.to_csv(\"/kaggle/working/Capsule_accuracy.csv\",index=False)\naccdf","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:06.24353Z","iopub.execute_input":"2021-06-13T13:43:06.244119Z","iopub.status.idle":"2021-06-13T14:15:08.770021Z","shell.execute_reply.started":"2021-06-13T13:43:06.244076Z","shell.execute_reply":"2021-06-13T14:15:08.76732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nmainloc = '../input/dftrain/'\n\ndf1=pd.read_csv('../input/dftrain/df_train.csv')\ndf1=df1[~df1['comment'].isna()]\n\ndf2=pd.read_csv('../input/malayalam-new/malayalam_data.csv')\ndf2=df2[df2['text'].map(lambda x: x.isascii())]\ndf2.columns=['comment','sentiment']\n\nprint(df1.head(2))\nprint(df2.head(2))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:29:03.053551Z","iopub.execute_input":"2021-06-13T17:29:03.053955Z","iopub.status.idle":"2021-06-13T17:29:03.181045Z","shell.execute_reply.started":"2021-06-13T17:29:03.053914Z","shell.execute_reply":"2021-06-13T17:29:03.180304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer1 = CountVectorizer()\nX = vectorizer1.fit_transform(df1.comment)\nvocab_df1=vectorizer1.get_feature_names()\nprint(len(vocab_df1))\n\nvectorizer2 = CountVectorizer()\nX = vectorizer2.fit_transform(df2.comment)\nvocab_df2=vectorizer2.get_feature_names()\nprint(len(vocab_df2))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:29:17.826711Z","iopub.execute_input":"2021-06-13T17:29:17.827251Z","iopub.status.idle":"2021-06-13T17:29:19.19328Z","shell.execute_reply.started":"2021-06-13T17:29:17.827218Z","shell.execute_reply":"2021-06-13T17:29:19.192072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import words\n\"fine1\" in words.words()\n\n#vocab_df1=['hello','fine','shana']\n\n#cnt1=0\n#for x in vocab_df1:\n#    if(x.lower() in words.words()):\n#        cnt1=cnt1+1\n    \ncnt2=0\nfor x in vocab_df2:\n    if(x.lower() in words.words()):\n        cnt2=cnt2+1\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T18:10:16.602924Z","iopub.execute_input":"2021-06-13T18:10:16.603433Z","iopub.status.idle":"2021-06-13T18:20:28.500446Z","shell.execute_reply.started":"2021-06-13T18:10:16.603398Z","shell.execute_reply":"2021-06-13T18:20:28.499524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print((cnt1/len(vocab_df1))*100)\n#print((cnt2/len(vocab_df2))*100)\n\n#cnt1=2322\n#cnt2=","metadata":{"execution":{"iopub.status.busy":"2021-06-13T18:10:01.63677Z","iopub.execute_input":"2021-06-13T18:10:01.637142Z","iopub.status.idle":"2021-06-13T18:10:01.767755Z","shell.execute_reply.started":"2021-06-13T18:10:01.637107Z","shell.execute_reply":"2021-06-13T18:10:01.766611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((cnt1/len(vocab_df1))*100)\nprint((cnt2/len(vocab_df2))*100)\n\nEnglish word percentage Telco dataset : 9.37%\nEnglish word percentage Malayalam dataset : 25.6%","metadata":{"execution":{"iopub.status.busy":"2021-06-13T18:20:43.405008Z","iopub.execute_input":"2021-06-13T18:20:43.405394Z","iopub.status.idle":"2021-06-13T18:20:43.41162Z","shell.execute_reply.started":"2021-06-13T18:20:43.40536Z","shell.execute_reply":"2021-06-13T18:20:43.41061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \ndf=pd.read_csv(\"../input/malayalam-new/malayalam_data.csv\")\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:55:32.022284Z","iopub.execute_input":"2021-06-14T14:55:32.022801Z","iopub.status.idle":"2021-06-14T14:55:32.084554Z","shell.execute_reply.started":"2021-06-14T14:55:32.022763Z","shell.execute_reply":"2021-06-14T14:55:32.083514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot = df.plot.pie(y='mass', figsize=(5, 5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def desc(x):\n    if (x.label==1):\n        return 'Gratitude'\n    elif (x.label==2):\n        return 'About the recipe'\n    elif (x.label==3):\n        return 'About the video'\n    elif (x.label==4):\n        return 'Praising'\n    elif (x.label==5):\n        return 'Hybrid'\n    elif (x.label==6):\n        return 'Undefined'\n    elif (x.label==7):\n        return 'Suggestions and Queries'\n    else:\n        return 1\n \ndf1=df.groupby('label').count().reset_index()\ndf1['labeldesc']=df1.apply(desc,axis=1)\ndf2=df1[['labeldesc','text']]\n#plot = df2.plot.pie(y='text', figsize=(5, 5),autopct='%1.0f%%')\ndf2.index=df2.labeldesc\ndf3=df2[['text']]\nplot = df3.plot.pie(y='text', figsize=(5, 5),autopct='%1.0f%%')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:08:02.349671Z","iopub.execute_input":"2021-06-14T15:08:02.350081Z","iopub.status.idle":"2021-06-14T15:08:02.626103Z","shell.execute_reply.started":"2021-06-14T15:08:02.350048Z","shell.execute_reply":"2021-06-14T15:08:02.624892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['class']=df.apply(desc,axis=1)\ndf.head(2)\ndf[['text','class']].tail(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:25:17.673611Z","iopub.execute_input":"2021-06-14T15:25:17.674035Z","iopub.status.idle":"2021-06-14T15:25:17.925738Z","shell.execute_reply.started":"2021-06-14T15:25:17.673999Z","shell.execute_reply":"2021-06-14T15:25:17.924688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:55:25.639041Z","iopub.execute_input":"2021-06-14T14:55:25.639481Z","iopub.status.idle":"2021-06-14T14:55:25.710059Z","shell.execute_reply.started":"2021-06-14T14:55:25.639397Z","shell.execute_reply":"2021-06-14T14:55:25.709118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/dftrain/df_train.csv\")\ndf.tail(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:47:45.681125Z","iopub.execute_input":"2021-06-14T15:47:45.681702Z","iopub.status.idle":"2021-06-14T15:47:45.781289Z","shell.execute_reply.started":"2021-06-14T15:47:45.681649Z","shell.execute_reply":"2021-06-14T15:47:45.780257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}