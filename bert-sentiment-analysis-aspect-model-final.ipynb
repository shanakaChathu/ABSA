{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######################################\n### -------- Load libraries ------- ###\n# Load Huggingface transformers\n\n#!pip install transformers\n#!pip install bert-tensorflow\n#!pip install simpletransformers\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval\n\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow\nprint(tensorflow.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:22:04.697936Z","iopub.execute_input":"2021-06-09T17:22:04.698311Z","iopub.status.idle":"2021-06-09T17:22:54.291794Z","shell.execute_reply.started":"2021-06-09T17:22:04.698232Z","shell.execute_reply":"2021-06-09T17:22:54.290836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow\nprint(tensorflow.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:23:16.24021Z","iopub.execute_input":"2021-06-09T17:23:16.240574Z","iopub.status.idle":"2021-06-09T17:23:16.24819Z","shell.execute_reply.started":"2021-06-09T17:23:16.240542Z","shell.execute_reply":"2021-06-09T17:23:16.247206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #Sentiment Model","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:03:51.896601Z","iopub.execute_input":"2021-06-09T18:03:51.896978Z","iopub.status.idle":"2021-06-09T18:03:51.982678Z","shell.execute_reply.started":"2021-06-09T18:03:51.896945Z","shell.execute_reply":"2021-06-09T18:03:51.981796Z"}}},{"cell_type":"code","source":"mainloc = '../input/dftrain/'\ndata_path= mainloc+\"df_train.csv\"\n\ndf=pd.read_csv(data_path)\ndf=df[~df['comment'].isna()]\nprint(\"Train size: \", df.shape)\ndf=df[['comment','sentiment']]\n\nx_train, x_test, y_train, y_test = train_test_split(df['comment'], df['sentiment'], test_size=0.2, random_state=1, stratify=df['sentiment'])\ntrain_df=pd.DataFrame({\"comment\":x_train,\"sentiment\":y_train})\ntest_df=pd.DataFrame({\"comment\":x_test,\"sentiment\":y_test})\n\nprint(\"train: \",train_df.shape)\nprint(\"train: \",test_df.shape)\n\ntrain_df['positive']=np.where(train_df['sentiment']==1,1,0)\ntrain_df['negative']=np.where(train_df['sentiment']==0,1,0)\ntrain_df['neutral']=np.where(train_df['sentiment']==-1,1,0)\ntrain_df=train_df[['comment','positive','negative','neutral']]\n\ntest_df['positive']=np.where(test_df['sentiment']==1,1,0)\ntest_df['negative']=np.where(test_df['sentiment']==0,1,0)\ntest_df['neutral']=np.where(test_df['sentiment']==-1,1,0)\ntest_df=test_df[['comment','positive','negative','neutral']]\n\ndf=train_df\n\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\ncols = df.columns\nlabel_cols = list(cols[1:])\nnum_labels = len(label_cols)\nprint('Label columns: ', label_cols)\n\ndf = df.sample(frac=1).reset_index(drop=True) #shuffle rows\ndf['one_hot_labels'] = list(df[label_cols].values)\ndf.head()\n\nlabels = list(df.one_hot_labels.values)\ncomments = list(df.comment.values)\n\nmax_length = 100\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\nencodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\nprint('tokenizer outputs: ', encodings.keys())\n\ninput_ids = encodings['input_ids'] # tokenized and encoded sentences\ntoken_type_ids = encodings['token_type_ids'] # token type ids\nattention_masks = encodings['attention_mask'] # attention masks\n\n# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\nlabel_counts = df.one_hot_labels.astype(str).value_counts()\none_freq = label_counts[label_counts==1].keys()\none_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\nprint('df label indices with only one instance: ', one_freq_idxs)\n\n# Gathering single instance inputs to force into the training set after stratified split\none_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\none_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\none_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\none_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\n                                                            random_state=1, test_size=0.20, stratify = labels)\n\n# Add one frequency data to train data\ntrain_inputs.extend(one_freq_input_ids)\ntrain_labels.extend(one_freq_labels)\ntrain_masks.extend(one_freq_attention_masks)\ntrain_token_types.extend(one_freq_token_types)\n\n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\ntrain_token_types = torch.tensor(train_token_types)\n\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\nvalidation_token_types = torch.tensor(validation_token_types)\n\n# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntorch.save(validation_dataloader,'output')\ntorch.save(train_dataloader,'output')\n\n#BERT:\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) \n\n# Load model, the pretrained model will include a single linear classification layer on top for classification. \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\nmodel.cuda()\n\n# setting custom optimization parameters. You may implement a scheduler here as well.\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n# optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 2\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n    model.train()\n\n    # Tracking variables\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels, b_token_types = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n\n        # # Forward pass for multiclass classification\n        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        # loss = outputs[0]\n        # logits = outputs[1]\n\n        # Forward pass for multilabel classification\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        logits = outputs[0]\n        loss_func = BCEWithLogitsLoss() \n        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n        # loss_func = BCELoss() \n        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n        train_loss_set.append(loss.item())    \n\n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        # scheduler.step()\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n###############################################################################\n\n  # Validation\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n# Variables to gather full output\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(validation_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    with torch.no_grad():\n        # Forward pass\n        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        b_logit_pred = outs[0]\n        pred_label = torch.sigmoid(b_logit_pred)\n\n        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n        pred_label = pred_label.to('cpu').numpy()\n        b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n    \npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n\n# Calculate Accuracy\nthreshold = 0.50\npred_bools = [pl>threshold for pl in pred_labels]\ntrue_bools = [tl==1 for tl in true_labels]\nval_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\nval_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n\nprint('F1 Validation Accuracy: ', val_f1_accuracy)\nprint('Flat Validation Accuracy: ', val_flat_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:07:16.255015Z","iopub.execute_input":"2021-06-09T18:07:16.255373Z","iopub.status.idle":"2021-06-09T18:07:24.19671Z","shell.execute_reply.started":"2021-06-09T18:07:16.255341Z","shell.execute_reply":"2021-06-09T18:07:24.195736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation \n\ncols = test_df.columns\ntest_label_cols = list(cols[1:])\nprint(test_label_cols)\n\ntest_df['one_hot_labels'] = list(test_df[test_label_cols].values)\ntest_df.head(2)\n\n# Gathering input data\ntest_labels = list(test_df.one_hot_labels.values)\ntest_comments = list(test_df.comment.values)\n\n# Encoding input data\nmax_length=100\ntest_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\ntest_input_ids = test_encodings['input_ids']\ntest_token_type_ids = test_encodings['token_type_ids']\ntest_attention_masks = test_encodings['attention_mask']\n\n# Make tensors out of data\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\ntest_token_types = torch.tensor(test_token_type_ids)\n# Create test dataloader\ntest_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n# Save test dataloader\ntorch.save(test_dataloader,'test_data_loader')\n\n# Test\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n#track variables\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(test_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    with torch.no_grad():\n        # Forward pass\n        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        b_logit_pred = outs[0]\n        pred_label = torch.sigmoid(b_logit_pred)\n\n        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n        pred_label = pred_label.to('cpu').numpy()\n        b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n\n# Flatten outputs\ntokenized_texts = [item for sublist in tokenized_texts for item in sublist]\npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n# Converting flattened binary values to boolean values\ntrue_bools = [tl==1 for tl in true_labels]\n\npred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n\n# Print and save classification report\nprint('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\nprint('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\nclf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\npickle.dump(clf_report, open('classification_report.txt','wb')) #save report\nprint(clf_report)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:16:42.056955Z","iopub.execute_input":"2021-06-09T18:16:42.057313Z","iopub.status.idle":"2021-06-09T18:16:49.848862Z","shell.execute_reply.started":"2021-06-09T18:16:42.057283Z","shell.execute_reply":"2021-06-09T18:16:49.846093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #Aspect Model \n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:29:31.331951Z","iopub.execute_input":"2021-06-09T18:29:31.332301Z","iopub.status.idle":"2021-06-09T18:29:31.413777Z","shell.execute_reply.started":"2021-06-09T18:29:31.332272Z","shell.execute_reply":"2021-06-09T18:29:31.412909Z"}}},{"cell_type":"code","source":"mainloc = '../input/dftrain/'\ndata_path= mainloc+\"df_train.csv\"\n\ndf=pd.read_csv(data_path)\ndf=df[~df['comment'].isna()]\nprint(\"Train size: \", df.shape)\naspects=['network','billing_price','package','customer_service','data','service_product']\n\nx_train, x_test, y_train, y_test = train_test_split(df['comment'], df[aspects], test_size=0.2, random_state=1)\n\ntrain_df=pd.concat([x_train,y_train],axis=1)\ntest_df=pd.concat([x_test,y_test],axis=1)\n\nprint(\"Train: \",train_df.shape)\nprint(\"Train: \",test_df.shape)\n\ndf=train_df\n\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\ncols = df.columns\nlabel_cols = list(cols[1:])\nnum_labels = len(label_cols)\nprint('Label columns: ', label_cols)\n\ndf = df.sample(frac=1).reset_index(drop=True) #shuffle rows\ndf['one_hot_labels'] = list(df[label_cols].values)\ndf.head()\n\nlabels = list(df.one_hot_labels.values)\ncomments = list(df.comment.values)\n\nmax_length = 100\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\nencodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\nprint('tokenizer outputs: ', encodings.keys())\n\ninput_ids = encodings['input_ids'] # tokenized and encoded sentences\ntoken_type_ids = encodings['token_type_ids'] # token type ids\nattention_masks = encodings['attention_mask'] # attention masks\n\n# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\nlabel_counts = df.one_hot_labels.astype(str).value_counts()\none_freq = label_counts[label_counts==1].keys()\none_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\nprint('df label indices with only one instance: ', one_freq_idxs)\n\n# Gathering single instance inputs to force into the training set after stratified split\none_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\none_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\none_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\none_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\n                                                            random_state=1, test_size=0.20, stratify = labels)\n\n# Add one frequency data to train data\ntrain_inputs.extend(one_freq_input_ids)\ntrain_labels.extend(one_freq_labels)\ntrain_masks.extend(one_freq_attention_masks)\ntrain_token_types.extend(one_freq_token_types)\n\n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\ntrain_token_types = torch.tensor(train_token_types)\n\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\nvalidation_token_types = torch.tensor(validation_token_types)\n\n# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntorch.save(validation_dataloader,'output')\ntorch.save(train_dataloader,'output')\n\n#BERT:\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) \n\n# Load model, the pretrained model will include a single linear classification layer on top for classification. \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\nmodel.cuda()\n\n# setting custom optimization parameters. You may implement a scheduler here as well.\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n# optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 2\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n    model.train()\n\n    # Tracking variables\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels, b_token_types = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n\n        # # Forward pass for multiclass classification\n        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        # loss = outputs[0]\n        # logits = outputs[1]\n\n        # Forward pass for multilabel classification\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        logits = outputs[0]\n        loss_func = BCEWithLogitsLoss() \n        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n        # loss_func = BCELoss() \n        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n        train_loss_set.append(loss.item())    \n\n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n        # scheduler.step()\n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n###############################################################################\n\n  # Validation\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n# Variables to gather full output\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(validation_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    with torch.no_grad():\n        # Forward pass\n        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        b_logit_pred = outs[0]\n        pred_label = torch.sigmoid(b_logit_pred)\n\n        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n        pred_label = pred_label.to('cpu').numpy()\n        b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n    \npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n\n# Calculate Accuracy\nthreshold = 0.50\npred_bools = [pl>threshold for pl in pred_labels]\ntrue_bools = [tl==1 for tl in true_labels]\nval_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\nval_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n\nprint('F1 Validation Accuracy: ', val_f1_accuracy)\nprint('Flat Validation Accuracy: ', val_flat_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:30:59.287601Z","iopub.execute_input":"2021-06-09T18:30:59.287916Z","iopub.status.idle":"2021-06-09T18:33:25.110295Z","shell.execute_reply.started":"2021-06-09T18:30:59.287887Z","shell.execute_reply":"2021-06-09T18:33:25.109334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation \n\ncols = test_df.columns\ntest_label_cols = list(cols[1:])\nprint(test_label_cols)\n\ntest_df['one_hot_labels'] = list(test_df[test_label_cols].values)\ntest_df.head(2)\n\n# Gathering input data\ntest_labels = list(test_df.one_hot_labels.values)\ntest_comments = list(test_df.comment.values)\n\n# Encoding input data\nmax_length=100\ntest_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\ntest_input_ids = test_encodings['input_ids']\ntest_token_type_ids = test_encodings['token_type_ids']\ntest_attention_masks = test_encodings['attention_mask']\n\n# Make tensors out of data\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\ntest_token_types = torch.tensor(test_token_type_ids)\n# Create test dataloader\ntest_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n# Save test dataloader\ntorch.save(test_dataloader,'test_data_loader')\n\n# Test\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n#track variables\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(test_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    with torch.no_grad():\n        # Forward pass\n        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n        b_logit_pred = outs[0]\n        pred_label = torch.sigmoid(b_logit_pred)\n\n        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n        pred_label = pred_label.to('cpu').numpy()\n        b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n\n# Flatten outputs\ntokenized_texts = [item for sublist in tokenized_texts for item in sublist]\npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n# Converting flattened binary values to boolean values\ntrue_bools = [tl==1 for tl in true_labels]\n\npred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n\n# Print and save classification report\nprint('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\nprint('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\nclf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\npickle.dump(clf_report, open('classification_report.txt','wb')) #save report\nprint(clf_report)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:33:39.15857Z","iopub.execute_input":"2021-06-09T18:33:39.15892Z","iopub.status.idle":"2021-06-09T18:33:47.242191Z","shell.execute_reply.started":"2021-06-09T18:33:39.15889Z","shell.execute_reply":"2021-06-09T18:33:47.240428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from simpletransformers.classification import MultiLabelClassificationModel","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:45:41.596509Z","iopub.execute_input":"2021-06-12T12:45:41.597198Z","iopub.status.idle":"2021-06-12T12:45:41.674423Z","shell.execute_reply.started":"2021-06-12T12:45:41.597078Z","shell.execute_reply":"2021-06-12T12:45:41.672604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}