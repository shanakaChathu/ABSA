{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aspect Prediction-Deep learning models_New",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saCX5Ugs1691",
        "outputId": "709e64a3-d8e7-4dec-b7dd-227a1912fe8a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCDsBBfd2Ab8",
        "outputId": "55b2be03-382c-4c93-cdf7-044b6ce11ca3"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import re\r\n",
        "from nltk.corpus import wordnet\r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import re\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from nltk.corpus import wordnet\r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import string\r\n",
        "!pip install nltk\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('treebank')\r\n",
        "from nltk.tag import UnigramTagger\r\n",
        "from nltk.corpus import treebank\r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import string\r\n",
        "from gensim.test.utils import common_texts, get_tmpfile\r\n",
        "from gensim.models import Word2Vec\r\n",
        "#import fasttext\r\n",
        "import gensim\r\n",
        "from gensim.test.utils import datapath\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "import seaborn as sns \r\n",
        "import re  # For preprocessing\r\n",
        "from time import time  # To time our operations\r\n",
        "from collections import defaultdict  # For word frequency\r\n",
        "import spacy  # For preprocessing\r\n",
        "import logging  # Setting up the loggings to monitor gensim\r\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk import pos_tag, word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from collections import Counter\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from xlwt import Workbook \r\n",
        "import wordcloud\r\n",
        "from wordcloud import WordCloud\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "import lightgbm as lgb\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "#from xgboost import XGBClassifier\r\n",
        "#import xgboost\r\n",
        "import scipy.sparse\r\n",
        "\r\n",
        "\r\n",
        "import time\r\n",
        "import warnings\r\n",
        "\r\n",
        "#settings\r\n",
        "start_time=time.time()\r\n",
        "color = sns.color_palette()\r\n",
        "sns.set_style(\"dark\")\r\n",
        "eng_stopwords = set(stopwords.words(\"english\"))\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "from wordcloud import WordCloud, STOPWORDS \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import pandas as pd \r\n",
        "\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "import os\r\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn import decomposition, ensemble\r\n",
        "from scipy import interp\r\n",
        "from itertools import cycle\r\n",
        "from sklearn.preprocessing import label_binarize\r\n",
        "from sklearn.metrics import roc_curve, auc\r\n",
        "from sklearn.metrics import precision_recall_curve\r\n",
        "from sklearn.metrics import average_precision_score\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import warnings\r\n",
        "import multiprocessing\r\n",
        "from gensim.models import Word2Vec\r\n",
        "#from gensim.models import FastText\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from numpy import asarray\r\n",
        "\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "import os\r\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn import decomposition, ensemble\r\n",
        "from scipy import interp\r\n",
        "from itertools import cycle\r\n",
        "from sklearn.preprocessing import label_binarize\r\n",
        "from sklearn.metrics import roc_curve, auc\r\n",
        "from sklearn.metrics import precision_recall_curve\r\n",
        "from sklearn.metrics import average_precision_score\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import warnings\r\n",
        "import multiprocessing\r\n",
        "from gensim.models import Word2Vec\r\n",
        "#from gensim.models import FastText\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.preprocessing import text, sequence\r\n",
        "from keras import layers, models, optimizers\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras import backend as K\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from keras.models import load_model\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "import matplotlib.pyplot as pyplot\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Activation, Dropout, Dense\r\n",
        "from keras.layers import Flatten, LSTM\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers.merge import Concatenate\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.preprocessing import text, sequence\r\n",
        "from keras import layers, models, optimizers\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from keras import backend as K\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from keras.models import load_model\r\n",
        "import tensorflow as tf \r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from numpy import array\r\n",
        "from numpy import asarray\r\n",
        "from numpy import zeros\r\n",
        "\r\n",
        "from keras import backend as K\r\n",
        "from keras.models import Sequential,Model,load_model\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.layers import Dropout, Activation, Flatten, \\\r\n",
        "    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\r\n",
        "    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D\r\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\r\n",
        "from keras.regularizers import l2, l1_l2\r\n",
        "from keras.constraints import maxnorm\r\n",
        "from keras import callbacks\r\n",
        "from keras.utils import generic_utils,plot_model\r\n",
        "from keras.optimizers import Adadelta\r\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.layers import Bidirectional\r\n",
        "\r\n",
        "\r\n",
        "#Capsule network related libraries \r\n",
        "\r\n",
        "import os\r\n",
        "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\r\n",
        "from keras.callbacks import Callback\r\n",
        "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\r\n",
        "from keras.preprocessing import text, sequence\r\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\r\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\r\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import Adam\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import multilabel_confusion_matrix\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m0cEvUfl2LGZ",
        "outputId": "fba90f62-63aa-4659-8d5a-970d6ad46863"
      },
      "source": [
        "mainloc='/content/drive/My Drive/Research/ABSA_Final/'\r\n",
        "mainloc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Research/ABSA_Final/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "CUADmvmi2P0M",
        "outputId": "497eadc3-7e14-4732-c9c5-e74dd91f6a57"
      },
      "source": [
        "df=pd.read_csv(mainloc+\"df_train.csv\")\r\n",
        "df=df[~df['cleanAnswer'].isna()]\r\n",
        "print(\"Size of training dataset: \",df.shape)\r\n",
        "\r\n",
        "df_we=pd.read_csv(mainloc+\"Data/data_clean_full_corpus.csv\")\r\n",
        "df_we=df_we[~df_we['cleanAnswer'].isna()]\r\n",
        "print(\"Size of word embedding dataset: \",df_we.shape)\r\n",
        "\r\n",
        "sent=df[['comment','cleanAnswer','cleanAnswer1','sentiment']]\r\n",
        "sent.head(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training dataset:  (10058, 15)\n",
            "Size of word embedding dataset:  (183718, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>cleanAnswer</th>\n",
              "      <th>cleanAnswer1</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Munge boru math damma package eka danna kalin ...</td>\n",
              "      <td>munge boru math damma package danna kalin 4g a...</td>\n",
              "      <td>munge boru math damma package danna kalin 4g a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dialog Axiata night time inne it8n a tika day ...</td>\n",
              "      <td>axiata night time inne it8n tika day time pawi...</td>\n",
              "      <td>axiata night time inne it8n tika day time pawi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  ... sentiment\n",
              "0  Munge boru math damma package eka danna kalin ...  ...         0\n",
              "1  Dialog Axiata night time inne it8n a tika day ...  ...         0\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZy3cir3zDU"
      },
      "source": [
        "#Functions \r\n",
        "\r\n",
        "def encodeY(y,classes):\r\n",
        "    encoder = preprocessing.LabelEncoder()\r\n",
        "    y_c = encoder.fit_transform(y)\r\n",
        "    y_c = label_binarize(y_c, classes=classes)\r\n",
        "    return y_c\r\n",
        "\r\n",
        "def emb_dictionary(filename):\r\n",
        "    embedding_dictionary = dict()\r\n",
        "    f = open(filename)\r\n",
        "    for line in f:\r\n",
        "        values = line.split()\r\n",
        "        word = values[0]\r\n",
        "        coefs = asarray(values[1:], dtype='float32')\r\n",
        "        embedding_dictionary[word] = coefs\r\n",
        "    f.close()\r\n",
        "    return embedding_dictionary\r\n",
        "\r\n",
        "def emb_matrix(vocab_length,word_tokenizer,embeddings_dictionary,we_size):\r\n",
        "    embedding_matrix = zeros((vocab_length, we_size))\r\n",
        "    for word, index in word_tokenizer.word_index.items():\r\n",
        "        embedding_vector = embeddings_dictionary.get(word)\r\n",
        "        if embedding_vector is not None:\r\n",
        "            embedding_matrix[index] = embedding_vector\r\n",
        "    return  embedding_matrix\r\n",
        "\r\n",
        "def get_emb_sentences(X):\r\n",
        "    word_tokenizer = Tokenizer()\r\n",
        "    word_tokenizer.fit_on_texts(x)\r\n",
        "    vocab_length = len(word_tokenizer.word_index) + 1\r\n",
        "    embedded_sentences = word_tokenizer.texts_to_sequences(x)\r\n",
        "    return word_tokenizer,embedded_sentences,vocab_length\r\n",
        "\r\n",
        "def get_padded_sequence(x,embedded_sentences):\r\n",
        "    word_count = lambda sentence: len(word_tokenize(sentence))\r\n",
        "    longest_sentence = max(x, key=word_count)\r\n",
        "    length_long_sentence = len(word_tokenize(longest_sentence))\r\n",
        "    padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\r\n",
        "    return length_long_sentence,padded_sentences \r\n",
        "\r\n",
        "def loadembedding_matrix(x,loc,emb_size):\r\n",
        "  word_tokenizer,embedded_sentences,vocab_length=get_emb_sentences(x)\r\n",
        "  length_long_sentence,padded_sentences = get_padded_sequence(x,embedded_sentences)\r\n",
        "  embeddings_dictionary=emb_dictionary(loc)\r\n",
        "  embedding_matrix=emb_matrix(vocab_length,word_tokenizer,embeddings_dictionary,emb_size)\r\n",
        "  return vocab_length,padded_sentences,length_long_sentence,embedding_matrix\r\n",
        "    \r\n",
        "def recall_m(y_true, y_pred):\r\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "  return recall\r\n",
        "\r\n",
        "def precision_m(y_true, y_pred):\r\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "  return precision\r\n",
        "\r\n",
        "def f1_m(y_true, y_pred):\r\n",
        "  precision = precision_m(y_true, y_pred)\r\n",
        "  recall = recall_m(y_true, y_pred)\r\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\r\n",
        "\r\n",
        "\r\n",
        "#Hold out method specific functions \r\n",
        "def splitdata(x,y,splitratio,valflag):\r\n",
        "    x_train_main, x_test, y_train_main, y_test = train_test_split(x, y, test_size=splitratio, random_state=10, stratify=y)\r\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train_main, y_train_main, test_size=splitratio, random_state=10, \r\n",
        "                                                      stratify=y_train_main)\r\n",
        "    encoder = preprocessing.LabelEncoder()\r\n",
        "    y_train_main = encoder.fit_transform(y_train_main)\r\n",
        "    y_train = encoder.fit_transform(y_train)\r\n",
        "    y_val = encoder.fit_transform(y_val)\r\n",
        "    y_test = encoder.fit_transform(y_test)\r\n",
        "    \r\n",
        "    y_train_main_c = label_binarize(y_train_main, classes=[0,1,2])\r\n",
        "    y_train_c = label_binarize(y_train, classes=[0,1,2])\r\n",
        "    y_val_c = label_binarize(y_val, classes=[0,1,2])\r\n",
        "    y_test_c = label_binarize(y_test, classes=[0,1,2])\r\n",
        "    \r\n",
        "    if(valflag):\r\n",
        "        return x_train,x_val,x_test,y_train,y_val, y_test,y_train_c,y_val_c,y_test_c \r\n",
        "    else:\r\n",
        "        return x_train_main,x_test,y_train_main, y_test, y_train_main_c,y_test_c\r\n",
        "\r\n",
        "def train_model(model,x_train, y_train):\r\n",
        "    print('Model Training')\r\n",
        "    es = EarlyStopping(monitor=MONITOR, mode=MONITOR_MODE, verbose=1, patience=PATIENCE)\r\n",
        "    mc = ModelCheckpoint(MODEL_SAVE_PATH, monitor=MONITOR, verbose=1, save_best_only=True, mode=MONITOR_MODE)\r\n",
        "    callbacks_list = [es,mc]\r\n",
        "    history = model.fit(x_train, y_train, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE,callbacks=callbacks_list, verbose=1)\r\n",
        "    return model,history \r\n",
        "\r\n",
        "def pred_neural(clf, feature_vector_valid,multiflag):\r\n",
        "    if (multiflag):\r\n",
        "      predictions_prob = clf.predict(feature_vector_valid)\r\n",
        "      predictions_prob_ = predictions_prob >= 0.5\r\n",
        "      predictions = predictions_prob_.astype(int)\r\n",
        "    else: \r\n",
        "      predictions = clf.predict(feature_vector_valid).argmax(axis=1)\r\n",
        "      predictions_prob = clf.predict(feature_vector_valid)\r\n",
        "    return predictions, predictions_prob    \r\n",
        "    \r\n",
        "def accuracy_neural(y_val,y_pred,avg_method,target_names):\r\n",
        "    print(\"Accuracy Score: \",metrics.accuracy_score(y_val,y_pred))\r\n",
        "    print(\"Precision: \", metrics.precision_score(y_val,y_pred,average=avg_method))\r\n",
        "    print(\"Recall: \", metrics.recall_score(y_val,y_pred,average=avg_method))\r\n",
        "    print(\"F1 score: \", metrics.f1_score(y_val,y_pred,average=avg_method))\r\n",
        "    \r\n",
        "    print(\"\\nClassification report\")\r\n",
        "    print(\"---------------------\")\r\n",
        "    print(metrics.classification_report(y_val, y_pred,target_names=target_names))\r\n",
        "\r\n",
        "def learning_curve(history,measure,title):\r\n",
        "    epochs=range(len(history.history[measure]))\r\n",
        "    plt.plot(epochs,history.history[measure], 'r')\r\n",
        "    plt.plot(epochs,history.history['val_'+measure], 'b')\r\n",
        "    plt.title('Training and validation '+title)\r\n",
        "    plt.xlabel(\"Epochs\")\r\n",
        "    plt.ylabel(title)\r\n",
        "    plt.legend([\"Training \"+measure, \"Validation \"+measure])\r\n",
        "    plt.figure()\r\n",
        "\r\n",
        "def plot_confusion_matrix(cm,target_names, title='Confusion matrix', cmap=None, normalize=True):\r\n",
        "    \"\"\"\r\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\r\n",
        "\r\n",
        "    Arguments\r\n",
        "    ---------\r\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\r\n",
        "\r\n",
        "    target_names: given classification classes such as [0, 1, 2]\r\n",
        "                  the class names, for example: ['high', 'medium', 'low']\r\n",
        "\r\n",
        "    title:        the text to display at the top of the matrix\r\n",
        "\r\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\r\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\r\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\r\n",
        "\r\n",
        "    normalize:    If False, plot the raw numbers\r\n",
        "                  If True, plot the proportions\r\n",
        "\r\n",
        "    Usage\r\n",
        "    -----\r\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\r\n",
        "                                                              # sklearn.metrics.confusion_matrix\r\n",
        "                          normalize    = True,                # show proportions\r\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\r\n",
        "                          title        = best_estimator_name) # title of graph\r\n",
        "\r\n",
        "    Citiation\r\n",
        "    ---------\r\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    import matplotlib.pyplot as plt\r\n",
        "    import numpy as np\r\n",
        "    import itertools\r\n",
        "\r\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\r\n",
        "    misclass = 1 - accuracy\r\n",
        "\r\n",
        "    if cmap is None:\r\n",
        "        cmap = plt.get_cmap('Blues')\r\n",
        "\r\n",
        "    plt.figure(figsize=(8, 6))\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "    plt.title(title)\r\n",
        "    plt.colorbar()\r\n",
        "\r\n",
        "    if target_names is not None:\r\n",
        "        tick_marks = np.arange(len(target_names))\r\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\r\n",
        "        plt.yticks(tick_marks, target_names)\r\n",
        "\r\n",
        "    if normalize:\r\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "\r\n",
        "\r\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        if normalize:\r\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\r\n",
        "                     horizontalalignment=\"center\",\r\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "        else:\r\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\r\n",
        "                     horizontalalignment=\"center\",\r\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('True label')\r\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\r\n",
        "    plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4UIsXua7DIm"
      },
      "source": [
        "#Neural network models \r\n",
        "\r\n",
        "def cnn_model():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Conv1D(256,3, activation='relu'))\r\n",
        "    model.add(GlobalMaxPooling1D())\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(NUM_CLASSES, activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model \r\n",
        "\r\n",
        "def gru_model():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(GRU(HIDDEN_DIMS))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model \r\n",
        "\r\n",
        "def lstm_model():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(LSTM(HIDDEN_DIMS))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model\r\n",
        "\r\n",
        "def simplernn_model():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(SimpleRNN(HIDDEN_DIMS))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model \r\n",
        "\r\n",
        "def stack_lstm2():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(LSTM(HIDDEN_DIMS,return_sequences=True))\r\n",
        "    model.add(LSTM(HIDDEN_DIMS))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model \r\n",
        "\r\n",
        "def stack_lstm3():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(LSTM(HIDDEN_DIMS,return_sequences=True))\r\n",
        "    model.add(LSTM(HIDDEN_DIMS,return_sequences=True))\r\n",
        "    model.add(LSTM(HIDDEN_DIMS))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model\r\n",
        "\r\n",
        "def bilstm():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(Bidirectional(LSTM(HIDDEN_DIMS)))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model\r\n",
        "\r\n",
        "def stacked_bilstm2():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(Bidirectional(LSTM(HIDDEN_DIMS,return_sequences=True)))\r\n",
        "    model.add(Bidirectional(LSTM(HIDDEN_DIMS)))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model\r\n",
        "\r\n",
        "def stacked_bilstm3():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_1))\r\n",
        "    model.add(Bidirectional(LSTM(HIDDEN_DIMS,return_sequences=True)))\r\n",
        "    model.add(Bidirectional(LSTM(HIDDEN_DIMS,return_sequences=True)))\r\n",
        "    model.add(Bidirectional(LSTM(HIDDEN_DIMS)))\r\n",
        "    model.add(Dense(HIDDEN_DIMS, activation='relu'))\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model\r\n",
        "\r\n",
        "def cnn_bilstm_model():\r\n",
        "  model = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "  model.add(embedding_layer)\r\n",
        "  model.add(Conv1D(filters=NB_FILTERS,kernel_size=KERNEL_SIZE, padding='same', activation='relu'))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Bidirectional(LSTM(HIDDEN_DIMS)))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "  model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "  return model \r\n",
        "\r\n",
        "def cnn_stackedbilstm2_model():\r\n",
        "  model = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "  model.add(embedding_layer)\r\n",
        "  model.add(Conv1D(filters=NB_FILTERS,kernel_size=KERNEL_SIZE, padding='same', activation='relu'))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Bidirectional(LSTM(HIDDEN_DIMS,return_sequences=True)))\r\n",
        "  model.add(Bidirectional(LSTM(HIDDEN_DIMS)))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "  model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "  return model \r\n",
        "\r\n",
        "def cnn_stackedbilstm3_model():\r\n",
        "  model = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "  model.add(embedding_layer)\r\n",
        "  model.add(Conv1D(filters=NB_FILTERS,kernel_size=KERNEL_SIZE, padding='same', activation='relu'))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Bidirectional(LSTM(HIDDEN_DIMS,return_sequences=True)))\r\n",
        "  model.add(Bidirectional(LSTM(HIDDEN_DIMS,return_sequences=True)))\r\n",
        "  model.add(Bidirectional(LSTM(HIDDEN_DIMS)))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "  model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "  return model \r\n",
        "\r\n",
        "def cnn_gru_model():\r\n",
        "  model = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "  model.add(embedding_layer)\r\n",
        "  model.add(Conv1D(filters=NB_FILTERS,kernel_size=KERNEL_SIZE, padding='same', activation='relu'))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(GRU(HIDDEN_DIMS,return_sequences=True))\r\n",
        "  model.add(GRU(HIDDEN_DIMS))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "  model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "  return model \r\n",
        "\r\n",
        "def cnn_lstm_model():\r\n",
        "  model = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\r\n",
        "  model.add(embedding_layer)\r\n",
        "  model.add(Conv1D(filters=NB_FILTERS,kernel_size=KERNEL_SIZE, padding='same', activation='relu'))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(LSTM(HIDDEN_DIMS,return_sequences=True))\r\n",
        "  model.add(GRU(HIDDEN_DIMS))\r\n",
        "  model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "  model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "  model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "  return model \r\n",
        "\r\n",
        "\r\n",
        "#Capsule network \r\n",
        "\r\n",
        "class RocAucEvaluation(Callback):\r\n",
        "    def __init__(self, validation_data=(), interval=1):\r\n",
        "        super(Callback, self).__init__()\r\n",
        "\r\n",
        "        self.interval = interval\r\n",
        "        self.X_val, self.y_val = validation_data\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if epoch % self.interval == 0:\r\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\r\n",
        "            score = roc_auc_score(self.y_val, y_pred)\r\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\r\n",
        "\r\n",
        "from keras.engine import Layer\r\n",
        "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\r\n",
        "gru_len = 128\r\n",
        "Routings = 5\r\n",
        "Num_capsule = 10\r\n",
        "Dim_capsule = 16\r\n",
        "dropout_p = 0.25\r\n",
        "rate_drop_dense = 0.28\r\n",
        "\r\n",
        "def squash(x, axis=-1):\r\n",
        "    # s_squared_norm is really small\r\n",
        "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\r\n",
        "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\r\n",
        "    # return scale * x\r\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\r\n",
        "    scale = K.sqrt(s_squared_norm + K.epsilon())\r\n",
        "    return x / scale\r\n",
        "\r\n",
        "\r\n",
        "# A Capsule Implement with Pure Keras\r\n",
        "class Capsule(Layer):\r\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\r\n",
        "                 activation='default', **kwargs):\r\n",
        "        super(Capsule, self).__init__(**kwargs)\r\n",
        "        self.num_capsule = num_capsule\r\n",
        "        self.dim_capsule = dim_capsule\r\n",
        "        self.routings = routings\r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.share_weights = share_weights\r\n",
        "        if activation == 'default':\r\n",
        "            self.activation = squash\r\n",
        "        else:\r\n",
        "            self.activation = Activation(activation)\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        super(Capsule, self).build(input_shape)\r\n",
        "        input_dim_capsule = input_shape[-1]\r\n",
        "        if self.share_weights:\r\n",
        "            self.W = self.add_weight(name='capsule_kernel',\r\n",
        "                                     shape=(1, input_dim_capsule,\r\n",
        "                                            self.num_capsule * self.dim_capsule),\r\n",
        "                                     # shape=self.kernel_size,\r\n",
        "                                     initializer='glorot_uniform',\r\n",
        "                                     trainable=True)\r\n",
        "        else:\r\n",
        "            input_num_capsule = input_shape[-2]\r\n",
        "            self.W = self.add_weight(name='capsule_kernel',\r\n",
        "                                     shape=(input_num_capsule,\r\n",
        "                                            input_dim_capsule,\r\n",
        "                                            self.num_capsule * self.dim_capsule),\r\n",
        "                                     initializer='glorot_uniform',\r\n",
        "                                     trainable=True)\r\n",
        "\r\n",
        "    def call(self, u_vecs):\r\n",
        "        if self.share_weights:\r\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\r\n",
        "        else:\r\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\r\n",
        "\r\n",
        "        batch_size = K.shape(u_vecs)[0]\r\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\r\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\r\n",
        "                                            self.num_capsule, self.dim_capsule))\r\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\r\n",
        "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\r\n",
        "\r\n",
        "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\r\n",
        "        for i in range(self.routings):\r\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\r\n",
        "            c = K.softmax(b)\r\n",
        "            c = K.permute_dimensions(c, (0, 2, 1))\r\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))\r\n",
        "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\r\n",
        "            #if i < self.routings - 1:\r\n",
        "            #    b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return (None, self.num_capsule, self.dim_capsule)\r\n",
        "\r\n",
        "def capsule_network():\r\n",
        "    model = Sequential()\r\n",
        "    print(1)\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(SpatialDropout1D(rate_drop_dense))\r\n",
        "    model.add(Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True)))\r\n",
        "    model.add(Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dropout(dropout_p))\r\n",
        "    model.add(Dense(NUM_CLASSES,activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])    \r\n",
        "    model.summary()\r\n",
        "    return model \r\n",
        "\r\n",
        "class AttentionWithContext(Layer):\r\n",
        "    \"\"\"\r\n",
        "    Attention operation, with a context/query vector, for temporal data.\r\n",
        "    Supports Masking.\r\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\r\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\r\n",
        "    by using a context vector to assist the attention\r\n",
        "    # Input shape\r\n",
        "        3D tensor with shape: `(samples, steps, features)`.\r\n",
        "    # Output shape\r\n",
        "        2D tensor with shape: `(samples, features)`.\r\n",
        "    How to use:\r\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\r\n",
        "    The dimensions are inferred based on the output shape of the RNN.\r\n",
        "    Note: The layer has been tested with Keras 2.0.6\r\n",
        "    Example:\r\n",
        "        model.add(LSTM(64, return_sequences=True))\r\n",
        "        model.add(AttentionWithContext())\r\n",
        "        # next add a Dense layer (for classification/regression) or whatever...\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\r\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\r\n",
        "                 bias=True, **kwargs):\r\n",
        "\r\n",
        "        self.supports_masking = True\r\n",
        "        self.init = initializers.get('glorot_uniform')\r\n",
        "\r\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\r\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\r\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\r\n",
        "\r\n",
        "        self.W_constraint = constraints.get(W_constraint)\r\n",
        "        self.u_constraint = constraints.get(u_constraint)\r\n",
        "        self.b_constraint = constraints.get(b_constraint)\r\n",
        "\r\n",
        "        self.bias = bias\r\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        assert len(input_shape) == 3\r\n",
        "\r\n",
        "        self.W = self.add_weight((input_shape[-1], ),\r\n",
        "                                 initializer=self.init,\r\n",
        "                                 name='{}_W'.format(self.name),\r\n",
        "                                 regularizer=self.W_regularizer,\r\n",
        "                                 constraint=self.W_constraint)\r\n",
        "        if self.bias:\r\n",
        "            self.b = self.add_weight((input_shape[-1],),\r\n",
        "                                     initializer='zero',\r\n",
        "                                     name='{}_b'.format(self.name),\r\n",
        "                                     regularizer=self.b_regularizer,\r\n",
        "                                     constraint=self.b_constraint)\r\n",
        "\r\n",
        "        self.u = self.add_weight((input_shape[-1],),\r\n",
        "                                 initializer=self.init,\r\n",
        "                                 name='{}_u'.format(self.name),\r\n",
        "                                 regularizer=self.u_regularizer,\r\n",
        "                                 constraint=self.u_constraint)\r\n",
        "\r\n",
        "        super(AttentionWithContext, self).build(input_shape)\r\n",
        "\r\n",
        "    def compute_mask(self, input, input_mask=None):\r\n",
        "        # do not pass the mask to the next layers\r\n",
        "        return None\r\n",
        "\r\n",
        "    def call(self, x, mask=None):\r\n",
        "        uit = dot_product(x, self.W)\r\n",
        "\r\n",
        "        if self.bias:\r\n",
        "            uit += self.b\r\n",
        "\r\n",
        "        uit = K.tanh(uit)\r\n",
        "        ait = dot_product(uit, self.u)\r\n",
        "\r\n",
        "        a = K.exp(ait)\r\n",
        "\r\n",
        "        # apply mask after the exp. will be re-normalized next\r\n",
        "        if mask is not None:\r\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\r\n",
        "            a *= K.cast(mask, K.floatx())\r\n",
        "\r\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\r\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number  to the sum.\r\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\r\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\r\n",
        "\r\n",
        "        a = K.expand_dims(a)\r\n",
        "        weighted_input = x * a\r\n",
        "        return K.sum(weighted_input, axis=1)\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVovZCSu2btV"
      },
      "source": [
        "#Extract X and Y \r\n",
        "\r\n",
        "categories=['network','billing_price','package','customer_service','data','service_product']   \r\n",
        "x=df['cleanAnswer']\r\n",
        "y=df[categories]\r\n",
        "y_c=y.values\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKKJ0mDo2cZs"
      },
      "source": [
        "#Compare all deep learing models with cross validation method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuzJgimt8zJF"
      },
      "source": [
        "def crossval(model_name):\r\n",
        "\r\n",
        "  # Define per-fold score containers <-- these are new\r\n",
        "  acc_per_fold = []\r\n",
        "  loss_per_fold = []\r\n",
        "  f1_per_fold = []\r\n",
        "  precision_per_fold =[]\r\n",
        "  recall_per_fold=[]\r\n",
        "\r\n",
        "  kf = KFold(n_splits=N_FOLDS)\r\n",
        "  fold_no = 1\r\n",
        "  for train_index, test_index in kf.split(padded_sentences,y):\r\n",
        "    if model_name=='cnn':\r\n",
        "      model=cnn_model()\r\n",
        "    elif model_name=='gru':\r\n",
        "      model=gru_model()\r\n",
        "    elif model_name=='lstm':\r\n",
        "      model=lstm_model()\r\n",
        "    elif model_name=='stacklstm2':\r\n",
        "      model=stack_lstm2()\r\n",
        "    elif model_name=='stacklstm3':\r\n",
        "      model=stack_lstm3()\r\n",
        "    elif model_name=='bilstm':\r\n",
        "      model=bilstm()\r\n",
        "    elif model_name=='stackbilstm2':\r\n",
        "      model=stacked_bilstm2()\r\n",
        "    elif model_name=='stackbilstm3':\r\n",
        "      model=stacked_bilstm3()\r\n",
        "    elif model_name=='cnnbilstm':\r\n",
        "      model=cnn_bilstm_model()\r\n",
        "    elif model_name=='cnnstackbilstm2':\r\n",
        "      model=cnn_stackedbilstm2_model()\r\n",
        "    elif model_name=='cnnstackbilstm3':\r\n",
        "      model=cnn_stackedbilstm3_model()\r\n",
        "    elif model_name=='simplernn':\r\n",
        "      model=simplernn_model()\r\n",
        "    elif model_name=='cnn_gru':\r\n",
        "      model=cnn_gru_model()\r\n",
        "    elif model_name=='cnn_lstm':\r\n",
        "      model=cnn_lstm_model()\r\n",
        "    elif model_name=='capsule':\r\n",
        "      model=capsule_network()\r\n",
        "\r\n",
        "    es = EarlyStopping(monitor=MONITOR, mode=MONITOR_MODE, verbose=1,patience= PATIENCE )\r\n",
        "    history = model.fit(padded_sentences[train_index],y_c[train_index],epochs=EPOCHS,callbacks=[es],batch_size=BATCH_SIZE,shuffle=SHUFFLE,validation_split=VALIDATION_SPLIT)\r\n",
        "    \r\n",
        "    # Generate generalization metrics\r\n",
        "    scores = model.evaluate(padded_sentences[test_index], y_c[test_index], verbose=0)\r\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% ;{model.metrics_names[2]} of {scores[2]} ;{model.metrics_names[3]} of {scores[3]} ; \\\r\n",
        "           {model.metrics_names[4]} of {scores[4]}')\r\n",
        "    loss_per_fold.append(scores[0])\r\n",
        "    acc_per_fold.append(scores[1] * 100)\r\n",
        "    precision_per_fold.append(scores[2])\r\n",
        "    recall_per_fold.append(scores[3])\r\n",
        "    f1_per_fold.append(scores[4])\r\n",
        "\r\n",
        "    # Increase fold number\r\n",
        "    fold_no = fold_no + 1\r\n",
        "\r\n",
        "  acc=np.mean(acc_per_fold)\r\n",
        "  precision=np.mean(precision_per_fold)\r\n",
        "  recall=np.mean(recall_per_fold)\r\n",
        "  f1=np.mean(f1_per_fold)\r\n",
        "  loss=np.mean(loss_per_fold)\r\n",
        "  return acc,precision,recall,f1,loss\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "syvGLr2w2tIP",
        "outputId": "0c44be75-5d47-4c8f-e212-3e2dfa475ba7"
      },
      "source": [
        "EMBEDDING_SIZE=400\r\n",
        "WE_TYPE='cbow'\r\n",
        "NUM_CLASSES=6\r\n",
        "NB_FILTERS=200 \r\n",
        "KERNEL_SIZE=5\r\n",
        "SHUFFLE=True\r\n",
        "HIDDEN_DIMS=NB_FILTERS*2\r\n",
        "DROPOUT_VALUE_1=0.5 \r\n",
        "DROPOUT_VALUE_2=0.5 \r\n",
        "L2_REG=0.1\r\n",
        "N_FOLDS=2\r\n",
        "MONITOR='val_f1_m' \r\n",
        "MONITOR_MODE='max' \r\n",
        "PATIENCE=10\r\n",
        "EPOCHS=2\r\n",
        "VALIDATION_SPLIT=0.2 \r\n",
        "SHUFFLE=True \r\n",
        "BATCH_SIZE=32 \r\n",
        "activation='sigmoid'\r\n",
        "loss='binary_crossentropy'\r\n",
        "\r\n",
        "MODELS_LIST=['cnn','gru','lstm','simplernn','stacklstm2','stacklstm3','bilstm','stackbilstm2','stackbilstm3','cnnbilstm','cnnstackbilstm2','cnnstackbilstm3','cnn_gru','cnn_lstm']\r\n",
        "MODELS_LIST=['cnn']\r\n",
        "\r\n",
        "acclist=[] ; prelist=[]  ; reclist=[]  ; f1list=[]  ; losslist=[] ; modelname=[] ; wename=[] ; wesize = []\r\n",
        "\r\n",
        "for mod in MODELS_LIST:\r\n",
        "  we_path=mainloc+'word_embeddings/w2v_'+WE_TYPE+'_'+str(EMBEDDING_SIZE)+'.txt'\r\n",
        "  vocab_length,padded_sentences,length_long_sentence,embedding_matrix=loadembedding_matrix(x,we_path,EMBEDDING_SIZE)\r\n",
        "  if (mod=='cnn'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('cnn')\r\n",
        "  elif (mod=='gru'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('gru')\r\n",
        "  elif (mod=='lstm'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('lstm')\r\n",
        "  elif (mod=='stacklstm2'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('stacklstm2')\r\n",
        "  elif (mod=='stacklstm3'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('stacklstm3')\r\n",
        "  elif (mod=='bilstm'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('bilstm')\r\n",
        "  elif (mod=='stackbilstm2'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('stackbilstm2')\r\n",
        "  elif (mod=='stackbilstm3'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('stackbilstm3')\r\n",
        "  elif (mod=='cnnbilstm'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('cnnbilstm')\r\n",
        "  elif (mod=='cnnstackbilstm2'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('cnnstackbilstm2')\r\n",
        "  elif (mod=='cnnstackbilstm3'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('cnnstackbilstm3')\r\n",
        "  elif (mod=='simplernn'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('simplernn')\r\n",
        "  elif (mod=='cnn_gru'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('cnn_gru')\r\n",
        "  elif (mod=='cnn_lstm'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('cnn_lstm')\r\n",
        "  elif (mod=='capsule'):\r\n",
        "    acc,precision,recall,f1,loss=crossval('capsule')\r\n",
        "  else: \r\n",
        "     print(\"Inputed model is not available\")\r\n",
        " \r\n",
        "  modelname.append(mod)\r\n",
        "  wename.append(WE_TYPE)\r\n",
        "  wesize.append(EMBEDDING_SIZE)\r\n",
        "  acclist.append(acc)\r\n",
        "  prelist.append(precision)\r\n",
        "  reclist.append(recall)\r\n",
        "  f1list.append(f1)\r\n",
        "  losslist.append(loss)        \r\n",
        "  \r\n",
        "  accdf=pd.DataFrame({'Model':modelname,'Word_Embedding':wename, 'Embedding_Size':wesize,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\r\n",
        "  #accdf.to_csv(mainloc+\"Sentiment_Accuracy_.csv\",index=False)\r\n",
        "\r\n",
        "accdf=pd.DataFrame({'Model':modelname,'Word_Embedding':wename, 'Embedding_Size':wesize,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\r\n",
        "accdf.to_csv(mainloc+\"Capsule_accuracy.csv\",index=False)\r\n",
        "accdf"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "126/126 [==============================] - 32s 247ms/step - loss: 0.4497 - accuracy: 0.4527 - precision_m: 0.6539 - recall_m: 0.4153 - f1_m: 0.4979 - val_loss: 0.2721 - val_accuracy: 0.6909 - val_precision_m: 0.8443 - val_recall_m: 0.7372 - val_f1_m: 0.7851\n",
            "Epoch 2/2\n",
            "126/126 [==============================] - 31s 243ms/step - loss: 0.3066 - accuracy: 0.6200 - precision_m: 0.7994 - recall_m: 0.6717 - f1_m: 0.7285 - val_loss: 0.2458 - val_accuracy: 0.7038 - val_precision_m: 0.8566 - val_recall_m: 0.7530 - val_f1_m: 0.7998\n",
            "Score for fold 1: loss of 0.43693414330482483; accuracy of 52.714258432388306% ;precision_m of 0.8468090891838074 ;recall_m of 0.5738110542297363 ;            f1_m of 0.6765560507774353\n",
            "Epoch 1/2\n",
            "126/126 [==============================] - 32s 246ms/step - loss: 0.5519 - accuracy: 0.3563 - precision_m: 0.6089 - recall_m: 0.3836 - f1_m: 0.4569 - val_loss: 0.3380 - val_accuracy: 0.6332 - val_precision_m: 0.7672 - val_recall_m: 0.7300 - val_f1_m: 0.7462\n",
            "Epoch 2/2\n",
            "126/126 [==============================] - 30s 242ms/step - loss: 0.3737 - accuracy: 0.5555 - precision_m: 0.7951 - recall_m: 0.6888 - f1_m: 0.7364 - val_loss: 0.3110 - val_accuracy: 0.6352 - val_precision_m: 0.8094 - val_recall_m: 0.7449 - val_f1_m: 0.7739\n",
            "Score for fold 2: loss of 0.2992106080055237; accuracy of 67.07099080085754% ;precision_m of 0.8147143721580505 ;recall_m of 0.7473100423812866 ;            f1_m of 0.7776970267295837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Word_Embedding</th>\n",
              "      <th>Embedding_Size</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cnn</td>\n",
              "      <td>cbow</td>\n",
              "      <td>400</td>\n",
              "      <td>59.892625</td>\n",
              "      <td>0.830762</td>\n",
              "      <td>0.660561</td>\n",
              "      <td>0.727127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model Word_Embedding  Embedding_Size  ...  Precision    Recall        F1\n",
              "0   cnn           cbow             400  ...   0.830762  0.660561  0.727127\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD-VFWEzEOsn"
      },
      "source": [
        "# Training Individual Model(Best Model) with holdout method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iyfK39xDhzz",
        "outputId": "607d471e-f363-46da-c8f6-4388b4a7324f"
      },
      "source": [
        "#CNN model is going to evalute with holdout method \r\n",
        "\r\n",
        "EMBEDDING_SIZE=400\r\n",
        "WE_TYPE='cbow'\r\n",
        "NUM_CLASSES=6\r\n",
        "NB_FILTERS=200 \r\n",
        "KERNEL_SIZE=5\r\n",
        "SHUFFLE=True\r\n",
        "HIDDEN_DIMS=NB_FILTERS*2\r\n",
        "DROPOUT_VALUE_1=0.5 \r\n",
        "DROPOUT_VALUE_2=0.5 \r\n",
        "L2_REG=0.1\r\n",
        "MONITOR='val_f1_m' \r\n",
        "MONITOR_MODE='max' \r\n",
        "PATIENCE=10\r\n",
        "EPOCHS=2\r\n",
        "VALIDATION_SPLIT=0.2 \r\n",
        "SHUFFLE=True \r\n",
        "BATCH_SIZE=32\r\n",
        "MODEL_SAVE_PATH=mainloc+'best_model.h5'\r\n",
        "TARGET_NAMES=['network','billing_price','package','customer_service','data','service_product']\r\n",
        "activation='sigmoid'\r\n",
        "loss='binary_crossentropy'\r\n",
        "\r\n",
        "\r\n",
        "#Define dependencied for model loading\r\n",
        "dependencies = {\r\n",
        "    'recall_m': recall_m,\r\n",
        "    'f1_m':f1_m,\r\n",
        "    'precision_m':precision_m    \r\n",
        "}\r\n",
        "        \r\n",
        "def cnn_model():\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(vocab_length,EMBEDDING_SIZE,weights=[embedding_matrix],input_length=length_long_sentence, trainable=False)\r\n",
        "    model.add(embedding_layer)\r\n",
        "    model.add(Conv1D(256,3, activation='relu'))\r\n",
        "    model.add(GlobalMaxPooling1D())\r\n",
        "    model.add(Dropout(DROPOUT_VALUE_2))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(NUM_CLASSES, activation=activation))\r\n",
        "    model.compile(loss=loss, optimizer='adam',metrics=[\"accuracy\",precision_m,recall_m,f1_m])\r\n",
        "    return model \r\n",
        "\r\n",
        "def splitdata(x,y,splitratio,valflag):\r\n",
        "    x_train_main, x_test, y_train_main, y_test = train_test_split(x, y, test_size=splitratio, random_state=100)\r\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train_main, y_train_main, test_size=splitratio, random_state=100)\r\n",
        "\r\n",
        "    y_train_main_c=y_train_main.values\r\n",
        "    y_test_c=y_test.values\r\n",
        "    y_train_c=y_train.values\r\n",
        "    y_val_c=y_val.values\r\n",
        "    \r\n",
        "    if(valflag):\r\n",
        "        return  x_train,x_val,x_test,y_train,y_val, y_test,y_train_c,y_val_c, y_test_c\r\n",
        "    else:\r\n",
        "        return  x_train_main,x_test,y_train_main, y_test,y_train_main_c, y_test_c\r\n",
        "\r\n",
        "def train_model(model,x_train, y_train):\r\n",
        "    print('Model Training')\r\n",
        "    #es = EarlyStopping(monitor=MONITOR, mode=MONITOR_MODE, verbose=1, patience=PATIENCE)\r\n",
        "    #mc = ModelCheckpoint(MODEL_SAVE_PATH, monitor=MONITOR, verbose=1, save_best_only=True, mode=MONITOR_MODE)\r\n",
        "    #callbacks_list = [es,mc]\r\n",
        "    #history = model.fit(x_train, y_train, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE,callbacks=callbacks_list, verbose=1)\r\n",
        "    history = model.fit(x_train, y_train, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\r\n",
        "    return model,history \r\n",
        "\r\n",
        "we_path=mainloc+'word_embeddings/w2v_'+WE_TYPE+'_'+str(EMBEDDING_SIZE)+'.txt'\r\n",
        "vocab_length,padded_sentences,length_long_sentence,embedding_matrix=loadembedding_matrix(x,we_path,EMBEDDING_SIZE)\r\n",
        "x_train,x_test,y_train,y_test,y_train_c,y_test_c = splitdata(padded_sentences,y,0.2,False)\r\n",
        "#model=cnn_model()\r\n",
        "model=capsule_network()\r\n",
        "model,history=train_model(model,x_train,y_train_c) \r\n",
        "\r\n",
        "print(\"\\nTraining accuracy\")\r\n",
        "print(\"--------------------------------------------\")\r\n",
        "predictions, predictions_prob  = pred_neural(model,x_train,True)\r\n",
        "accuracy_neural(y_train,predictions,'weighted',TARGET_NAMES)\r\n",
        "\r\n",
        "print(\"\\n Testing accuracy\")\r\n",
        "print(\"--------------------------------------------\")\r\n",
        "predictions, predictions_prob  = pred_neural(model,x_test,True)\r\n",
        "accuracy_neural(y_test,predictions,'weighted',TARGET_NAMES)\r\n",
        "\r\n",
        "multilabel_confusion_matrix(y_test_c, predictions)\r\n",
        "\r\n",
        "learning_curve(history,'loss','loss')\r\n",
        "learning_curve(history,'accuracy','accuracy')\r\n",
        "learning_curve(history,'f1_m','F1')\r\n",
        "learning_curve(history,'precision_m','Precision')\r\n",
        "learning_curve(history,'recall_m','Recall')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 278, 400)          9257600   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 278, 400)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 278, 256)          407040    \n",
            "_________________________________________________________________\n",
            "capsule_1 (Capsule)          (None, 10, 10, 16)        40960     \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 6)                 9606      \n",
            "=================================================================\n",
            "Total params: 9,715,206\n",
            "Trainable params: 457,606\n",
            "Non-trainable params: 9,257,600\n",
            "_________________________________________________________________\n",
            "Model Training\n",
            "Epoch 1/2\n",
            "202/202 [==============================] - 580s 3s/step - loss: 0.4707 - accuracy: 0.4173 - precision_m: 0.6597 - recall_m: 0.4100 - f1_m: 0.4810 - val_loss: 0.2963 - val_accuracy: 0.6267 - val_precision_m: 0.7911 - val_recall_m: 0.7798 - val_f1_m: 0.7837\n",
            "Epoch 2/2\n",
            "115/202 [================>.............] - ETA: 3:55 - loss: 0.3062 - accuracy: 0.6239 - precision_m: 0.8035 - recall_m: 0.7220 - f1_m: 0.7590"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}