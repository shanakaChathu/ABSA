{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Data Cleaning - Training set.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2Gk1v0cDXpO",
        "outputId": "4b2924a3-6f9c-495d-a6eb-cae994303ce1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IpMTfBYRDazQ",
        "outputId": "396d9a3b-03ef-491d-b14d-4789bb022412"
      },
      "source": [
        "mainloc='/content/drive/My Drive/Research/ABSA_Final/'\r\n",
        "mainloc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Research/ABSA_Final/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3QOeF07DWOf",
        "outputId": "a5d4326e-45c4-4e0e-a786-340c739e58dd"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import string\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('treebank')\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.corpus import treebank\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import string\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "#import fasttext\n",
        "import gensim\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import KeyedVectors\n",
        "import seaborn as sns \n",
        "import re  # For preprocessing\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import spacy  # For preprocessing\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFJvp7jzEonn",
        "outputId": "3e6150d4-edc0-4d6b-e41d-db2789d5e150"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUNmJ6zJDWOq"
      },
      "source": [
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def setContractions(self,contractions):\n",
        "        self.contractions=contractions \n",
        "  \n",
        "    def setStop_singlish(self,stop_singlish):\n",
        "        self.stop_singlish=stop_singlish\n",
        "        \n",
        "    def setLem(self,lemCln):\n",
        "        self.lemCln=lemCln\n",
        "        \n",
        "    def setremnum(self,remnum):\n",
        "        self.remnum=remnum\n",
        "            \n",
        "    def remove_mentions(self, input_text):\n",
        "        return re.sub(r'@\\w+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def remove_email(self,input_text):\n",
        "        return re.sub(r'[\\w\\.-]+@[\\w\\.-]+','',input_text)\n",
        "    \n",
        "    def remove_hash(self,input_text):\n",
        "        return re.sub(r'(?<=#)\\w+','',input_text)\n",
        "    \n",
        "    def remove_time(self,input_text):\n",
        "        return re.sub(r'(2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9]','',input_text)\n",
        "    \n",
        "    def remove_float(self,input_text):\n",
        "        return re.sub(\"\\d*\\.\\d+\",\"\",input_text)\n",
        "        \n",
        "    def remove_phoneno(self,input_text):\n",
        "        y= re.sub(r\"\\b\\d{5}\\b\",'',input_text)\n",
        "        y= re.sub(r\"\\b\\d{4}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{6}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{7}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{8}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{9}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{10}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{11}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{12}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{13}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{14}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{15}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{16}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{17}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{18}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{19}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{20}\\b\",'',y)\n",
        "        return y \n",
        "        \n",
        "    def remove_year(self,input_text):\n",
        "        return re.sub(r\"\\b(19[40][0-9]|20[0-1][0-9]|2020)\\b\",'',input_text)\n",
        "     \n",
        "    def remove_emoji(self, input_text):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', input_text)\n",
        "    \n",
        "    def apostrophe(self,input_text):\n",
        "        words = input_text.split()\n",
        "        for word in words:\n",
        "            if word.lower() in self.contractions:\n",
        "                input_text = input_text.replace(word, self.contractions[word.lower()])\n",
        "        return input_text\n",
        "    \n",
        "    def spell_check(self,input_text):\n",
        "        input_text=str(input_text)\n",
        "        spell = SpellChecker()\n",
        "        words = input_text.split() \n",
        "        spellcheck_words = [spell.correction(word) for word in words] \n",
        "        return \" \".join(spellcheck_words) \n",
        "\n",
        "    def remove_punctuation(self, input_text):\n",
        "        temp=re.sub(\"[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\",' ',input_text).strip()\n",
        "        output_text=' '.join(temp.split())\n",
        "        return output_text\n",
        "       \n",
        "    def remove_singlewords(self,input_text):\n",
        "        return ' '.join( [w for w in input_text.split() if len(w)>1] )\n",
        "        \n",
        "    def remove_digits(self, input_text):\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        return str(input_text).lower()\n",
        "    \n",
        "    def remove_stop_singlish(self, input_text):\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in self.stop_singlish)] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def remove_stop_numbers(self,input_text):\n",
        "        words=input_text.split()\n",
        "        clean_words = [word for word in words if (word not in self.remnum)] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        words = input_text.split() \n",
        "        slist= stopwords_list\n",
        "        clean_words = [word for word in words if (word not in slist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def lemmetzing(self,input_text):\n",
        "        wnl = WordNetLemmatizer() \n",
        "        words = input_text.split() \n",
        "        lem=[]\n",
        "        for word,tag in pos_tag(word_tokenize(input_text)):\n",
        "            wntag=tag[0].lower()\n",
        "            wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
        "            if not wntag:\n",
        "                lemma=word\n",
        "                lem.append(lemma)\n",
        "            else:\n",
        "                lemma=wnl.lemmatize(word,wntag)\n",
        "                lem.append(lemma)\n",
        "        \n",
        "        return \" \".join(lem) \n",
        "    \n",
        "    def lemmetize_sin(self,input_text):\n",
        "        vec=[]\n",
        "        words=input_text.split()\n",
        "        for x in range(len(words)):\n",
        "            if(words[x] in list(self.lemCln.original)):\n",
        "                ind=self.lemCln[self.lemCln['original']==words[x]]['subs'].index.values.astype(int)[0]\n",
        "                w=self.lemCln[self.lemCln['original']==words[x]]['subs'][ind]\n",
        "                vec.append(w)   \n",
        "            else:\n",
        "                w=words[x]\n",
        "                vec.append(w)\n",
        "        vec1=' '.join(vec)\n",
        "        return vec1\n",
        "    \n",
        "    def seperate_numwords(self,input_text):\n",
        "        words=input_text.split()\n",
        "        output=''\n",
        "        for word in words:\n",
        "            temp = re.compile(\"([0-9]+)([a-zA-Z]+)\") \n",
        "            if temp.match(word) and len(word)>2:\n",
        "                res = temp.match(word).groups() \n",
        "                output=output+' '+ ' '.join(res).strip()\n",
        "            else:\n",
        "                output=output+' '+word\n",
        "        output= ' '.join(output.split())\n",
        "        return output.strip()\n",
        "    \n",
        "    def remove_zerostartwords(self,input_text):\n",
        "        #temp=re.sub(r'\\b0\\d{2}\\b',' ',input_text).strip()\n",
        "        output= \" \".join(filter(lambda x:x[0]!='0', input_text.split()))\n",
        "        return output \n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_txt = X.apply(self.to_lower).apply(self.apostrophe).apply(self.remove_urls).apply(self.remove_email)\\\n",
        "        .apply(self.remove_emoji).apply(self.remove_hash).apply(self.remove_year)\\\n",
        "        .apply(self.remove_mentions).apply(self.remove_time).apply(self.remove_punctuation).apply(self.remove_phoneno)\\\n",
        "        .apply(self.remove_float).apply(self.seperate_numwords).apply(self.remove_zerostartwords).apply(self.remove_phoneno)\\\n",
        "        .apply(self.remove_stop_numbers).apply(self.remove_stop_singlish).apply(self.lemmetzing).apply(self.lemmetize_sin)\n",
        "        return clean_txt\n",
        "      \n",
        "def readTxtFile(loc):\n",
        "    s = open(loc, 'r').read()\n",
        "    whip = eval(s)\n",
        "    return whip\n",
        "\n",
        "def lemFile(loc):\n",
        "    lem=pd.read_csv(loc,header=None,index_col=None)\n",
        "    lem.columns=['original','subs']\n",
        "    lem=lem.dropna()\n",
        "    lem=lem.drop_duplicates()\n",
        "    a=lem.groupby('original').count().reset_index()\n",
        "    lis=a[a['subs']>1].original.unique()\n",
        "    lemCln=lem[~(lem['original'].isin(lis))]\n",
        "    return lemCln\n",
        "\n",
        "def stopfile(loc):\n",
        "    swords=pd.read_csv(loc,header=None)\n",
        "    swords.columns=['sword']\n",
        "    swords=list(swords.dropna()['sword'])\n",
        "    return swords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWfFdkdsDWO1"
      },
      "source": [
        "df=pd.read_csv(mainloc+\"data.csv\")\n",
        "df.columns=['id','comment','network','billing_price','package','customer_service','data','service_product','sentiment']\n",
        "df=df[~df['sentiment'].isna()]\n",
        "df=df.fillna(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ObjcjpDWO5",
        "outputId": "cb0f963b-c104-4d33-9626-6e1e674223f3"
      },
      "source": [
        "#Network \n",
        "df.loc[df['network']=='0', 'network'] = 0\n",
        "df.loc[df['network']=='1', 'network'] = 1\n",
        "df.network.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVqXptR2DWO6",
        "outputId": "14eea4fc-c17f-42e3-f6d7-7ed59469cd0d"
      },
      "source": [
        "#Billing_price \n",
        "df.billing_price.unique()\n",
        "df.loc[df['billing_price']=='0', 'billing_price'] = 0\n",
        "df.loc[df['billing_price']=='1', 'billing_price'] = 1\n",
        "df.loc[df['billing_price']=='2', 'billing_price'] = 1\n",
        "df.loc[df['billing_price']==' ', 'billing_price'] = 1\n",
        "df.billing_price.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7hP-fTlDWO7",
        "outputId": "1495f805-4bce-490a-923b-deacd33501d2"
      },
      "source": [
        "#Customer service \n",
        "df.customer_service.unique()\n",
        "df.loc[df['customer_service']=='0', 'customer_service'] = 0\n",
        "df.loc[df['customer_service']=='1', 'customer_service'] = 1\n",
        "df.loc[df['customer_service']=='   ', 'customer_service'] = 1\n",
        "df.customer_service.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByfGQuq_DWO8",
        "outputId": "bd5f412d-67bc-4c8b-d7b9-59ec5ae7ee8f"
      },
      "source": [
        "#Sentiment \n",
        "df.sentiment.unique()\n",
        "\n",
        "df.loc[df['sentiment']=='0', 'sentiment'] = 0\n",
        "df.loc[df['sentiment']=='1', 'sentiment'] = 1\n",
        "df.loc[df['sentiment']=='2', 'sentiment'] = -1\n",
        "df.loc[df['sentiment']=='-11', 'sentiment'] = -1\n",
        "df.loc[df['sentiment']=='1-', 'sentiment'] = -1\n",
        "df.loc[df['sentiment']=='10', 'sentiment'] = -1\n",
        "df.loc[df['sentiment']=='-1', 'sentiment'] = -1\n",
        "df.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, -1], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "WP3yCFiwDWO8",
        "outputId": "d69bae0c-ee50-4478-9701-b4baa7bbc221"
      },
      "source": [
        "df['id']=df['id'].astype(int)\n",
        "df['network']=df['network'].astype(int)\n",
        "df['billing_price']=df['billing_price'].astype(int)\n",
        "df['package']=df['package'].astype(int)\n",
        "df['customer_service']=df['customer_service'].astype(int)\n",
        "df['data']=df['data'].astype(int)\n",
        "df['service_product']=df['service_product'].astype(int)\n",
        "df['sentiment']=df['sentiment'].astype(int)\n",
        "print(\"size of dataframe: \", df.shape[0])\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of dataframe:  10060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment</th>\n",
              "      <th>network</th>\n",
              "      <th>billing_price</th>\n",
              "      <th>package</th>\n",
              "      <th>customer_service</th>\n",
              "      <th>data</th>\n",
              "      <th>service_product</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>125940</td>\n",
              "      <td>Munge boru math damma package eka danna kalin ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61803</td>\n",
              "      <td>Dialog Axiata night time inne it8n a tika day ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... sentiment\n",
              "0  125940  ...         0\n",
              "1   61803  ...         0\n",
              "\n",
              "[2 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDHsdF6oDWO8",
        "outputId": "3868c78c-7451-4d2f-d526-e5586cf3609e"
      },
      "source": [
        "#Witholut lemetize sinhala and populating it to cleanAnswer\n",
        "t = time()\n",
        "\n",
        "contracloc=mainloc+\"Helper_Files/Contractions.txt\"\n",
        "stopdigitloc = mainloc+'Helper_Files/datacln.xlsx'\n",
        "lemloc=mainloc+\"Helper_Files/lemmatization_singlish.csv\"\n",
        "\n",
        "contractions= readTxtFile(contracloc)\n",
        "\n",
        "#Extract stop values related to digits \n",
        "stop_digit= pd.read_excel(stopdigitloc)\n",
        "stop_digit['word']=stop_digit['word'].astype(str)\n",
        "remnum=stop_digit[(stop_digit['isdigit']==True) & (stop_digit['freq']<=10)]\n",
        "remnum=list(remnum.word)\n",
        "\n",
        "#Extract stop value related to singlish \n",
        "stop_singlish=stop_digit[stop_digit['Stopflag']==1]\n",
        "stop_singlish=list(stop_singlish.word)\n",
        "\n",
        "#Extract lemmatization related to singlish\n",
        "lemcln=lemFile(lemloc)\n",
        "\n",
        "ct = CleanText()\n",
        "ct.setStop_singlish(stop_singlish)\n",
        "ct.setContractions(contractions)\n",
        "ct.setremnum(remnum)\n",
        "ct.setLem(lemcln)\n",
        "sr_clean = ct.fit_transform(df.comment)\n",
        "df['cleanAnswer1']=sr_clean\n",
        "\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to clean up everything: 0.76 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "rNXcHMKkDWO9",
        "outputId": "917829fb-7865-4cf2-ed72-c933579b82eb"
      },
      "source": [
        "def token(x):\n",
        "    y=word_tokenize(x.cleanAnswer)\n",
        "    return y \n",
        "\n",
        "def nwords(x):\n",
        "    y=len(x.tokens)\n",
        "    return y \n",
        "\n",
        "def englishWordsPerc(x):\n",
        "    words=x.cleanAnswer.split()\n",
        "    if(len(words)>0):\n",
        "        l=len([word for word in words if (wordnet.synsets(word))])\n",
        "        perc=100-(l/len(words))*100\n",
        "        return perc\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "df['tokens']=df.apply(token,axis=1)\n",
        "df['nwords']=df.apply(nwords,axis=1)\n",
        "df['singperc']=df.apply(englishWordsPerc,axis=1)\n",
        "df['comment_len']= df['cleanAnswer'].apply(lambda x:len(x))\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment</th>\n",
              "      <th>network</th>\n",
              "      <th>billing_price</th>\n",
              "      <th>package</th>\n",
              "      <th>customer_service</th>\n",
              "      <th>data</th>\n",
              "      <th>service_product</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleanAnswer</th>\n",
              "      <th>cleanAnswer1</th>\n",
              "      <th>tokens</th>\n",
              "      <th>nwords</th>\n",
              "      <th>singperc</th>\n",
              "      <th>comment_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>125940</td>\n",
              "      <td>Munge boru math damma package eka danna kalin ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>munge boru math damma package danna kalin 4g a...</td>\n",
              "      <td>munge boru math damma package danna kalin 4g a...</td>\n",
              "      <td>[munge, boru, math, damma, package, danna, kal...</td>\n",
              "      <td>278</td>\n",
              "      <td>70.503597</td>\n",
              "      <td>1648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61803</td>\n",
              "      <td>Dialog Axiata night time inne it8n a tika day ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>axiata night time inne it8n tika day time pawi...</td>\n",
              "      <td>axiata night time inne it8n tika day time pawi...</td>\n",
              "      <td>[axiata, night, time, inne, it8n, tika, day, t...</td>\n",
              "      <td>229</td>\n",
              "      <td>71.615721</td>\n",
              "      <td>1372</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... comment_len\n",
              "0  125940  ...        1648\n",
              "1   61803  ...        1372\n",
              "\n",
              "[2 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XPLSpR5DWO9"
      },
      "source": [
        "df.to_csv(mainloc+\"df_train.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}