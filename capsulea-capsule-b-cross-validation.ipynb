{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Installing and importing dependecies**","metadata":{"id":"k8D9EuJnBnDV"}},{"cell_type":"code","source":"#!pip install tensorflow==1.14.0\n#!pip install tensorflow-gpu==1.15","metadata":{"id":"0RJTK5LXWz6Z","outputId":"6df95abc-5edb-4c51-a181-6b0c8dc4855c","execution":{"iopub.status.busy":"2021-06-13T16:30:43.357844Z","iopub.execute_input":"2021-06-13T16:30:43.358575Z","iopub.status.idle":"2021-06-13T16:30:43.36335Z","shell.execute_reply.started":"2021-06-13T16:30:43.358486Z","shell.execute_reply":"2021-06-13T16:30:43.362388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint('Using Tensorflow version:', tf.__version__)\n#print('Using transformers version:', transformers.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:30:43.365054Z","iopub.execute_input":"2021-06-13T16:30:43.365793Z","iopub.status.idle":"2021-06-13T16:30:44.918286Z","shell.execute_reply.started":"2021-06-13T16:30:43.365755Z","shell.execute_reply":"2021-06-13T16:30:44.91651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:30:44.924365Z","iopub.execute_input":"2021-06-13T16:30:44.925862Z","iopub.status.idle":"2021-06-13T16:30:45.448558Z","shell.execute_reply.started":"2021-06-13T16:30:44.925817Z","shell.execute_reply":"2021-06-13T16:30:45.447591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib \nprint(device_lib.list_local_devices())","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:30:45.450495Z","iopub.execute_input":"2021-06-13T16:30:45.450953Z","iopub.status.idle":"2021-06-13T16:30:45.574305Z","shell.execute_reply.started":"2021-06-13T16:30:45.450913Z","shell.execute_reply":"2021-06-13T16:30:45.573312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"sOF29ZnzBjht"}},{"cell_type":"code","source":"import pandas as pd\nimport collections\nimport gensim\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models.fasttext import FastText\nfrom gensim.models import word2vec\n\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_fscore_support\n\nimport pandas as pd\nimport numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import initializers\n\n# import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt","metadata":{"id":"zlXBE_-86l4q","outputId":"bfa8bfb0-9540-42ba-81a0-cc64666bd895","execution":{"iopub.status.busy":"2021-06-13T16:30:45.575666Z","iopub.execute_input":"2021-06-13T16:30:45.576244Z","iopub.status.idle":"2021-06-13T16:30:47.219398Z","shell.execute_reply.started":"2021-06-13T16:30:45.576191Z","shell.execute_reply":"2021-06-13T16:30:47.218504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install keras==2.1.5","metadata":{"id":"Wb6SNszhX4Jc","execution":{"iopub.status.busy":"2021-06-13T16:30:47.220629Z","iopub.execute_input":"2021-06-13T16:30:47.221011Z","iopub.status.idle":"2021-06-13T16:30:47.228007Z","shell.execute_reply.started":"2021-06-13T16:30:47.220972Z","shell.execute_reply":"2021-06-13T16:30:47.227064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set folder paths\nfolder_path = ''\nEMBEDDING_SIZE = 400 \nembedding_type = \"fasttext\"\ncontext = 5\n\ndata_path = \"../input/dftrain/df_train.csv\"\n# lankadeepa_data_path = folder_path + 'corpus/new/preprocess_from_isuru/lankadeepa_tagged_comments.csv'\n# gossip_lanka_data_path = folder_path + 'corpus/new/preprocess_from_unicode_values/gossip_lanka_tagged_comments.csv'\n\n\n# word_embedding_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(EMBEDDING_SIZE)+\"/\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n# word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(EMBEDDING_SIZE)+\"/keyed_vectors/keyed.kv\"\nembedding_matrix_path = \"../input/wordembeddings/w2v_cbow_400.txt\"","metadata":{"id":"1QeMxau54aMg","execution":{"iopub.status.busy":"2021-06-13T16:30:47.229363Z","iopub.execute_input":"2021-06-13T16:30:47.229825Z","iopub.status.idle":"2021-06-13T16:30:47.238423Z","shell.execute_reply.started":"2021-06-13T16:30:47.229779Z","shell.execute_reply":"2021-06-13T16:30:47.237451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Load Dataset**","metadata":{"id":"nRbpY2FTDl8p"}},{"cell_type":"code","source":"all_data = pd.read_csv(data_path)\n# gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n# gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])","metadata":{"id":"IyuNHf_14aTQ","execution":{"iopub.status.busy":"2021-06-13T16:30:47.239801Z","iopub.execute_input":"2021-06-13T16:30:47.240212Z","iopub.status.idle":"2021-06-13T16:30:47.322014Z","shell.execute_reply.started":"2021-06-13T16:30:47.24016Z","shell.execute_reply":"2021-06-13T16:30:47.321185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data","metadata":{"id":"9QEa9WKy4aao","outputId":"312e9737-3e7b-4f23-b3a5-c5cbfc76f664","execution":{"iopub.status.busy":"2021-06-13T16:30:47.324954Z","iopub.execute_input":"2021-06-13T16:30:47.325272Z","iopub.status.idle":"2021-06-13T16:30:47.356729Z","shell.execute_reply.started":"2021-06-13T16:30:47.325244Z","shell.execute_reply":"2021-06-13T16:30:47.355964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n# all_data.shape","metadata":{"id":"1r88f-JQ4aIq","execution":{"iopub.status.busy":"2021-06-13T16:30:47.359586Z","iopub.execute_input":"2021-06-13T16:30:47.359938Z","iopub.status.idle":"2021-06-13T16:30:47.363914Z","shell.execute_reply.started":"2021-06-13T16:30:47.359896Z","shell.execute_reply":"2021-06-13T16:30:47.362877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Process data**","metadata":{"id":"9dh6y6RxDv6w"}},{"cell_type":"code","source":"def text_preprocessing_2(data):\n  comments = data['comment']\n  labels = data['sentiment']\n  comments_splitted = []\n\n  for comment in comments:\n    lines = []\n    try:\n      words = comment.split()\n      lines += words\n    except:\n      continue\n    comments_splitted.append(lines)\n\n  return comments_splitted,labels","metadata":{"id":"9NJfQGfj7VMz","execution":{"iopub.status.busy":"2021-06-13T16:30:47.365228Z","iopub.execute_input":"2021-06-13T16:30:47.365865Z","iopub.status.idle":"2021-06-13T16:30:47.373594Z","shell.execute_reply.started":"2021-06-13T16:30:47.365827Z","shell.execute_reply":"2021-06-13T16:30:47.372768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_texts, comment_labels = text_preprocessing_2(all_data)\n\n# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(comment_texts)\nvocab_size = len(t.word_index) + 1\nprint(vocab_size)","metadata":{"id":"IiasDeTS66ef","outputId":"76a0477d-8a06-4b37-9af3-7057a1b6cfba","execution":{"iopub.status.busy":"2021-06-13T16:30:47.374999Z","iopub.execute_input":"2021-06-13T16:30:47.375435Z","iopub.status.idle":"2021-06-13T16:30:47.578157Z","shell.execute_reply.started":"2021-06-13T16:30:47.375351Z","shell.execute_reply":"2021-06-13T16:30:47.576946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_docs = t.texts_to_sequences(comment_texts)","metadata":{"id":"f3KdBCcN66aU","execution":{"iopub.status.busy":"2021-06-13T16:30:47.57977Z","iopub.execute_input":"2021-06-13T16:30:47.580132Z","iopub.status.idle":"2021-06-13T16:30:47.875779Z","shell.execute_reply.started":"2021-06-13T16:30:47.580092Z","shell.execute_reply":"2021-06-13T16:30:47.874899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_length = len(max(encoded_docs, key=len))\nmax_length = 50\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='post')\ncomment_labels = np.array(comment_labels)\npadded_docs = np.array(padded_docs)","metadata":{"id":"WJ1R0ipc66XX","execution":{"iopub.status.busy":"2021-06-13T16:30:47.877164Z","iopub.execute_input":"2021-06-13T16:30:47.87753Z","iopub.status.idle":"2021-06-13T16:30:48.016416Z","shell.execute_reply.started":"2021-06-13T16:30:47.877475Z","shell.execute_reply":"2021-06-13T16:30:48.01565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"id":"RjE1Kh0AYr_R","outputId":"72e494d0-464b-4bd0-8585-bc0eea9dbcc0","execution":{"iopub.status.busy":"2021-06-13T16:30:48.017581Z","iopub.execute_input":"2021-06-13T16:30:48.017992Z","iopub.status.idle":"2021-06-13T16:30:48.025969Z","shell.execute_reply.started":"2021-06-13T16:30:48.017962Z","shell.execute_reply":"2021-06-13T16:30:48.02491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_labels = pd.get_dummies(comment_labels).values","metadata":{"id":"UtFzHWCC66U5","execution":{"iopub.status.busy":"2021-06-13T16:30:48.027537Z","iopub.execute_input":"2021-06-13T16:30:48.028Z","iopub.status.idle":"2021-06-13T16:30:48.035726Z","shell.execute_reply.started":"2021-06-13T16:30:48.027958Z","shell.execute_reply":"2021-06-13T16:30:48.034946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.1, random_state=0)","metadata":{"id":"KL3qusEd70hr","execution":{"iopub.status.busy":"2021-06-13T16:30:48.037065Z","iopub.execute_input":"2021-06-13T16:30:48.037459Z","iopub.status.idle":"2021-06-13T16:30:48.046798Z","shell.execute_reply.started":"2021-06-13T16:30:48.037424Z","shell.execute_reply":"2021-06-13T16:30:48.045961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(embedding_matrix_path, encoding='utf8') as f:\n#   for line in f:\n#     print(line)\n    ","metadata":{"id":"ADZ_K6xqbOSJ","execution":{"iopub.status.busy":"2021-06-13T16:30:48.049572Z","iopub.execute_input":"2021-06-13T16:30:48.049974Z","iopub.status.idle":"2021-06-13T16:30:48.055464Z","shell.execute_reply.started":"2021-06-13T16:30:48.049945Z","shell.execute_reply":"2021-06-13T16:30:48.054652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Embeddings**","metadata":{"id":"S1iFKGQvgGNm"}},{"cell_type":"code","source":"#set embedding path to load embeddings\nembedding_path = ''","metadata":{"id":"vd3DeM14Rpp9","execution":{"iopub.status.busy":"2021-06-13T16:30:48.056839Z","iopub.execute_input":"2021-06-13T16:30:48.057221Z","iopub.status.idle":"2021-06-13T16:30:48.064624Z","shell.execute_reply.started":"2021-06-13T16:30:48.057174Z","shell.execute_reply":"2021-06-13T16:30:48.063783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_embedding_matrix():\n  embeddings_index={}\n  with open(embedding_matrix_path, encoding='utf8') as f:\n    for line in f:\n      line = line.strip().split(' ')\n      word = line[0]\n      embedding = [float(x) for x in line[1:]]\n      embeddings_index[word]=embedding\n\n  # create a weight matrix for words in training docs\n  embedding_matrix = np.zeros((vocab_size, EMBEDDING_SIZE))\n  for word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[i] = embedding_vector\n\n  # pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n  return embedding_matrix","metadata":{"id":"e2AYK7My_wEK","execution":{"iopub.status.busy":"2021-06-13T16:30:48.06561Z","iopub.execute_input":"2021-06-13T16:30:48.065926Z","iopub.status.idle":"2021-06-13T16:30:48.075514Z","shell.execute_reply.started":"2021-06-13T16:30:48.065902Z","shell.execute_reply":"2021-06-13T16:30:48.074314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_vectors = generate_embedding_matrix()","metadata":{"id":"pjJm-nkjRpm5","execution":{"iopub.status.busy":"2021-06-13T16:30:48.076866Z","iopub.execute_input":"2021-06-13T16:30:48.077327Z","iopub.status.idle":"2021-06-13T16:31:00.19692Z","shell.execute_reply.started":"2021-06-13T16:30:48.077279Z","shell.execute_reply":"2021-06-13T16:31:00.196008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = max_length\nmax_features = vocab_size\nembedding_dims = 400","metadata":{"id":"Yk7iSweShnqA","execution":{"iopub.status.busy":"2021-06-13T16:31:00.198336Z","iopub.execute_input":"2021-06-13T16:31:00.198756Z","iopub.status.idle":"2021-06-13T16:31:00.204095Z","shell.execute_reply.started":"2021-06-13T16:31:00.198682Z","shell.execute_reply":"2021-06-13T16:31:00.203095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Capsule Network Model**\n","metadata":{"id":"UehnBw_8exkL"}},{"cell_type":"code","source":"def _conv2d_wrapper(inputs, shape, strides, padding, add_bias, activation_fn, name, stddev=0.1):\n  with tf.variable_scope(name,reuse=tf.AUTO_REUSE) as scope:\n    kernel = _get_weights_wrapper(\n      name='weights', shape=shape, weights_decay_factor=0.0, )\n    output = tf.nn.conv2d(inputs, filter=kernel, strides=strides, padding=padding, name='conv')\n    if add_bias:\n      biases = _get_biases_wrapper(name='biases', shape=[shape[-1]] )\n      output = tf.add(output, biases, name='biasAdd')\n    if activation_fn is not None:\n      output = activation_fn(output, name='activation')\n  return output","metadata":{"id":"9Z06E9a1aBVb","execution":{"iopub.status.busy":"2021-06-13T16:31:00.205663Z","iopub.execute_input":"2021-06-13T16:31:00.206127Z","iopub.status.idle":"2021-06-13T16:31:00.215498Z","shell.execute_reply.started":"2021-06-13T16:31:00.206091Z","shell.execute_reply":"2021-06-13T16:31:00.214756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _get_weights_wrapper(name, shape, dtype=tf.float32, initializer=initializers.xavier_initializer(),weights_decay_factor=None):\n  weights = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n  if weights_decay_factor is not None and weights_decay_factor > 0.0:\n    weights_wd = tf.multiply(tf.nn.l2_loss(weights), weights_decay_factor, name=name + '/l2loss')\n    tf.add_to_collection('losses', weights_wd)\n  return weights","metadata":{"id":"k8F7Y8CGbsB4","execution":{"iopub.status.busy":"2021-06-13T16:31:00.218209Z","iopub.execute_input":"2021-06-13T16:31:00.218489Z","iopub.status.idle":"2021-06-13T16:31:00.229974Z","shell.execute_reply.started":"2021-06-13T16:31:00.218462Z","shell.execute_reply":"2021-06-13T16:31:00.229029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _get_biases_wrapper(name, shape, dtype=tf.float32, initializer=tf.constant_initializer(0.0)):\n  \"\"\"Wrapper over _get_variable_wrapper() to get bias.\n  \"\"\"\n  biases = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n  return biases","metadata":{"id":"nkO3w5gDcC1b","execution":{"iopub.status.busy":"2021-06-13T16:31:00.231224Z","iopub.execute_input":"2021-06-13T16:31:00.231647Z","iopub.status.idle":"2021-06-13T16:31:00.240008Z","shell.execute_reply.started":"2021-06-13T16:31:00.23161Z","shell.execute_reply":"2021-06-13T16:31:00.239066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _get_variable_wrapper(name, shape=None, dtype=None, initializer=None,regularizer=None,trainable=True,collections=None,caching_device=None,partitioner=None,validate_shape=True,custom_getter=None):\n  with tf.device('/cpu:0'):\n    var = tf.get_variable(\n      name, shape=shape, dtype=dtype, initializer=initializer,\n      regularizer=regularizer, trainable=trainable,\n      collections=collections, caching_device=caching_device,\n      partitioner=partitioner, validate_shape=validate_shape,\n      custom_getter=custom_getter\n    )\n  return var","metadata":{"id":"0GVuHgh2EWw2","execution":{"iopub.status.busy":"2021-06-13T16:31:00.241339Z","iopub.execute_input":"2021-06-13T16:31:00.241776Z","iopub.status.idle":"2021-06-13T16:31:00.25044Z","shell.execute_reply.started":"2021-06-13T16:31:00.241739Z","shell.execute_reply":"2021-06-13T16:31:00.249461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex/K.sum(ex, axis=axis, keepdims=True)","metadata":{"id":"MBkd3MRuccQh","execution":{"iopub.status.busy":"2021-06-13T16:31:00.255015Z","iopub.execute_input":"2021-06-13T16:31:00.255292Z","iopub.status.idle":"2021-06-13T16:31:00.260534Z","shell.execute_reply.started":"2021-06-13T16:31:00.255267Z","shell.execute_reply":"2021-06-13T16:31:00.259479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def squash_v1(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    return scale * x","metadata":{"id":"hSVP0ymhc6G6","execution":{"iopub.status.busy":"2021-06-13T16:31:00.262669Z","iopub.execute_input":"2021-06-13T16:31:00.263234Z","iopub.status.idle":"2021-06-13T16:31:00.270104Z","shell.execute_reply.started":"2021-06-13T16:31:00.263189Z","shell.execute_reply":"2021-06-13T16:31:00.269174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def squash_v0(s, axis=-1, epsilon=1e-7, name=None):\n    s_squared_norm = K.sum(K.square(s), axis, keepdims=True) + K.epsilon()\n    safe_norm = K.sqrt(s_squared_norm)\n    scale = 1 - tf.exp(-safe_norm)\n    return scale * s / safe_norm","metadata":{"id":"RQzm6uHwc8Rq","execution":{"iopub.status.busy":"2021-06-13T16:31:00.272971Z","iopub.execute_input":"2021-06-13T16:31:00.273256Z","iopub.status.idle":"2021-06-13T16:31:00.283877Z","shell.execute_reply.started":"2021-06-13T16:31:00.273208Z","shell.execute_reply":"2021-06-13T16:31:00.28308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations):\n    b = tf.keras.backend.zeros_like(u_hat_vecs[:,:,:,0])\n    if i_activations is not None:\n        i_activations = i_activations[...,tf.newaxis]\n    for i in range(iterations):\n        if False:\n            leak = tf.zeros_like(b, optimize=True)\n            leak = tf.reduce_sum(leak, axis=1, keep_dims=True)\n            leaky_logits = tf.concat([leak, b], axis=1)\n            leaky_routing = tf.nn.softmax(leaky_logits, dim=1)        \n            c = tf.split(leaky_routing, [1, output_capsule_num], axis=1)[1]\n        else:\n            c = softmax(b, 1)   \n        outputs = squash_v1(K.batch_dot(c, u_hat_vecs, [2, 2]))\n        if i < iterations - 1:\n            b = b + K.batch_dot(outputs, u_hat_vecs, [2, 3])                                    \n    poses = outputs \n    activations = K.sqrt(K.sum(K.square(poses), 2))\n    return poses, activations\n","metadata":{"id":"ObeP2wKDc-Zq","execution":{"iopub.status.busy":"2021-06-13T16:31:00.286862Z","iopub.execute_input":"2021-06-13T16:31:00.287129Z","iopub.status.idle":"2021-06-13T16:31:00.297434Z","shell.execute_reply.started":"2021-06-13T16:31:00.287105Z","shell.execute_reply":"2021-06-13T16:31:00.29672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vec_transformationByConv(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num):                            \n    kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_dim, output_capsule_dim*output_capsule_num], weights_decay_factor=0.0)\n    u_hat_vecs = tf.keras.backend.conv1d(poses, kernel)\n    u_hat_vecs = tf.keras.backend.reshape(u_hat_vecs, (-1, input_capsule_num, output_capsule_num, output_capsule_dim))\n    u_hat_vecs = tf.keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n    return u_hat_vecs\n","metadata":{"id":"zSdVM9YCdEMp","execution":{"iopub.status.busy":"2021-06-13T16:31:00.298643Z","iopub.execute_input":"2021-06-13T16:31:00.299112Z","iopub.status.idle":"2021-06-13T16:31:00.308215Z","shell.execute_reply.started":"2021-06-13T16:31:00.299071Z","shell.execute_reply":"2021-06-13T16:31:00.307402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vec_transformationByMat(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num, shared=True):                        \n    inputs_poses_shape = poses.get_shape().as_list()\n    poses = poses[..., tf.newaxis, :]        \n    poses = tf.tile(poses, [1, 1, output_capsule_num, 1])    \n    if shared:\n        kernel = _get_weights_wrapper(name='weights', shape=[1, 1, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n        kernel = tf.tile(kernel, [inputs_poses_shape[0], input_capsule_num, 1, 1, 1])\n    else:\n        kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_num, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n        kernel = tf.tile(kernel, [inputs_poses_shape[0], 1, 1, 1, 1])\n    u_hat_vecs = tf.squeeze(tf.matmul(kernel, poses[...,tf.newaxis]),axis=-1)\n    u_hat_vecs = tf.keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n    return u_hat_vecs","metadata":{"id":"i-pIavredIK6","execution":{"iopub.status.busy":"2021-06-13T16:31:00.309324Z","iopub.execute_input":"2021-06-13T16:31:00.309937Z","iopub.status.idle":"2021-06-13T16:31:00.319846Z","shell.execute_reply.started":"2021-06-13T16:31:00.309903Z","shell.execute_reply":"2021-06-13T16:31:00.31798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsules_init(inputs, shape, strides, padding, pose_shape, add_bias, name):\n    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):   \n        poses = _conv2d_wrapper(\n          inputs,\n          shape=shape[0:-1] + [shape[-1] * pose_shape],\n          strides=strides,\n          padding=padding,\n          add_bias=add_bias,\n          activation_fn=None,\n          name='pose_stacked'\n        )        \n        poses_shape = poses.get_shape().as_list()    \n        poses = tf.reshape(poses, [-1, poses_shape[1], poses_shape[2], shape[-1], pose_shape])        \n        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, shape[-1]])    \n        poses = squash_v1(poses, axis=-1)  \n        activations = K.sqrt(K.sum(K.square(poses), axis=-1)) + beta_a        \n\n    return poses, activations","metadata":{"id":"mcvHPFCOdKyX","execution":{"iopub.status.busy":"2021-06-13T16:31:00.322128Z","iopub.execute_input":"2021-06-13T16:31:00.3224Z","iopub.status.idle":"2021-06-13T16:31:00.332118Z","shell.execute_reply.started":"2021-06-13T16:31:00.322377Z","shell.execute_reply":"2021-06-13T16:31:00.331308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsule_fc_layer(nets, output_capsule_num, iterations, name):\n    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):   \n        poses, i_activations = nets\n        input_pose_shape = poses.get_shape().as_list()\n\n        u_hat_vecs = vec_transformationByConv(poses,input_pose_shape[-1], input_pose_shape[1],input_pose_shape[-1], output_capsule_num,)\n        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, output_capsule_num])\n        poses, activations = routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations)\n    return poses, activations","metadata":{"id":"hAJ1njh6dOpT","execution":{"iopub.status.busy":"2021-06-13T16:31:00.33342Z","iopub.execute_input":"2021-06-13T16:31:00.333895Z","iopub.status.idle":"2021-06-13T16:31:00.342839Z","shell.execute_reply.started":"2021-06-13T16:31:00.333859Z","shell.execute_reply":"2021-06-13T16:31:00.342011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsule_flatten(nets):\n    poses, activations = nets\n    input_pose_shape = poses.get_shape().as_list()\n    \n    poses = tf.reshape(poses, [\n                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3], input_pose_shape[-1]]) \n    activations = tf.reshape(activations, [\n                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3]])\n    return poses, activations","metadata":{"id":"jE-SPLBwdR2a","execution":{"iopub.status.busy":"2021-06-13T16:31:00.343887Z","iopub.execute_input":"2021-06-13T16:31:00.344467Z","iopub.status.idle":"2021-06-13T16:31:00.355543Z","shell.execute_reply.started":"2021-06-13T16:31:00.34443Z","shell.execute_reply":"2021-06-13T16:31:00.354711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsule_conv_layer(nets, shape, strides, iterations, name):   \n    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):              \n        poses, i_activations = nets\n        \n        inputs_poses_shape = poses.get_shape().as_list()\n\n        hk_offsets = [\n          [(h_offset + k_offset) for k_offset in range(0, shape[0])] for h_offset in\n          range(0, inputs_poses_shape[1] + 1 - shape[0], strides[1])\n        ]\n        wk_offsets = [\n          [(w_offset + k_offset) for k_offset in range(0, shape[1])] for w_offset in\n          range(0, inputs_poses_shape[2] + 1 - shape[1], strides[2])\n        ]\n    \n        inputs_poses_patches = tf.transpose(\n          tf.gather(\n            tf.gather(\n              poses, hk_offsets, axis=1, name='gather_poses_height_kernel'\n            ), wk_offsets, axis=3, name='gather_poses_width_kernel'\n          ), perm=[0, 1, 3, 2, 4, 5, 6], name='inputs_poses_patches'\n        )\n        inputs_poses_shape = inputs_poses_patches.get_shape().as_list()\n        inputs_poses_patches = tf.reshape(inputs_poses_patches, [\n                                -1, shape[0]*shape[1]*shape[2], inputs_poses_shape[-1]\n                                ])\n\n        i_activations_patches = tf.transpose(\n          tf.gather(\n            tf.gather(\n              i_activations, hk_offsets, axis=1, name='gather_activations_height_kernel'\n            ), wk_offsets, axis=3, name='gather_activations_width_kernel'\n          ), perm=[0, 1, 3, 2, 4, 5], name='inputs_activations_patches'\n        )\n        i_activations_patches = tf.reshape(i_activations_patches, [\n                                -1, shape[0]*shape[1]*shape[2]]\n                                )\n        u_hat_vecs = vec_transformationByConv(\n                  inputs_poses_patches,\n                  inputs_poses_shape[-1], shape[0]*shape[1]*shape[2],\n                  inputs_poses_shape[-1], shape[3],\n                  )  \n        beta_a = _get_weights_wrapper(\n                name='beta_a', shape=[1, shape[3]]\n                )\n        poses, activations = routing(u_hat_vecs, beta_a, iterations, shape[3], i_activations_patches)\n        poses = tf.reshape(poses, [\n                    inputs_poses_shape[0], inputs_poses_shape[1],\n                    inputs_poses_shape[2], shape[3],\n                    inputs_poses_shape[-1]]\n                ) \n        activations = tf.reshape(activations, [\n                    inputs_poses_shape[0],inputs_poses_shape[1],\n                    inputs_poses_shape[2],shape[3]]\n                ) \n        nets = poses, activations            \n    return nets","metadata":{"id":"1uMUkhf1dVJy","execution":{"iopub.status.busy":"2021-06-13T16:31:00.357021Z","iopub.execute_input":"2021-06-13T16:31:00.357444Z","iopub.status.idle":"2021-06-13T16:31:00.37138Z","shell.execute_reply.started":"2021-06-13T16:31:00.357407Z","shell.execute_reply":"2021-06-13T16:31:00.370349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsule_model_A(X, num_classes):\n    with tf.variable_scope('capsule_'+str(3),reuse=tf.AUTO_REUSE ):   \n        nets = _conv2d_wrapper(\n                X, shape=[3, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n            )\n        nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n                             padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n        nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n        nets = capsule_flatten(nets)\n        poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2') \n    return poses, activations","metadata":{"id":"jam_fKSJjYfg","execution":{"iopub.status.busy":"2021-06-13T16:31:00.372908Z","iopub.execute_input":"2021-06-13T16:31:00.373517Z","iopub.status.idle":"2021-06-13T16:31:00.382845Z","shell.execute_reply.started":"2021-06-13T16:31:00.373481Z","shell.execute_reply":"2021-06-13T16:31:00.381999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capsule_model_B(X, num_classes):\n    poses_list = []\n    for _, ngram in enumerate([3,4,5]):\n        with tf.variable_scope('capsule_'+str(ngram),reuse=tf.AUTO_REUSE): \n            nets = _conv2d_wrapper(\n                X, shape=[ngram, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n            )\n            nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n                                 padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n            nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n            nets = capsule_flatten(nets)\n            poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2')\n            poses_list.append(poses)\n    \n    poses = tf.reduce_mean(tf.convert_to_tensor(poses_list), axis=0) \n    activations = K.sqrt(K.sum(K.square(poses), 2))\n    return poses, activations","metadata":{"id":"dbqUEzcbotXD","execution":{"iopub.status.busy":"2021-06-13T16:31:00.385743Z","iopub.execute_input":"2021-06-13T16:31:00.38621Z","iopub.status.idle":"2021-06-13T16:31:00.396396Z","shell.execute_reply.started":"2021-06-13T16:31:00.386181Z","shell.execute_reply":"2021-06-13T16:31:00.395553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loss functions**","metadata":{"id":"PARyB28Vi9VI"}},{"cell_type":"code","source":"def spread_loss(labels, activations, margin):\n    activations_shape = activations.get_shape().as_list()\n    mask_t = tf.equal(labels, 1)\n    mask_i = tf.equal(labels, 0)    \n    activations_t = tf.reshape(\n      tf.boolean_mask(activations, mask_t), [activations_shape[0], 1]\n    )    \n    activations_i = tf.reshape(\n      tf.boolean_mask(activations, mask_i), [activations_shape[0], activations_shape[1] - 1]\n    )    \n    gap_mit = tf.reduce_sum(tf.square(tf.nn.relu(margin - (activations_t - activations_i))))\n    return gap_mit        ","metadata":{"id":"WkAyzy_3hrYf","execution":{"iopub.status.busy":"2021-06-13T16:31:00.397627Z","iopub.execute_input":"2021-06-13T16:31:00.398056Z","iopub.status.idle":"2021-06-13T16:31:00.405751Z","shell.execute_reply.started":"2021-06-13T16:31:00.398019Z","shell.execute_reply":"2021-06-13T16:31:00.404972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_entropy(y, preds):    \n    y = tf.argmax(y, axis=1)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=y)                                               \n    loss = tf.reduce_mean(loss) \n    return loss","metadata":{"id":"n2UfkOrSjFvX","execution":{"iopub.status.busy":"2021-06-13T16:31:00.407076Z","iopub.execute_input":"2021-06-13T16:31:00.407486Z","iopub.status.idle":"2021-06-13T16:31:00.416544Z","shell.execute_reply.started":"2021-06-13T16:31:00.40745Z","shell.execute_reply":"2021-06-13T16:31:00.415848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def margin_loss(y, preds):    \n    y = tf.cast(y,tf.float32)\n    loss = y * tf.square(tf.maximum(0., 0.9 - preds)) + \\\n        0.25 * (1.0 - y) * tf.square(tf.maximum(0., preds - 0.1))\n    loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n    return loss","metadata":{"id":"qEI1tgTnjUg_","execution":{"iopub.status.busy":"2021-06-13T16:31:00.418442Z","iopub.execute_input":"2021-06-13T16:31:00.41917Z","iopub.status.idle":"2021-06-13T16:31:00.426771Z","shell.execute_reply.started":"2021-06-13T16:31:00.419059Z","shell.execute_reply":"2021-06-13T16:31:00.425893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training process**","metadata":{"id":"Qz6aicUlku28"}},{"cell_type":"code","source":"class Args:\n  embedding_type = \"static\"\n  dataset = \"\"\n  loss_type = \"margin_loss\"\n  model_type = \"capsule-B\"\n  has_test = 1\n  has_dev = 1\n  num_epochs = 2\n  batch_size = 16\n  use_orphan = False\n  use_leaky = False\n  learning_rate = 0.001\n  margin = 0.2\n  num_classes = 3\n  vocab_size = vocab_size\n  vec_size = 300\n  max_sent = max_length\n","metadata":{"id":"Un6p6OLXkPnt","execution":{"iopub.status.busy":"2021-06-13T16:31:00.428337Z","iopub.execute_input":"2021-06-13T16:31:00.428605Z","iopub.status.idle":"2021-06-13T16:31:00.435805Z","shell.execute_reply.started":"2021-06-13T16:31:00.428581Z","shell.execute_reply":"2021-06-13T16:31:00.434962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Args()","metadata":{"id":"3cA5u4HrkuCb","execution":{"iopub.status.busy":"2021-06-13T16:31:00.437064Z","iopub.execute_input":"2021-06-13T16:31:00.437456Z","iopub.status.idle":"2021-06-13T16:31:00.444234Z","shell.execute_reply.started":"2021-06-13T16:31:00.437422Z","shell.execute_reply":"2021-06-13T16:31:00.443311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with tf.device('/gpu:0'):\n#    global_step = tf.train.get_or_create_global_step()\n    \n#global_step","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:31:00.445863Z","iopub.execute_input":"2021-06-13T16:31:00.446175Z","iopub.status.idle":"2021-06-13T16:31:00.452989Z","shell.execute_reply.started":"2021-06-13T16:31:00.446146Z","shell.execute_reply":"2021-06-13T16:31:00.452214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:31:00.454008Z","iopub.execute_input":"2021-06-13T16:31:00.454371Z","iopub.status.idle":"2021-06-13T16:31:00.468364Z","shell.execute_reply.started":"2021-06-13T16:31:00.454345Z","shell.execute_reply":"2021-06-13T16:31:00.467371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/cpu:0'):\n    global_step = tf.train.get_or_create_global_step()","metadata":{"id":"cVRu8KNo3GcR","execution":{"iopub.status.busy":"2021-06-13T16:31:00.47139Z","iopub.execute_input":"2021-06-13T16:31:00.471748Z","iopub.status.idle":"2021-06-13T16:31:00.481368Z","shell.execute_reply.started":"2021-06-13T16:31:00.471715Z","shell.execute_reply":"2021-06-13T16:31:00.480528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.device","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:31:00.482509Z","iopub.execute_input":"2021-06-13T16:31:00.482909Z","iopub.status.idle":"2021-06-13T16:31:00.491193Z","shell.execute_reply.started":"2021-06-13T16:31:00.482871Z","shell.execute_reply":"2021-06-13T16:31:00.489817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BatchGenerator(object):\n    \"\"\"Generate and hold batches.\"\"\"\n    def __init__(self, dataset,label, batch_size,input_size, is_shuffle=True):\n      self._dataset = dataset\n      self._label = label\n      self._batch_size = batch_size    \n      self._cursor = 0      \n      self._input_size = input_size      \n      \n      if is_shuffle:\n          index = np.arange(len(self._dataset))\n          np.random.shuffle(index)\n          self._dataset = np.array(self._dataset)[index]\n          self._label = np.array(self._label)[index]\n      else:\n          self._dataset = np.array(self._dataset)\n          self._label = np.array(self._label)\n    def next(self):\n      if self._cursor + self._batch_size > len(self._dataset):\n          self._cursor = 0\n      \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"      \n      batch_x = self._dataset[self._cursor : self._cursor + self._batch_size,:]\n      batch_y = self._label[self._cursor : self._cursor + self._batch_size]\n      self._cursor += self._batch_size\n      return batch_x, batch_y","metadata":{"id":"lFGsOcj50Eov","execution":{"iopub.status.busy":"2021-06-13T16:31:00.493606Z","iopub.execute_input":"2021-06-13T16:31:00.494185Z","iopub.status.idle":"2021-06-13T16:31:00.502746Z","shell.execute_reply.started":"2021-06-13T16:31:00.494145Z","shell.execute_reply":"2021-06-13T16:31:00.501954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = None\nbest_epoch = 0\nbest_acc_val = 0.","metadata":{"id":"rVlmXymbzaFP","execution":{"iopub.status.busy":"2021-06-13T16:31:00.504824Z","iopub.execute_input":"2021-06-13T16:31:00.505109Z","iopub.status.idle":"2021-06-13T16:31:00.514723Z","shell.execute_reply.started":"2021-06-13T16:31:00.505083Z","shell.execute_reply":"2021-06-13T16:31:00.513886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = args.learning_rate\nm = args.margin","metadata":{"id":"yd1LUgSK0cKv","execution":{"iopub.status.busy":"2021-06-13T16:31:00.516002Z","iopub.execute_input":"2021-06-13T16:31:00.516625Z","iopub.status.idle":"2021-06-13T16:31:00.523871Z","shell.execute_reply.started":"2021-06-13T16:31:00.516586Z","shell.execute_reply":"2021-06-13T16:31:00.523026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_per_fold = []\nprecision_per_fold = []\nrecall_per_fold = []\nf1_per_fold = []\n\nkfold = KFold(n_splits=3, shuffle=True)\n\nfold_no = 1\ninputs = padded_docs\ntargets = comment_labels\n\nfor train, test in kfold.split(inputs, targets):\n  \n\n    n_iterations_per_epoch = len(inputs[train]) // args.batch_size\n    n_iterations_test = len(inputs[test]) // args.batch_size\n\n    mr_train1 = BatchGenerator(inputs[train], targets[train], args.batch_size, 0)    \n    mr_test1 = BatchGenerator(inputs[test], targets[test], args.batch_size, 0, is_shuffle=False)\n    best_accuracy = 0.\n    best_precision = 0.\n    best_recall = 0.\n    best_f1 = 0.\n\n    X = tf.placeholder(tf.int32, [args.batch_size, args.max_sent], name=\"input_x\")\n    y = tf.placeholder(tf.int64, [args.batch_size, args.num_classes], name=\"input_y\")\n    is_training = tf.placeholder_with_default(False, shape=())    \n    learning_rate = tf.placeholder(dtype='float32') \n    margin = tf.placeholder(shape=(),dtype='float32') \n\n    l2_loss = tf.constant(0.0)\n    w2v = np.array(embedding_vectors,dtype=np.float32)\n\n    W1 = tf.Variable(w2v, trainable = False)\n    X_embedding = tf.nn.embedding_lookup(W1, X)\n    X_embedding = X_embedding[...,tf.newaxis] \n\n\n    poses, activations = capsule_model_A(X_embedding, args.num_classes)\n    loss = margin_loss(y, activations) \n    y_pred = tf.argmax(activations, axis=1, name=\"y_proba\")    \n    correct = tf.equal(tf.argmax(y, axis=1), y_pred, name=\"correct\")\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name = 'opt'+str(fold_no))   \n    training_op = optimizer.minimize(loss, name=\"training_op\")\n    gradients, variables = zip(*optimizer.compute_gradients(loss)) \n    with tf.Session() as sess:\n\n      init = tf.global_variables_initializer()\n      sess.run(init)\n\n      for epoch in range(0,6): \n\n        for iteration in range(1, n_iterations_per_epoch + 1):\n            \n          \n          X_batch, y_batch = mr_train1.next()          \n          _, loss_train, probs, capsule_pose = sess.run(\n                [training_op, loss, activations, poses],\n                feed_dict={X: X_batch[:,:args.max_sent],\n                          y: y_batch,\n                          is_training: True,\n                          learning_rate:lr,\n                          margin:m})        \n          print(\"\\rIteration: {}/{} ({:.1f}%) epoch:{}  Loss: {:.5f}\".format(iteration, n_iterations_per_epoch, iteration * 100 / n_iterations_per_epoch, epoch+1, loss_train), end=\"\")                        \n        preds_list, y_list = [], []\n        for iteration in range(1, n_iterations_test + 1):\n          X_batch, y_batch = mr_test1.next()             \n          probs = sess.run([activations],\n                    feed_dict={X:X_batch[:,:args.max_sent],\n                                is_training: False})\n          preds_list = preds_list + probs[0].tolist()\n          y_list = y_list + y_batch.tolist()\n\n        y_list = np.array(y_list)\n        preds_probs = np.array(preds_list)  \n        labels = np.argmax(y_list, axis=1)\n        predictions = np.argmax(preds_probs, axis=1)\n\n        accuracy_fold = accuracy_score(labels, predictions)\n        precision_fold = precision_score(labels, predictions, average='weighted', zero_division = 0 )\n        recall_fold = recall_score(labels, predictions, average='weighted')\n        f1_fold = f1_score(labels, predictions, average='weighted')\n        if best_f1 <= f1_fold :\n          best_accuracy = accuracy_fold\n          best_precision = precision_fold\n          best_recall = recall_fold\n          best_f1 = f1_fold\n        \n\n      acc_per_fold.append(best_accuracy)\n      precision_per_fold.append(best_precision)\n      recall_per_fold.append(best_recall)\n      f1_per_fold.append(best_f1)\n      print(\"\\rFold: {} accuracy: {:.4f}%  Precision: {:.4f} recall: {:.4f} F1: {:.4f}\".format(fold_no, best_accuracy, best_precision, best_recall, best_f1))\n      if args.loss_type == 'margin_loss':    \n            m = min(0.9, m + 0.1)\n      fold_no += 1\n\naccuracy = np.mean(acc_per_fold)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = np.mean(precision_per_fold)\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = np.mean(recall_per_fold)\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = np.mean(f1_per_fold)\nprint('F1 score: %f' % f1)","metadata":{"id":"wV3H1xPDUZwU","outputId":"ae4edb71-7c80-4e31-c3a5-d105d4ff3bc7","execution":{"iopub.status.busy":"2021-06-13T16:31:00.525158Z","iopub.execute_input":"2021-06-13T16:31:00.525689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"T9ERjLROdsax"},"execution_count":null,"outputs":[]}]}