{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Data Cleaning - Full Corpus.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC9MvzoPI7Op",
        "outputId": "1f67b033-3686-429c-ad06-208deee3ddf9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QtqLcjdEI-Q6",
        "outputId": "cf009afe-d2d2-4a2f-e719-1089d21313ec"
      },
      "source": [
        "mainloc='/content/drive/My Drive/Research/ABSA_Final/'\r\n",
        "mainloc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Research/ABSA_Final/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxpeiv5hIaEF",
        "outputId": "cddc8f50-18df-4c4b-bdaf-de4b7dc4d4fd"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import string\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('treebank')\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.corpus import treebank\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import string\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "#import fasttext\n",
        "import gensim\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import KeyedVectors\n",
        "import seaborn as sns \n",
        "import re  # For preprocessing\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "import spacy  # For preprocessing\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9BXAleMIaD9"
      },
      "source": [
        "Data/data_original_all.csv  : This contain the original dataset of full corpus \n",
        "Data/data_clean_full_corpus.csv : This contain the cleand dataset using full corpus \n",
        "df_we.csv : This contain cleand dataset using full corpus for word embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A3y9TGpIaEK"
      },
      "source": [
        "#dialog \n",
        "#dialog1,dialg2 : 2020\n",
        "#dialog3: 2019\n",
        "#dialog4: 2018 \n",
        "#dialog5: 2017\n",
        "#dialog6: 2016\n",
        "#dialog7: 2015\n",
        "#dialog8: 2014 \n",
        "#dialog9: 2020 ( 2020-11-01 to 2020-12-15) \n",
        "\n",
        "#Mobitel \n",
        "#mobitel1,mobitel2 : 2020\n",
        "#mobitel3: 2019 \n",
        "#mobitel4: 2018 \n",
        "#mobitel5: 2017\n",
        "#mobitel6: 2016 \n",
        "\n",
        "#SLT \n",
        "#SLT1: 2020\n",
        "#SLT2: 2019 \n",
        "#SLT3: 2018\n",
        "#SLT4: 2017 \n",
        "#SLT5: 2016\n",
        "\n",
        "\n",
        "#hUTCH\n",
        "#HUTCH1: 2020\n",
        "#HUTCH2: 2019\n",
        "#HUTCH3: 2018\n",
        "#HUTCH4: 2017\n",
        "#HUTCH5: 2016\n",
        "\n",
        "#AIRTEL\n",
        "#airtel1: 2020\n",
        "#airtel2: 2019 \n",
        "\n",
        "#LANKABEL:\n",
        "#lankabell: 2020\n",
        "\n",
        "#ezcash\n",
        "#ezcash: 2015-2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIn-n6PcIaEL"
      },
      "source": [
        "#Data loading "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erR8lqWSIaEL",
        "outputId": "c5b371af-95e8-4971-a14d-dbb838c79c75"
      },
      "source": [
        "#Dialog \n",
        "dialog1=pd.read_csv(mainloc+\"Data/dialog1.csv\",sep=';')\n",
        "dialog2=pd.read_csv(mainloc+\"Data/dialog2.csv\",sep=';')\n",
        "dialog3=pd.read_csv(mainloc+\"Data/dialog3.csv\",sep=';')\n",
        "dialog4=pd.read_csv(mainloc+\"Data/dialog4.csv\",sep=';')\n",
        "dialog5=pd.read_csv(mainloc+\"Data/dialog5.csv\",sep=';')\n",
        "dialog6=pd.read_csv(mainloc+\"Data/dialog6.csv\",sep=';')\n",
        "dialog7=pd.read_csv(mainloc+\"Data/dialog7.csv\",sep=';')\n",
        "dialog8=pd.read_csv(mainloc+\"Data/dialog8.csv\",sep=';')\n",
        "dialog9=pd.read_csv(mainloc+\"Data/dialog8.csv\",sep=';')\n",
        "dialog=pd.concat([dialog1,dialog2,dialog3,dialog4,dialog5,dialog6,dialog7,dialog8,dialog9],axis=0)\n",
        "dialog=dialog[~(dialog['message'].isna())]\n",
        "dialog['telco']='dialog'\n",
        "print(\"Number of comments Dialog\", dialog.shape[0])\n",
        "\n",
        "#Mobitel \n",
        "mobitel1=pd.read_csv(mainloc+\"Data/mobitel1.csv\",sep=';')\n",
        "mobitel2=pd.read_csv(mainloc+\"Data/mobitel2.csv\",sep=';')\n",
        "mobitel3=pd.read_csv(mainloc+\"Data/mobitel3.csv\",sep=';')\n",
        "mobitel4=pd.read_csv(mainloc+\"Data/mobitel4.csv\",sep=';')\n",
        "mobitel5=pd.read_csv(mainloc+\"Data/mobitel5.csv\",sep=';')\n",
        "mobitel6=pd.read_csv(mainloc+\"Data/mobitel6.csv\",sep=';')\n",
        "mobitel=pd.concat([mobitel1,mobitel2,mobitel3,mobitel4,mobitel5,mobitel6],axis=0)\n",
        "mobitel=mobitel[~(mobitel['message'].isna())]\n",
        "mobitel['telco']='mobitel'\n",
        "print(\"Number of comments Mobitel\", mobitel.shape[0])\n",
        "\n",
        "#slt\n",
        "slt1=pd.read_csv(mainloc+\"Data/slt1.csv\",sep=';')\n",
        "slt2=pd.read_csv(mainloc+\"Data/slt2.csv\",sep=';')\n",
        "slt3=pd.read_csv(mainloc+\"Data/slt1.csv\",sep=';')\n",
        "slt4=pd.read_csv(mainloc+\"Data/slt2.csv\",sep=';')\n",
        "slt5=pd.read_csv(mainloc+\"Data/slt2.csv\",sep=';')\n",
        "slt=pd.concat([slt1,slt2,slt3,slt4,slt5],axis=0)\n",
        "slt=slt[~(slt['message'].isna())]\n",
        "slt['telco']='slt'\n",
        "print(\"Number of comments SLT\", slt.shape[0])\n",
        "\n",
        "#hutch\n",
        "hutch1=pd.read_csv(mainloc+\"Data/hutch1.csv\",sep=';')\n",
        "hutch2=pd.read_csv(mainloc+\"Data/hutch2.csv\",sep=';')\n",
        "hutch3=pd.read_csv(mainloc+\"Data/hutch3.csv\",sep=';')\n",
        "hutch4=pd.read_csv(mainloc+\"Data/hutch4.csv\",sep=';')\n",
        "hutch5=pd.read_csv(mainloc+\"Data/hutch5.csv\",sep=';')\n",
        "hutch=pd.concat([hutch1,hutch2,hutch3,hutch4,hutch5],axis=0)\n",
        "hutch=hutch[~(hutch['message'].isna())]\n",
        "hutch['telco']='hutch'\n",
        "print(\"Number of comments Hutch\", hutch.shape[0])\n",
        "\n",
        "#airtel\n",
        "airtel1=pd.read_csv(mainloc+\"Data/airtel1.csv\",sep=';')\n",
        "airtel2=pd.read_csv(mainloc+\"Data/airtel2.csv\",sep=';')\n",
        "airtel=pd.concat([airtel1,airtel2],axis=0)\n",
        "airtel=airtel[~(airtel['message'].isna())]\n",
        "airtel['telco']='airtel'\n",
        "print(\"Number of comments Airtel\", airtel.shape[0])\n",
        "\n",
        "#lankabell\n",
        "lb=pd.read_csv(mainloc+\"Data/lankabell.csv\",sep=';')\n",
        "lb=lb[~(lb['message'].isna())]\n",
        "lb['telco']='lb'\n",
        "print(\"Number of comments LankaBell\", lb.shape[0])\n",
        "\n",
        "#lankabell\n",
        "ez=pd.read_csv(mainloc+\"Data/ezcash1.csv\",sep=';')\n",
        "ez=lb[~(ez['message'].isna())]\n",
        "ez['telco']='ez'\n",
        "print(\"Number of comments ezcash\", lb.shape[0])\n",
        "\n",
        "#Concatnte all dataframe together \n",
        "df=pd.concat([dialog,mobitel,slt,hutch,airtel,lb,ez],axis=0)\n",
        "print(\"Number of total comments: \",df.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of comments Dialog 213932\n",
            "Number of comments Mobitel 55166\n",
            "Number of comments SLT 97853\n",
            "Number of comments Hutch 66718\n",
            "Number of comments Airtel 29786\n",
            "Number of comments LankaBell 1162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of comments ezcash 1162\n",
            "Number of total comments:  465315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNqqYiyEIaEL"
      },
      "source": [
        "#This contain the original dataset \n",
        "df['id']=df['message'].rank(method='max').astype(int)\n",
        "df.to_csv(mainloc+\"Data/data_original_all.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acwsm_OcIaEM",
        "outputId": "1973d717-0c46-4c47-c11f-cdb41e9c63cc"
      },
      "source": [
        "df=pd.read_csv(mainloc+\"Data/data_original_all.csv\")\n",
        "df=df.drop(columns='telco')\n",
        "df.to_csv(mainloc+\"Full_Courpus.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BJA4gCg7IaEM",
        "outputId": "178bed70-c6f9-43da-e206-da58e25b849f"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>level</th>\n",
              "      <th>object_id</th>\n",
              "      <th>object_type</th>\n",
              "      <th>query_status</th>\n",
              "      <th>query_time</th>\n",
              "      <th>query_type</th>\n",
              "      <th>message</th>\n",
              "      <th>created_time</th>\n",
              "      <th>updated_time</th>\n",
              "      <th>error.message</th>\n",
              "      <th>like_count</th>\n",
              "      <th>comment_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>55826981202/55826981202_10158235905431203</td>\n",
              "      <td>151963</td>\n",
              "      <td>26891</td>\n",
              "      <td>1</td>\n",
              "      <td>55826981202_10158235905431203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:16:29.319263</td>\n",
              "      <td>Facebook:/&lt;page-id&gt;/posts</td>\n",
              "      <td>Dialog ViU App එක download කරගෙන ඔබ කැමතිම සිං...</td>\n",
              "      <td>2020-05-30T15:13:10+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>253963</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158238179226203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Mulinma honda sewawak lebuna ..den slwo ..oba ...</td>\n",
              "      <td>2020-05-31T03:50:11+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>443518</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158239957411203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>වංචාකාරී මුදල් ගසා කන ආයතනක්.සියලු දෙනා එක්වෙම...</td>\n",
              "      <td>2020-05-31T18:12:26+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>272933</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158236232881203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Onema quality ekakin baluwoth free da?</td>\n",
              "      <td>2020-05-30T16:15:44+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>348550</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158239823961203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>When will you get this app ready for Android T...</td>\n",
              "      <td>2020-05-31T17:29:47+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>200115</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158569669101203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>How to activate ipl in viu</td>\n",
              "      <td>2020-09-21T16:36:54+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>337487</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158371044046203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>VIU app ekata dialog tv connect karoth dialog ...</td>\n",
              "      <td>2020-07-09T17:03:17+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>423043</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158241132221203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>දිනක්සි කීව කියල හිතා ගන්ඩ ඈ 😅😅😅</td>\n",
              "      <td>2020-06-01T00:41:32+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>380962</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158242513561203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>news channels like Cnn Bbc can't be viewed</td>\n",
              "      <td>2020-06-01T12:28:29+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>195162</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158236101781203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Hi dialog axiata assistan kenek ona mata dekak...</td>\n",
              "      <td>2020-05-30T15:20:19+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>210951</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158260899726203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>I read all comments, but application is still ...</td>\n",
              "      <td>2020-06-06T17:37:27+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>19897</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158239277546203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>18th century video quality 360p</td>\n",
              "      <td>2020-05-31T14:34:59+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>390057</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158296345541203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>viu app eke kalin data kapune na.eth dan kapen...</td>\n",
              "      <td>2020-06-17T05:30:54+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>169665</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158236226391203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Fb hadala denna slow</td>\n",
              "      <td>2020-05-30T16:13:10+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>246800</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158236219646203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Meka hodi thama viuhub stock nedda anne gana k...</td>\n",
              "      <td>2020-05-30T16:10:58+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>223628</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158347968276203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Kapenne na kiuwata data kad eken eka movies ek...</td>\n",
              "      <td>2020-07-02T02:28:36+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>345281</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158238732961203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Wena network ekekin app eka pawichchi karoth d...</td>\n",
              "      <td>2020-05-31T09:44:18+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>409548</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158238298236203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>ඔයාලගෙ කිසියම් package එකක වෙනස් කිරීමක් කරනවන...</td>\n",
              "      <td>2020-05-31T04:55:43+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>169842</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158238253421203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Fb ගොඩක් slow . ඒක හදල දෙන්න ඩයිලොග්</td>\n",
              "      <td>2020-05-31T04:26:23+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>55826981202/55826981202_10158235905431203/1015...</td>\n",
              "      <td>262536</td>\n",
              "      <td>26892</td>\n",
              "      <td>2</td>\n",
              "      <td>10158235905431203_10158237736401203</td>\n",
              "      <td>data</td>\n",
              "      <td>fetched (200)</td>\n",
              "      <td>2020-11-19 00:17:51.232415</td>\n",
              "      <td>Facebook:/&lt;comment-id&gt;/comments</td>\n",
              "      <td>Nikan internet wath hriyta weda nehe</td>\n",
              "      <td>2020-05-31T00:51:45+0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 path  ...  comment_count\n",
              "0           55826981202/55826981202_10158235905431203  ...            NaN\n",
              "1   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "2   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "3   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "4   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "5   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "6   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "7   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "8   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "9   55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "10  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "11  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "12  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "13  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "14  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "15  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "16  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "17  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "18  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "19  55826981202/55826981202_10158235905431203/1015...  ...            NaN\n",
              "\n",
              "[20 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYNTEQ2IaEM",
        "outputId": "444b639b-4980-4c83-ee80-572bda3ffdec"
      },
      "source": [
        "df=df[['id','message']]\n",
        "df=df.drop_duplicates()\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(213735, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "-zkjvhP4IaEN",
        "outputId": "028ef8e3-73c4-4318-d685-1b898cfcdaa8"
      },
      "source": [
        "def cleansin(x):\n",
        "    y=re.sub(\"[^!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0-9a-zA-Z]+\", ' ', x.message)\n",
        "    return y \n",
        "\n",
        "df['cleansin']=df.apply(cleansin,axis=1)\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>message</th>\n",
              "      <th>cleansin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151963</td>\n",
              "      <td>Dialog ViU App එක download කරගෙන ඔබ කැමතිම සිං...</td>\n",
              "      <td>Dialog ViU App download , , ViU . #DialogViU #ViU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>253963</td>\n",
              "      <td>Mulinma honda sewawak lebuna ..den slwo ..oba ...</td>\n",
              "      <td>Mulinma honda sewawak lebuna ..den slwo ..oba ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ...                                           cleansin\n",
              "0  151963  ...  Dialog ViU App download , , ViU . #DialogViU #ViU\n",
              "1  253963  ...  Mulinma honda sewawak lebuna ..den slwo ..oba ...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU_l8K7zIaEN"
      },
      "source": [
        "#Data cleaning process "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiekKyj8IaEN",
        "outputId": "1308cd29-6cb0-4ec4-b592-2d8d4fc8057f"
      },
      "source": [
        "#Data cleaning \n",
        "df.isnull().sum()\n",
        "#Remove missing if any \n",
        "df = df.dropna().reset_index(drop=True)\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 18:57:13: NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0\n",
              "message     0\n",
              "cleansin    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnm6kgNBIaEO"
      },
      "source": [
        "#test=df[df['message'].str.contains(r'(?:\\s|^)4g(?:\\s|$)')]\n",
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def setContractions(self,contractions):\n",
        "        self.contractions=contractions \n",
        "  \n",
        "    def setStop_singlish(self,stop_singlish):\n",
        "        self.stop_singlish=stop_singlish\n",
        "        \n",
        "    def setLem(self,lemCln):\n",
        "        self.lemCln=lemCln\n",
        "        \n",
        "    def setremnum(self,remnum):\n",
        "        self.remnum=remnum\n",
        "            \n",
        "    def remove_mentions(self, input_text):\n",
        "        return re.sub(r'@\\w+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def remove_email(self,input_text):\n",
        "        return re.sub(r'[\\w\\.-]+@[\\w\\.-]+','',input_text)\n",
        "    \n",
        "    def remove_hash(self,input_text):\n",
        "        return re.sub(r'(?<=#)\\w+','',input_text)\n",
        "    \n",
        "    def remove_time(self,input_text):\n",
        "        return re.sub(r'(2[0-3]|[01][0-9]):[0-5][0-9]:[0-5][0-9]','',input_text)\n",
        "    \n",
        "    def remove_float(self,input_text):\n",
        "        return re.sub(\"\\d*\\.\\d+\",\"\",input_text)\n",
        "        \n",
        "    def remove_phoneno(self,input_text):\n",
        "        y= re.sub(r\"\\b\\d{5}\\b\",'',input_text)\n",
        "        y= re.sub(r\"\\b\\d{4}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{6}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{7}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{8}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{9}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{10}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{11}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{12}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{13}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{14}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{15}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{16}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{17}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{18}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{19}\\b\",'',y)\n",
        "        y= re.sub(r\"\\b\\d{20}\\b\",'',y)\n",
        "        return y \n",
        "        \n",
        "    def remove_year(self,input_text):\n",
        "        return re.sub(r\"\\b(19[40][0-9]|20[0-1][0-9]|2020)\\b\",'',input_text)\n",
        "     \n",
        "    def remove_emoji(self, input_text):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', input_text)\n",
        "    \n",
        "    def apostrophe(self,input_text):\n",
        "        words = input_text.split()\n",
        "        for word in words:\n",
        "            if word.lower() in self.contractions:\n",
        "                input_text = input_text.replace(word, self.contractions[word.lower()])\n",
        "        return input_text\n",
        "    \n",
        "    def spell_check(self,input_text):\n",
        "        input_text=str(input_text)\n",
        "        spell = SpellChecker()\n",
        "        words = input_text.split() \n",
        "        spellcheck_words = [spell.correction(word) for word in words] \n",
        "        return \" \".join(spellcheck_words) \n",
        "\n",
        "    def remove_punctuation(self, input_text):\n",
        "        temp=re.sub(\"[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\",' ',input_text).strip()\n",
        "        output_text=' '.join(temp.split())\n",
        "        return output_text\n",
        "       \n",
        "    def remove_singlewords(self,input_text):\n",
        "        return ' '.join( [w for w in input_text.split() if len(w)>1] )\n",
        "        \n",
        "    def remove_digits(self, input_text):\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        return input_text.lower()\n",
        "    \n",
        "    def remove_stop_singlish(self, input_text):\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in self.stop_singlish)] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def remove_stop_numbers(self,input_text):\n",
        "        words=input_text.split()\n",
        "        clean_words = [word for word in words if (word not in self.remnum)] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        words = input_text.split() \n",
        "        slist= stopwords_list\n",
        "        clean_words = [word for word in words if (word not in slist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def lemmetzing(self,input_text):\n",
        "        wnl = WordNetLemmatizer() \n",
        "        words = input_text.split() \n",
        "        lem=[]\n",
        "        for word,tag in pos_tag(word_tokenize(input_text)):\n",
        "            wntag=tag[0].lower()\n",
        "            wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
        "            if not wntag:\n",
        "                lemma=word\n",
        "                lem.append(lemma)\n",
        "            else:\n",
        "                lemma=wnl.lemmatize(word,wntag)\n",
        "                lem.append(lemma)\n",
        "        \n",
        "        return \" \".join(lem) \n",
        "    \n",
        "    def lemmetize_sin(self,input_text):\n",
        "        vec=[]\n",
        "        words=input_text.split()\n",
        "        for x in range(len(words)):\n",
        "            if(words[x] in list(self.lemCln.original)):\n",
        "                ind=self.lemCln[self.lemCln['original']==words[x]]['subs'].index.values.astype(int)[0]\n",
        "                w=self.lemCln[self.lemCln['original']==words[x]]['subs'][ind]\n",
        "                vec.append(w)   \n",
        "            else:\n",
        "                w=words[x]\n",
        "                vec.append(w)\n",
        "        vec1=' '.join(vec)\n",
        "        return vec1\n",
        "    \n",
        "    def seperate_numwords(self,input_text):\n",
        "        words=input_text.split()\n",
        "        output=''\n",
        "        for word in words:\n",
        "            temp = re.compile(\"([0-9]+)([a-zA-Z]+)\") \n",
        "            if temp.match(word) and len(word)>2:\n",
        "                res = temp.match(word).groups() \n",
        "                output=output+' '+ ' '.join(res).strip()\n",
        "            else:\n",
        "                output=output+' '+word\n",
        "        output= ' '.join(output.split())\n",
        "        return output.strip()\n",
        "    \n",
        "    def remove_zerostartwords(self,input_text):\n",
        "        #temp=re.sub(r'\\b0\\d{2}\\b',' ',input_text).strip()\n",
        "        output= \" \".join(filter(lambda x:x[0]!='0', input_text.split()))\n",
        "        return output \n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_txt = X.apply(self.to_lower).apply(self.apostrophe).apply(self.remove_urls).apply(self.remove_email)\\\n",
        "        .apply(self.remove_emoji).apply(self.remove_hash).apply(self.remove_year)\\\n",
        "        .apply(self.remove_mentions).apply(self.remove_time).apply(self.remove_punctuation).apply(self.remove_phoneno)\\\n",
        "        .apply(self.remove_float).apply(self.seperate_numwords).apply(self.remove_zerostartwords).apply(self.remove_phoneno)\\\n",
        "        .apply(self.remove_stop_numbers).apply(self.remove_stop_singlish).apply(self.lemmetzing).apply(self.lemmetize_sin)\n",
        "        return clean_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtaIoJANIaES"
      },
      "source": [
        "def readTxtFile(loc):\n",
        "    s = open(loc, 'r').read()\n",
        "    whip = eval(s)\n",
        "    return whip\n",
        "\n",
        "def lemFile(loc):\n",
        "    lem=pd.read_csv(loc,header=None,index_col=None)\n",
        "    lem.columns=['original','subs']\n",
        "    lem=lem.dropna()\n",
        "    lem=lem.drop_duplicates()\n",
        "    a=lem.groupby('original').count().reset_index()\n",
        "    lis=a[a['subs']>1].original.unique()\n",
        "    lemCln=lem[~(lem['original'].isin(lis))]\n",
        "    return lemCln\n",
        "\n",
        "def stopfile(loc):\n",
        "    swords=pd.read_csv(loc,header=None)\n",
        "    swords.columns=['sword']\n",
        "    swords=list(swords.dropna()['sword'])\n",
        "    return swords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su2gHKDLIaEU"
      },
      "source": [
        "df=pd.read_csv(mainloc+\"/Data/data_after_clean.csv\")\n",
        "#cleanAnswer1 is the without sinsligsh lemmatization "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcIVCwNIaEV",
        "outputId": "e261b071-fc15-4cc5-ae1d-81da19ed1caa"
      },
      "source": [
        "t = time()\n",
        "\n",
        "contracloc=\"Helper_Files/Contractions.txt\"\n",
        "stopdigitloc = 'Helper_Files/datacln.xlsx'\n",
        "lemloc=\"Helper_Files/lemmatization_singlish.csv\"\n",
        "\n",
        "contractions= readTxtFile(contracloc)\n",
        "\n",
        "#Extract stop values related to digits \n",
        "stop_digit= pd.read_excel(stopdigitloc)\n",
        "stop_digit['word']=stop_digit['word'].astype(str)\n",
        "remnum=stop_digit[(stop_digit['isdigit']==True) & (stop_digit['freq']<=10)]\n",
        "remnum=list(remnum.word)\n",
        "\n",
        "#Extract stop value related to singlish \n",
        "stop_singlish=stop_digit[stop_digit['Stopflag']==1]\n",
        "stop_singlish=list(stop_singlish.word)\n",
        "\n",
        "#Extract lemmatization related to singlish\n",
        "lemcln=lemFile(lemloc)\n",
        "\n",
        "ct = CleanText()\n",
        "ct.setStop_singlish(stop_singlish)\n",
        "ct.setContractions(contractions)\n",
        "ct.setremnum(remnum)\n",
        "ct.setLem(lemcln)\n",
        "sr_clean = ct.fit_transform(df.cleansin)\n",
        "df['cleanAnswer1']=sr_clean\n",
        "\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to clean up everything: 6.81 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaCkuSrGIaEV"
      },
      "source": [
        "#Find the remove numbers by removing .apply(self.remove_stop_numbers) from transfomr function in cleantext class \n",
        "def remove_numbers(df):\n",
        "    cnt = Counter()\n",
        "    df_=df[df['cleananswer_nwords']>2]\n",
        "    for text in df_[\"cleanAnswer\"].values:\n",
        "        for word in text.split():\n",
        "            cnt[word] += 1\n",
        "        \n",
        "    df_wordana = pd.DataFrame.from_dict(cnt, orient='index').reset_index()\n",
        "    df_wordana.columns=['word','freq']\n",
        "    df_wordana['isdigit']= df_wordana['word'].apply(lambda x:x.isdigit())\n",
        "    return df_wordana\n",
        "\n",
        "df_wordana=remove_numbers(df)\n",
        "#df_wordana.to_excel('../DataClnFiles/datacln.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yke51Y5PIaEV",
        "outputId": "d2b37dd8-1bf8-477d-808f-4d2251a13731"
      },
      "source": [
        "def token(x):\n",
        "    y=word_tokenize(x.cleanAnswer)\n",
        "    return y \n",
        "\n",
        "def nwords(x):\n",
        "    y=len(x.tokens)\n",
        "    return y \n",
        "\n",
        "def englishWordsPerc(x):\n",
        "    words=x.cleanAnswer.split()\n",
        "    if(len(words)>0):\n",
        "        l=len([word for word in words if (wordnet.synsets(word))])\n",
        "        perc=100-(l/len(words))*100\n",
        "        return perc\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "df['tokens']=df.apply(token,axis=1)\n",
        "df['nwords']=df.apply(nwords,axis=1)\n",
        "df['singperc']=df.apply(englishWordsPerc,axis=1)\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>message</th>\n",
              "      <th>cleansin</th>\n",
              "      <th>cleanAnswer</th>\n",
              "      <th>cleanAnswer1</th>\n",
              "      <th>tokens</th>\n",
              "      <th>nwords</th>\n",
              "      <th>singperc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151963</td>\n",
              "      <td>Dialog ViU App එක download කරගෙන ඔබ කැමතිම සිං...</td>\n",
              "      <td>Dialog ViU App download , , ViU . #DialogViU #ViU</td>\n",
              "      <td>viu app download viu</td>\n",
              "      <td>viu app download viu</td>\n",
              "      <td>[viu, app, download, viu]</td>\n",
              "      <td>4</td>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>253963</td>\n",
              "      <td>Mulinma honda sewawak lebuna ..den slwo ..oba ...</td>\n",
              "      <td>Mulinma honda sewawak lebuna ..den slwo ..oba ...</td>\n",
              "      <td>mulinma honda sewawak lebuna den slwo oba ayth...</td>\n",
              "      <td>mulinma honda sewawak lebuna den slwo oba ayth...</td>\n",
              "      <td>[mulinma, honda, sewawak, lebuna, den, slwo, o...</td>\n",
              "      <td>22</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                                            message  \\\n",
              "0  151963  Dialog ViU App එක download කරගෙන ඔබ කැමතිම සිං...   \n",
              "1  253963  Mulinma honda sewawak lebuna ..den slwo ..oba ...   \n",
              "\n",
              "                                            cleansin  \\\n",
              "0  Dialog ViU App download , , ViU . #DialogViU #ViU   \n",
              "1  Mulinma honda sewawak lebuna ..den slwo ..oba ...   \n",
              "\n",
              "                                         cleanAnswer  \\\n",
              "0                               viu app download viu   \n",
              "1  mulinma honda sewawak lebuna den slwo oba ayth...   \n",
              "\n",
              "                                        cleanAnswer1  \\\n",
              "0                               viu app download viu   \n",
              "1  mulinma honda sewawak lebuna den slwo oba ayth...   \n",
              "\n",
              "                                              tokens  nwords   singperc  \n",
              "0                          [viu, app, download, viu]       4  75.000000  \n",
              "1  [mulinma, honda, sewawak, lebuna, den, slwo, o...      22  86.363636  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxJ0dS4mIaEV"
      },
      "source": [
        "df.to_csv(\"Data/data_clean_full_corpus.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx8Vy3BZIaEV"
      },
      "source": [
        "#Dataset of word embedding training\n",
        "df_=df[['id','message','cleansin']]\n",
        "df_.to_csv(\"df_we.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}