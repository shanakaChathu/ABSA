{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \n\ndf=pd.read_csv(\"../input/malayalam-new/malayalam_data.csv\")\nd=df.tail(10)\nlist(d.text)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:53:49.155242Z","iopub.execute_input":"2021-06-15T15:53:49.155666Z","iopub.status.idle":"2021-06-15T15:53:49.180179Z","shell.execute_reply.started":"2021-06-15T15:53:49.155632Z","shell.execute_reply":"2021-06-15T15:53:49.178916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install datasets transformers\n#!pip install sentencepiece\n\n#!pip install transformers==4.6.1\n#!pip install tensorflow==2.5\n\nimport collections\nimport pickle\nimport re\nimport random\nimport sys\nimport os \nimport time\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom numpy import cumsum\n\nimport transformers\nfrom transformers import pipeline\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn import preprocessing\nfrom transformers import AutoTokenizer #this automatically identifies tokenizer related to the model\nfrom transformers import TFAutoModelForSequenceClassification, TFTrainingArguments, TFTrainer\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score\n\nfrom transformers import TFAutoModel\nfrom tensorflow.keras.layers import Input, Dropout, Dense\n\n\n#######################################\n### -------- Load libraries ------- ###\n# Load Huggingface transformers\n\n#!pip install transformers\n#!pip install bert-tensorflow\n#!pip install simpletransformers\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval\n\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import classification_report\nfrom pprint import pprint as pp\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\n\n\nimport tensorflow as tf \nimport transformers\nprint('Using Tensorflow version:', tf.__version__)\nprint('Using transformers version:', transformers.__version__)\n\nimport nltk\nnltk.download('stopwords')\n\nimport pandas as pd \nimport numpy as np \nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.corpus import wordnet\nimport pandas as pd \nimport numpy as np \nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom nltk.corpus import wordnet\nimport pandas as pd \nimport numpy as np \nimport string\n!pip install nltk\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('treebank')\nfrom nltk.tag import UnigramTagger\nfrom nltk.corpus import treebank\nimport pandas as pd \nimport numpy as np \nimport string\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\n#import fasttext\nimport gensim\nfrom gensim.test.utils import datapath\nfrom gensim.models import KeyedVectors\nimport seaborn as sns \nimport re  # For preprocessing\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\nimport spacy  # For preprocessing\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n#from xlwt import Workbook \nimport wordcloud\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport lightgbm as lgb\nfrom sklearn.naive_bayes import MultinomialNB\n#from xgboost import XGBClassifier\n#import xgboost\nimport scipy.sparse\n\n\nimport time\nimport warnings\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \nimport pandas as pd \n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport os\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nimport multiprocessing\nfrom gensim.models import Word2Vec\n#from gensim.models import FastText\nfrom sklearn.metrics import roc_auc_score\nfrom numpy import asarray\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport os\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nimport multiprocessing\nfrom gensim.models import Word2Vec\n#from gensim.models import FastText\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers import BatchNormalization\nfrom keras.models import model_from_json\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import BatchNormalization\nfrom keras import backend as K\nfrom sklearn.metrics import recall_score\nfrom keras.models import load_model\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nimport matplotlib.pyplot as pyplot\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers import GlobalMaxPooling1D\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers import BatchNormalization\nfrom keras.models import model_from_json\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import BatchNormalization\nfrom keras import backend as K\nfrom sklearn.metrics import recall_score\nfrom keras.models import load_model\nimport tensorflow as tf \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras import backend as K\nfrom keras.models import Sequential,Model,load_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dropout, Activation, Flatten, \\\n    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D\nfrom keras.layers.recurrent import LSTM, GRU, SimpleRNN\nfrom keras.regularizers import l2, l1_l2\nfrom keras.constraints import maxnorm\nfrom keras import callbacks\nfrom keras.utils import generic_utils,plot_model\nfrom keras.optimizers import Adadelta\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import Bidirectional\n\n\n#Capsule network related libraries \n\nimport os\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras.layers import Input","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:55:54.970784Z","iopub.execute_input":"2021-06-15T17:55:54.97152Z","iopub.status.idle":"2021-06-15T17:56:18.782139Z","shell.execute_reply.started":"2021-06-15T17:55:54.971399Z","shell.execute_reply":"2021-06-15T17:56:18.781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Common Functions \n\ndef accuracy_neural(y_val,y_pred,avg_method,target_names):\n    print(\"Accuracy Score: \",metrics.accuracy_score(y_val,y_pred))\n    print(\"Precision: \", metrics.precision_score(y_val,y_pred,average=avg_method))\n    print(\"Recall: \", metrics.recall_score(y_val,y_pred,average=avg_method))\n    print(\"F1 score: \", metrics.f1_score(y_val,y_pred,average=avg_method))\n    \n    print(\"\\nClassification report\")\n    print(\"---------------------\")\n    print(metrics.classification_report(y_val, y_pred,target_names=target_names))\n\ndef build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(NUM_CLASSES, activation=ACTIVATION)(cls_token)\n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss=LOSS, metrics=['accuracy'])\n    return model\n\ndef regular_encode(texts, tokenizer, maxlen=100):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             #return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    return np.array(enc_di['input_ids'])\n\ndef encodeY(y,classes):\n    encoder = preprocessing.LabelEncoder()\n    y_c = encoder.fit_transform(y)\n    y_c = label_binarize(y_c, classes=classes)\n    return y_c\n\ndef data_read(path):\n    df=pd.read_csv(path)\n    df=df[~df['comment'].isna()]\n    return df ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T17:58:34.661985Z","iopub.execute_input":"2021-06-15T17:58:34.662558Z","iopub.status.idle":"2021-06-15T17:58:34.680072Z","shell.execute_reply.started":"2021-06-15T17:58:34.662515Z","shell.execute_reply":"2021-06-15T17:58:34.678714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xlmr(X_train,y_train,X_test,y_test):\n    \n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy()\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    # Configuration\n    BATCH_SIZE =8 #16 * strategy.num_replicas_in_sync\n    MODEL = 'jplu/tf-xlm-roberta-base' # bert-base-multilingual-uncased\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.1,random_state=1,stratify=y_train)                                                  \n    print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    MAX_LEN = 104 # chosen from EDA\n    X_train = regular_encode(list(X_train.values), tokenizer, maxlen=MAX_LEN)\n    X_val = regular_encode(list(X_val.values), tokenizer, maxlen=MAX_LEN)\n    X_test = regular_encode(list(X_test.values), tokenizer, maxlen=MAX_LEN)\n\n    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().shuffle(1024).\n                     batch(BATCH_SIZE).prefetch(AUTO))\n    valid_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).cache().prefetch(AUTO))\n    test_dataset = (tf.data.Dataset.from_tensor_slices(X_test).batch(BATCH_SIZE))\n\n    with strategy.scope():\n        transformer_layer = TFAutoModel.from_pretrained(MODEL)\n        model = build_model(transformer_layer, max_len=MAX_LEN)\n    #model.summary()\n\n    n_steps = X_train.shape[0] // BATCH_SIZE\n    train_history = model.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=EPOCHS)\n\n    pred = model.predict(test_dataset, verbose=1)\n    #predictions = pred >=0.5\n    #y_pred = predictions.astype(int)\n    \n    y_pred = np.argmax(pred, axis=1)\n    y_test1=np.argmax(y_test, axis=1)\n    \n    acc=metrics.accuracy_score(y_test1,y_pred)\n    pre=metrics.precision_score(y_test1,y_pred,average=avg_method)\n    rec=metrics.recall_score(y_test1,y_pred,average=avg_method)\n    f1=metrics.f1_score(y_test1,y_pred,average=avg_method)\n    \n    return acc,pre,rec,f1\n    \ndef crossval(model_name,x,y):\n    print(model_name)\n    acc_per_fold = []\n    loss_per_fold = []\n    f1_per_fold = []\n    precision_per_fold =[]\n    recall_per_fold=[]\n    \n    if(model_name =='xlmr_sentiment'):\n        kf = StratifiedKFold(n_splits=N_FOLDS,random_state=None, shuffle=False)\n        lst=[]\n        for arr in y:\n            lst.append(np.argmax(arr))\n        kf_split=kf.split(x,lst)\n        \n    elif(model_name =='xlmr_aspect'):\n        kf = KFold(n_splits=N_FOLDS)\n        kf_split=kf.split(x,y)\n    \n    #kf = StratifiedKFold(n_splits=N_FOLDS,random_state=None, shuffle=False)\n    #kf = KFold(n_splits=N_FOLDS)\n    fold_no = 1\n    for train_index, test_index in kf_split:\n        X_train=x[train_index]\n        y_train=y[train_index]\n        X_test=x[test_index]\n        y_test=y[test_index]\n    \n        if model_name in ('xlmr_aspect','xlmr_sentiment'):\n            acc,pre,rec,f1=xlmr(X_train,y_train,X_test,y_test)    \n        else:\n            return print(\"No Model found\")\n            \n        acc_per_fold.append(acc * 100)\n        precision_per_fold.append(pre)\n        recall_per_fold.append(rec)\n        f1_per_fold.append(f1)\n        \n        print(\"Accuracy:\",acc,rec,rec,f1)\n\n    # Increase fold number\n    fold_no = fold_no + 1\n\n    acc=np.mean(acc_per_fold)\n    precision=np.mean(precision_per_fold)\n    recall=np.mean(recall_per_fold)\n    f1=np.mean(f1_per_fold)\n    \n    return model_name,acc,precision,recall,f1","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:36:10.580188Z","iopub.execute_input":"2021-06-15T19:36:10.580552Z","iopub.status.idle":"2021-06-15T19:36:10.605447Z","shell.execute_reply.started":"2021-06-15T19:36:10.58052Z","shell.execute_reply":"2021-06-15T19:36:10.604363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Executing sentiment for xlm-R with Malayalam dataset  \n\nEPOCHS=25\nN_FOLDS=3\navg_method='weighted'\n\n#df=data_read('../input/dftrain/df_train.csv')\n#MODELS_LIST=['xlmr_aspect','xlmr_sentiment']\n\ndf=pd.read_csv(\"../input/malayalam-new/malayalam_data.csv\")\ndf=df[df['text'].map(lambda x: x.isascii())]\ndf.columns=['comment','sentiment']\ndf=df.reset_index()\ndf=df[['comment','sentiment']]\n\n\nMODELS_LIST=['xlmr_sentiment']\nacclist=[] ; prelist=[]  ; reclist=[]  ; f1list=[]  ; losslist=[] ; modelname=[]\n\nfor x in MODELS_LIST:\n    if(x=='xlmr_sentiment'):\n        NUM_CLASSES=7\n        ACTIVATION='sigmoid'\n        LOSS='binary_crossentropy'\n        x=df['comment']\n        y=df['sentiment']\n        y=encodeY(y,[0,1,2,3,4,5,6])\n        mod,acc,precision,recall,f1=crossval('xlmr_sentiment',x,y)   \n        \n    else: \n        print(\"No Model Found in MODEL_LIST\")\n        \n    modelname.append(mod)\n    acclist.append(acc)\n    prelist.append(precision)\n    reclist.append(recall)\n    f1list.append(f1)\n    \n    accdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n    print(accdf)\n    \n\naccdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\naccdf\n\nprint(accdf)\naccdf.to_csv(\"/kaggle/working/accuracy_xlmr_malayalam_new.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:01:59.92084Z","iopub.execute_input":"2021-06-15T21:01:59.921459Z","iopub.status.idle":"2021-06-15T21:35:08.733266Z","shell.execute_reply.started":"2021-06-15T21:01:59.921406Z","shell.execute_reply":"2021-06-15T21:35:08.731856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    #XLMR malayalam   \n    Model   Accuracy  Precision    Recall        F1\n0  xlmr_sentiment  64.822732   0.655688  0.648227  0.649198","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Executing sentiment and aspect model for xlm-R with telco dataset \n\nEPOCHS=10\nN_FOLDS=3\navg_method='weighted'\n\ndf=data_read('../input/dftrain/df_train.csv')\nMODELS_LIST=['xlmr_aspect','xlmr_sentiment']\n\ndf['neutral']=np.where(df['sentiment']==-1,1,0)\ndf['negative']=np.where(df['sentiment']==0,1,0)\ndf['positive']=np.where(df['sentiment']==1,1,0)\n\nMODELS_LIST=['xlmr_sentiment']\nacclist=[] ; prelist=[]  ; reclist=[]  ; f1list=[]  ; losslist=[] ; modelname=[]\n\nfor x in MODELS_LIST:\n    if(x=='xlmr_aspect'):\n        aspects=['network','billing_price','package','customer_service','data','service_product']\n        NUM_CLASSES=6\n        ACTIVATION='sigmoid'\n        LOSS='binary_crossentropy'\n        x=df['comment']\n        y=df[aspects]\n        y=y.values\n        mod,acc,precision,recall,f1=crossval('xlmr_aspect',x,y)  \n        \n    elif(x=='xlmr_sentiment'):\n        NUM_CLASSES=3\n        ACTIVATION='sigmoid'\n        LOSS='binary_crossentropy'\n        x=df['comment']\n        y=df[['neutral','negative','positive']].values\n        #y=encodeY(y,[0,1,2])\n        mod,acc,precision,recall,f1=crossval('xlmr_sentiment',x,y)  \n        \n    else: \n        print(\"No Model Found in MODEL_LIST\")\n        \n    modelname.append(mod)\n    acclist.append(acc)\n    prelist.append(precision)\n    reclist.append(recall)\n    f1list.append(f1)\n    \n    accdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\n    print(accdf)\n    \n\naccdf=pd.DataFrame({'Model':modelname,'Accuracy':acclist,'Precision':prelist,'Recall':reclist, 'F1':f1list})\naccdf\n\nprint(accdf)\naccdf.to_csv(\"/kaggle/working/accuracy_xlmr_malayalam_epoch10_new.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:03:07.787989Z","iopub.execute_input":"2021-06-15T20:03:07.788344Z","iopub.status.idle":"2021-06-15T20:40:08.023621Z","shell.execute_reply.started":"2021-06-15T20:03:07.788312Z","shell.execute_reply":"2021-06-15T20:40:08.022034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Telcoeataet \n\nModel   Accuracy  Precision    Recall        F1\n0  xlmr_sentiment  70.029385   0.771692  0.700294  0.690424","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XLM results of telco dataset \n\nModel   Accuracy  Precision    Recall        F1\n0     xlmr_aspect  36.182270   0.423346  0.497257  0.440639\n1  xlmr_sentiment  66.183074   0.543216  0.661831  0.571339\n\n#XLM results for malayalam dataset \n          Model   Accuracy  Precision    Recall        F1\n0  xlmr_sentiment  41.177589   0.388595  0.411776  0.380334","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XLMR with telco dataset Cross validation 10 \nModel   Accuracy  Precision    Recall        F1\n0     xlmr_aspect  52.376702   0.817193  0.790874  0.793604\n1  xlmr_sentiment  64.453280   0.415423  0.644533  0.505216\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Runing with holdout method \n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xlmr(X_train,y_train,X_test,y_test):\n    \n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy()\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    # Configuration\n    BATCH_SIZE =8 #16 * strategy.num_replicas_in_sync\n    MODEL = 'jplu/tf-xlm-roberta-base' # bert-base-multilingual-uncased\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.1,random_state=1)                                                  \n    print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    MAX_LEN = 104 # chosen from EDA\n    X_train = regular_encode(list(X_train.values), tokenizer, maxlen=MAX_LEN)\n    X_val = regular_encode(list(X_val.values), tokenizer, maxlen=MAX_LEN)\n    X_test = regular_encode(list(X_test.values), tokenizer, maxlen=MAX_LEN)\n\n    train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().shuffle(1024).\n                     batch(BATCH_SIZE).prefetch(AUTO))\n    valid_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).cache().prefetch(AUTO))\n    test_dataset = (tf.data.Dataset.from_tensor_slices(X_test).batch(BATCH_SIZE))\n\n    with strategy.scope():\n        transformer_layer = TFAutoModel.from_pretrained(MODEL)\n        model = build_model(transformer_layer, max_len=MAX_LEN)\n    #model.summary()\n\n    n_steps = X_train.shape[0] // BATCH_SIZE\n    train_history = model.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=EPOCHS)\n    \n    pred = model.predict(test_dataset, verbose=1)\n    #predictions = pred >=0.5\n    #y_pred = predictions.astype(int)\n    \n    #pred_sentiment = np.argmax(pred, axis=1)\n    \n    return train_history,pred\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:02:10.887269Z","iopub.execute_input":"2021-06-15T19:02:10.887747Z","iopub.status.idle":"2021-06-15T19:02:10.903041Z","shell.execute_reply.started":"2021-06-15T19:02:10.887706Z","shell.execute_reply":"2021-06-15T19:02:10.90185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES=3\nACTIVATION='sigmoid'\nLOSS='binary_crossentropy'\nEPOCHS=3\nTARGET_NAMES=['network','billing_price','package','customer_service','data','service_product']\n#['NEUTRAL','NEGATIVE','POSITIVE']\n\ndef learning_curve(history,measure,title):\n    epochs=range(len(history.history[measure]))\n    plt.plot(epochs,history.history[measure], 'r')\n    plt.plot(epochs,history.history['val_'+measure], 'b')\n    plt.title('Training and validation '+title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(title)\n    plt.legend([\"Training \"+measure, \"Validation \"+measure])\n    plt.figure()\n\ndf=data_read('../input/dftrain/df_train.csv')\n\ndf['neutral']=np.where(df['sentiment']==-1,1,0)\ndf['negative']=np.where(df['sentiment']==0,1,0)\ndf['positive']=np.where(df['sentiment']==1,1,0)\n\n# convert to one-hot-encoding-labels\n\nx=df['comment']\n#y=df[TARGET_NAMES].values\n\ny=df[['neutral','negative','positive']].values\n#['neutral','negative','positive']\n#train_labels = to_categorical(y, num_classes=3)\n\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)   \nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n\ntrain_history,pred=xlmr(X_train,y_train,X_test,y_test)\n#accuracy_neural(y_test,y_pred,'weighted',TARGET_NAMES)\n\ny_pred = np.argmax(pred, axis=1)\ny_test1=np.argmax(y_test, axis=1)\naccuracy_neural(y_test1,y_pred,'weighted',['neu','neg','pos'])\n\n#learning_curve(train_history,'loss','loss')\n#learning_curve(train_history,'accuracy','accuracy')\n#learning_curve(train_history,'f1_m','F1')\n#learning_curve(train_history,'precision_m','Precision')\n#learning_curve(train_history,'recall_m','Recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:18:38.007071Z","iopub.execute_input":"2021-06-15T19:18:38.007497Z","iopub.status.idle":"2021-06-15T19:24:26.518322Z","shell.execute_reply.started":"2021-06-15T19:18:38.007442Z","shell.execute_reply":"2021-06-15T19:24:26.517005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:37:11.097685Z","iopub.execute_input":"2021-06-15T18:37:11.098085Z","iopub.status.idle":"2021-06-15T18:37:11.105461Z","shell.execute_reply.started":"2021-06-15T18:37:11.098044Z","shell.execute_reply":"2021-06-15T18:37:11.104005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:55:18.459414Z","iopub.execute_input":"2021-06-15T18:55:18.460212Z","iopub.status.idle":"2021-06-15T18:55:18.474715Z","shell.execute_reply.started":"2021-06-15T18:55:18.460033Z","shell.execute_reply":"2021-06-15T18:55:18.473144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf[['neutral','negative','positive']].values","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:23:28.489196Z","iopub.execute_input":"2021-06-15T18:23:28.489644Z","iopub.status.idle":"2021-06-15T18:23:28.49755Z","shell.execute_reply.started":"2021-06-15T18:23:28.489607Z","shell.execute_reply":"2021-06-15T18:23:28.496693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sentiment = np.argmax(pred, axis=1)\n#print(set(pred_sentiment))\npred_sentiment\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:33:46.23922Z","iopub.execute_input":"2021-06-15T18:33:46.239686Z","iopub.status.idle":"2021-06-15T18:33:46.247125Z","shell.execute_reply.started":"2021-06-15T18:33:46.239646Z","shell.execute_reply":"2021-06-15T18:33:46.246013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=data_read('../input/dftrain/df_train.csv')\n\ndf['neutral']=np.where(df['sentiment']==-1,1,0)\ndf['negative']=np.where(df['sentiment']==0,1,0)\ndf['positive']=np.where(df['sentiment']==1,1,0)\n\ndf[['neutral','negative','positive']].values","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:01:28.030143Z","iopub.execute_input":"2021-06-15T18:01:28.030506Z","iopub.status.idle":"2021-06-15T18:01:28.142879Z","shell.execute_reply.started":"2021-06-15T18:01:28.03046Z","shell.execute_reply":"2021-06-15T18:01:28.141734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sentiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######################################\n### -------- Load libraries ------- ###\n# Load Huggingface transformers\n\n#!pip install transformers\n#!pip install bert-tensorflow\n#!pip install simpletransformers\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval\n\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=data_read('../input/dftrain/df_train.csv')\ndf=df[['comment','sentiment']]\n\ndf['neutral']=np.where(df['sentiment']==-1,1,0)\ndf['negative']=np.where(df['sentiment']==0,1,0)\ndf['positive']=np.where(df['sentiment']==1,1,0)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T13:44:59.198218Z","iopub.execute_input":"2021-06-15T13:44:59.198587Z","iopub.status.idle":"2021-06-15T13:44:59.30052Z","shell.execute_reply.started":"2021-06-15T13:44:59.198558Z","shell.execute_reply":"2021-06-15T13:44:59.29956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=data['review']\ny=data['rating']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\ntrain_df=pd.DataFrame({\"review\":x_train,\"rating\":y_train})\ntest_df=pd.DataFrame({\"review\":x_test,\"rating\":y_test})\n\nprint(train_df.shape)\nprint(test_df.shape)\n\ndef build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(3, activation='softmax')(cls_token) # 3 ratings to predict\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 4\nBATCH_SIZE =8 #16 * strategy.num_replicas_in_sync\nMODEL = 'jplu/tf-xlm-roberta-large' # bert-base-multilingual-uncased","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:16:12.656813Z","iopub.execute_input":"2021-06-15T14:16:12.657215Z","iopub.status.idle":"2021-06-15T14:16:19.379417Z","shell.execute_reply.started":"2021-06-15T14:16:12.657181Z","shell.execute_reply":"2021-06-15T14:16:19.378519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since keras takes 0 as the reference, our category should start from 0 not 1\nrating_mapper_encode = {-1:0,\n                         0: 1,\n                         1: 2\n                       }\n\n# convert back to original rating after prediction later\nrating_mapper_decode = {0: -1,\n                        1: 0,\n                        2: 1,\n                        }\n\ntrain_df['rating'] = train_df['rating'].map(rating_mapper_encode)\n\nfrom tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\n\ntrain_labels = to_categorical(train_df['rating'], num_classes=3)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'],\n                                                  train_labels,\n                                                  stratify=train_labels,\n                                                  test_size=0.1,\n                                                  random_state=1111)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:16:24.915303Z","iopub.execute_input":"2021-06-15T14:16:24.916646Z","iopub.status.idle":"2021-06-15T14:16:25.249576Z","shell.execute_reply.started":"2021-06-15T14:16:24.916597Z","shell.execute_reply":"2021-06-15T14:16:25.245518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"execution":{"iopub.status.busy":"2021-06-15T13:39:44.113999Z","iopub.execute_input":"2021-06-15T13:39:44.11442Z","iopub.status.idle":"2021-06-15T13:39:44.121417Z","shell.execute_reply.started":"2021-06-15T13:39:44.114387Z","shell.execute_reply":"2021-06-15T13:39:44.12033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ndef regular_encode(texts, tokenizer, maxlen=100):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             #return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])\n\n\n\nMAX_LEN = 104 # chosen from EDA\nX_train = regular_encode(list(X_train.values), tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(list(X_val.values), tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(list(test_df['review'].values), tokenizer, maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:16:34.475821Z","iopub.execute_input":"2021-06-15T14:16:34.47636Z","iopub.status.idle":"2021-06-15T14:16:38.75558Z","shell.execute_reply.started":"2021-06-15T14:16:34.476328Z","shell.execute_reply":"2021-06-15T14:16:38.754733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:17:44.629909Z","iopub.execute_input":"2021-06-15T14:17:44.630313Z","iopub.status.idle":"2021-06-15T14:17:44.637573Z","shell.execute_reply.started":"2021-06-15T14:17:44.630282Z","shell.execute_reply":"2021-06-15T14:17:44.636491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .batch(BATCH_SIZE)\n)\n\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\n\n\nEPOCHS=2\nn_steps = X_train.shape[0] // BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T13:29:39.805731Z","iopub.execute_input":"2021-06-15T13:29:39.806133Z","iopub.status.idle":"2021-06-15T13:37:31.934084Z","shell.execute_reply.started":"2021-06-15T13:29:39.806096Z","shell.execute_reply":"2021-06-15T13:37:31.933017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_dataset, verbose=1)\npred_sentiment = np.argmax(pred, axis=1)\nprint(set(pred_sentiment))\nsubmission = pd.DataFrame({'review_id': test_df['review'],\n                           'rating': pred_sentiment})\n\nsubmission['rating'] = submission['rating'].map(rating_mapper_decode)\nsubmission.head(2)\n\ny_pred=list(submission.rating)\ny_val=list(test_df.rating)\n\naccuracy_neural(y_val,y_pred,'weighted',['NEUTRAL','NEGATIVE','POSITIVE'])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T13:37:59.72549Z","iopub.execute_input":"2021-06-15T13:37:59.725878Z","iopub.status.idle":"2021-06-15T13:38:18.913572Z","shell.execute_reply.started":"2021-06-15T13:37:59.725846Z","shell.execute_reply":"2021-06-15T13:38:18.911748Z"},"trusted":true},"execution_count":null,"outputs":[]}]}